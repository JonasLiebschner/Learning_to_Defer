{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde69683-8722-4058-9324-43c6bd027cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 20:18:14.912909: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from LinearModel import LinearNN\n",
    "import datasets.cifar as cifar\n",
    "import datasets.nih as nih\n",
    "from utils import accuracy, setup_default_logging, AverageMeter, WarmupCosineLrScheduler\n",
    "from utils import load_from_checkpoint\n",
    "from Expert import CIFAR100Expert, NIHExpert\n",
    "from feature_extractor.embedding_model import EmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05b2f55-e04d-4174-b323-e6c77b63e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a58800-f68e-4545-8e03-3cc4650b9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image number: 0\n",
      "Loaded image number: 200\n",
      "Loaded image number: 400\n",
      "Loaded image number: 600\n",
      "Loaded image number: 800\n",
      "Loaded image number: 1000\n",
      "Loaded image number: 1200\n",
      "Loaded image number: 1400\n",
      "Loaded image number: 1600\n",
      "Loaded image number: 1800\n",
      "Loaded image number: 2000\n",
      "Loaded image number: 2200\n",
      "Loaded image number: 2400\n",
      "Loaded image number: 2600\n",
      "Loaded image number: 2800\n",
      "Loaded image number: 3000\n",
      "Loaded image number: 3200\n",
      "Loaded image number: 3400\n",
      "Loaded image number: 3600\n",
      "Loaded image number: 3800\n",
      "Loaded image number: 4000\n",
      "Loaded image number: 4200\n",
      "Full length: 4381\n",
      "Loaded image number: 0\n",
      "Loaded image number: 200\n",
      "Loaded image number: 400\n",
      "Loaded image number: 600\n",
      "Loaded image number: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Loaded set number 0\n",
      "Loaded set number 1\n",
      "Loaded set number 2\n",
      "Loaded set number 3\n",
      "Loaded set number 4\n",
      "Loaded set number 5\n",
      "Loaded set number 6\n",
      "Loaded set number 7\n",
      "Loaded set number 8\n",
      "Loaded set number 9\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n"
     ]
    }
   ],
   "source": [
    "import Dataset as ds\n",
    "\n",
    "param = {\n",
    "    \"TARGET\": \"Airspace_Opacity\",\n",
    "    \"PATH\": \"../../Datasets/NIH/\",\n",
    "    \"K\": 10, #Number of folds\n",
    "    \"LABELER_IDS\": [4323195249, 4295232296],\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"alpha\": 1.0, #scaling parameter for the loss function, default=1.0\n",
    "    #\"epochs\": 50,\n",
    "    \"epochs\": 50,\n",
    "    \"patience\": 15, #number of patience steps for early stopping the training\n",
    "    \"expert_type\": \"MLPMixer\", #specify the expert type. For the type of experts available, see-> models -> experts. defualt=predict\n",
    "    \"n_classes\": 2, #K for K class classification\n",
    "    \"K\": 10, #\n",
    "    \n",
    "    \"TRAIN_BATCH_SIZE\": 64,\n",
    "    \"TEST_BATCH_SIZE\": 64,\n",
    "    \"NUM_EXPERTS\": 2,\n",
    "\n",
    "    \"PRELOAD\": True,\n",
    "    \"PREBUILD\": True,\n",
    "    \n",
    "    \"OVERLAP K\": 8,\n",
    "    \"NUMBER LABELS\": 8,\n",
    "}\n",
    "\n",
    "dataManager = ds.DataManager(path = param[\"PATH\"], target = param[\"TARGET\"], param=param, seeds=[0])\n",
    "dataManager.createData()\n",
    "sslDataset = dataManager.getSSLDataset(0)\n",
    "#train_dataloader, val_dataloader, test_dataloader = sslDataset.get_data_loader_for_fold(0)\n",
    "#train_dataloader, val_dataloader, test_dataloader = nih_dataloader.get_data_loader_for_fold(1)\n",
    "#dataloaders = (train_dataloader, val_dataloader, test_dataloader)\n",
    "#sslDataset.get_train_loader_interface(expert=exper(4323195249), batch_size=64, mu = 5, n_iters_per_epoch = 10, L=8, method='comatch', imsize=128, fold_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb22208-cd4e-447a-b434-2a59bf3ecce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(args):\n",
    "    \"\"\"Initialize models\n",
    "\n",
    "    Lineare Modelle, welche später die extrahierten Features übergeben bekommen\n",
    "\n",
    "    :param args: training arguments\n",
    "    :return: tuple\n",
    "        - model: Initialized model\n",
    "        - criteria_x: Supervised loss function\n",
    "        - ema_model: Initialized ema model\n",
    "    \"\"\"\n",
    "    if args[\"dataset\"].lower() == 'cifar100':\n",
    "        feature_dim = 1280\n",
    "    elif args[\"dataset\"].lower() == 'nih':\n",
    "        feature_dim = 512\n",
    "    else:\n",
    "        print(f'Dataset {args[\"dataset\"]} not defined')\n",
    "        sys.exit()\n",
    "    model = LinearNN(num_classes=args[\"n_classes\"], feature_dim=feature_dim, proj=True)\n",
    "\n",
    "    model.train()\n",
    "    model.cuda()  \n",
    "    \n",
    "    if args[\"eval_ema\"]:\n",
    "        ema_model = LinearNN(num_classes=args[\"n_classes\"], feature_dim=feature_dim, proj=True)\n",
    "        for param_q, param_k in zip(model.parameters(), ema_model.parameters()):\n",
    "            param_k.data.copy_(param_q.detach().data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient for eval_net\n",
    "        ema_model.cuda()  \n",
    "        ema_model.eval()\n",
    "    else:\n",
    "        ema_model = None\n",
    "        \n",
    "    criteria_x = nn.CrossEntropyLoss().cuda()\n",
    "    return model, criteria_x, ema_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85889983-5c01-4610-b8d9-fc3b9b019acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class exper:\n",
    "    def __init__(self, id):\n",
    "        self.labeler_id = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2055bfd1-c863-4aa5-8e0e-02a67ad1bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = {\n",
    "        \"root\": \"\", #Dataset direcotry\n",
    "        \"dataset\": \"NIH\", #\n",
    "        \"wresnet_k\": 2, #width factor of wide resnet\n",
    "        \"wresnet_n\": 28, #depth of wide resnet\n",
    "        \"dataset\": \"nih\",\n",
    "        \"n_classes\": 2, #number of classes in dataset\n",
    "        \"n_labeled\": 40, #number of labeled samples for training\n",
    "        \"n_epoches\": 10, #number of training epoches\n",
    "        \"batchsize\": 16, #train batch size of labeled samples\n",
    "        \"mu\": 7, #factor of train batch size of unlabeled samples\n",
    "        \"n_imgs_per_epoch\": 32768, #number of training images for each epoch\n",
    "        \"eval_ema\": True, #whether to use ema model for evaluation\n",
    "        \"ema_m\": 0.999, #\n",
    "        \"lam_u\": 1., #coefficient of unlabeled loss\n",
    "        \"lr\": 0.03, #learning rate for training\n",
    "        \"weight_decay\": 5e-4, #weight decay\n",
    "        \"momentum\": 0.9, #momentum for optimizer\n",
    "        \"seed\": 1, #seed for random behaviors, no seed if negtive\n",
    "        \"temperature\": 0.2, #softmax temperature\n",
    "        \"low_dim\": 64, #\n",
    "        \"lam_c\": 1, #coefficient of contrastive loss\n",
    "        \"contrast_th\": 0.8, #pseudo label graph threshold\n",
    "        \"thr\": 0.95, #pseudo label threshold\n",
    "        \"alpha\": 0.9, #\n",
    "        \"queue-batch\": 5, #number of batches stored in memory bank\n",
    "        \"exp_dir\": \"EmbeddingCM_bin\", #experiment id\n",
    "        \"ex_strength\": 4323195249, #Strength of the expert \n",
    "    }\n",
    "\n",
    "    #Setzt Logger fest\n",
    "    logger, output_dir = setup_default_logging(args)\n",
    "    #logger.info(dict(args[\"_get_kwargs()))\n",
    "    \n",
    "    #tb_logger = SummaryWriter(output_dir)\n",
    "\n",
    "    print(args[\"seed\"])\n",
    "\n",
    "    #Seed init\n",
    "    if args[\"seed\"] >= 0:\n",
    "        set_seed(args[\"seed\"])\n",
    "\n",
    "    #Calculates number of iterations\n",
    "    n_iters_per_epoch = args[\"n_imgs_per_epoch\"] // args[\"batchsize\"]  # 1024\n",
    "    n_iters_all = n_iters_per_epoch * args[\"n_epoches\"]  # 1024 * 200\n",
    "\n",
    "    path = \"../../../Datasets/NIH/\"\n",
    "\n",
    "    #Erstellt das Modell\n",
    "    model, criteria_x, ema_model = set_model(args)\n",
    "    #Lädt das trainierte eingebettete Modell\n",
    "    emb_model = EmbeddingModel(os.getcwd(), args[\"dataset\"])\n",
    "    #logger.info(\"Total params: {:.2f}M\".format(\n",
    "    #    sum(p.numel() for p in model.parameters()) / 1e6))\n",
    "\n",
    "    #if 'cifar' in args[\"dataset.lower\"]():\n",
    "    #    expert = CIFAR100Expert(20, int(args[\"ex_strength\"]), 1, 0, 123)\n",
    "    #    dltrain_x, dltrain_u = cifar.get_train_loader(\n",
    "    #        args[\"dataset, expert\"], args[\"batchsize\"], args[\"mu\"], n_iters_per_epoch\"], L=args[\"n_labeled\"], root=args[\"root\"],\n",
    "    #        method='comatch')\n",
    "    #    dlval = cifar.get_val_loader(args[\"dataset\"], expert, batch_size=64, num_workers=2)\n",
    "    if 'nih' in args[\"dataset\"].lower(): #Erstellt den Experten mit seiner ID\n",
    "        expert = NIHExpert(int(args[\"ex_strength\"]), 2)\n",
    "        exp = exper(int(args[\"ex_strength\"]))\n",
    "        #dltrain_x, dltrain_u = nih.get_train_loader( \n",
    "        #    expert, args[\"batchsize\"], args[\"mu, n_iters_per_epoch\"], L=args[\"n_labeled\"], method='comatch')\n",
    "        #dlval = nih.get_val_loader(expert, batch_size=64, num_workers=4)\n",
    "        #dlval = nih.get_test_loader(expert, batch_size=64, num_workers=4)\n",
    "        \n",
    "        dltrain_x, dltrain_u = sslDataset.get_train_loader_interface( \n",
    "            exp, args[\"batchsize\"], args[\"mu\"], n_iters_per_epoch, L=args[\"n_labeled\"], method='comatch')\n",
    "        dlval = sslDataset.get_val_loader_interface(expert, batch_size=64, num_workers=4)\n",
    "        dlval = sslDataset.get_test_loader_interface(expert, batch_size=64, num_workers=4)\n",
    "\n",
    "    wd_params, non_wd_params = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bn' in name:\n",
    "            non_wd_params.append(param)  \n",
    "        else:\n",
    "            wd_params.append(param)\n",
    "    param_list = [\n",
    "        {'params': wd_params}, {'params': non_wd_params, 'weight_decay': 0}]\n",
    "    optim = torch.optim.SGD(param_list, lr=args[\"lr\"], weight_decay=args[\"weight_decay\"],\n",
    "                            momentum=args[\"momentum\"], nesterov=True)\n",
    "\n",
    "    lr_schdlr = WarmupCosineLrScheduler(optim, n_iters_all, warmup_iter=0)\n",
    "    \n",
    "    model, ema_model, optim, lr_schdlr, start_epoch, metrics, prob_list, queue = \\\n",
    "        load_from_checkpoint(output_dir, model, ema_model, optim, lr_schdlr)\n",
    "\n",
    "    # memory bank\n",
    "    args[\"queue_size\"] = args[\"queue_batch\"]*(args[\"mu\"]+1)*args[\"batchsize\"]\n",
    "    if queue is not None:\n",
    "        queue_feats = queue['queue_feats']\n",
    "        queue_probs = queue['queue_probs']\n",
    "        queue_ptr = queue['queue_ptr']\n",
    "    else:\n",
    "        queue_feats = torch.zeros(args[\"queue_size\"], args[\"low_dim\"]).cuda()\n",
    "        queue_probs = torch.zeros(args[\"queue_size\"], args[\"n_classes\"]).cuda()\n",
    "        queue_ptr = 0\n",
    "\n",
    "    train_args = dict(\n",
    "        model=model,\n",
    "        ema_model=ema_model,\n",
    "        emb_model=emb_model,\n",
    "        prob_list=prob_list,\n",
    "        criteria_x=criteria_x,\n",
    "        optim=optim,\n",
    "        lr_schdlr=lr_schdlr,\n",
    "        dltrain_x=dltrain_x,\n",
    "        dltrain_u=dltrain_u,\n",
    "        args=args,\n",
    "        n_iters=n_iters_per_epoch,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    best_acc = -1\n",
    "    best_epoch = 0\n",
    "\n",
    "    if metrics is not None:\n",
    "        best_acc = metrics['best_acc']\n",
    "        best_epoch = metrics['best_epoch']\n",
    "    logger.info('-----------start training--------------')\n",
    "    for epoch in range(start_epoch, args[\"n_epoches\"]):\n",
    "        \n",
    "        loss_x, loss_u, loss_c, mask_mean, num_pos, guess_label_acc, queue_feats, queue_probs, queue_ptr, prob_list = \\\n",
    "        train_one_epoch(epoch, **train_args, queue_feats=queue_feats,queue_probs=queue_probs,queue_ptr=queue_ptr)\n",
    "\n",
    "        top1, ema_top1 = evaluate(model, ema_model, emb_model, dlval)\n",
    "\n",
    "\n",
    "        tb_logger.add_scalar('loss_x', loss_x, epoch)\n",
    "        tb_logger.add_scalar('loss_u', loss_u, epoch)\n",
    "        tb_logger.add_scalar('loss_c', loss_c, epoch)\n",
    "        tb_logger.add_scalar('guess_label_acc', guess_label_acc, epoch)\n",
    "        tb_logger.add_scalar('test_acc', top1, epoch)\n",
    "        tb_logger.add_scalar('test_ema_acc', ema_top1, epoch)\n",
    "        tb_logger.add_scalar('mask', mask_mean, epoch)\n",
    "        tb_logger.add_scalar('num_pos', num_pos, epoch)\n",
    "\n",
    "        if best_acc < top1:\n",
    "            best_acc = top1\n",
    "            best_epoch = epoch\n",
    "\n",
    "        logger.info(\"Epoch {}. Acc: {:.4f}. Ema-Acc: {:.4f}. best_acc: {:.4f} in epoch{}\".\n",
    "                    format(epoch, top1, ema_top1, best_acc, best_epoch))\n",
    "        \n",
    "        save_obj = {\n",
    "            'model': model.state_dict(),\n",
    "            'ema_model': ema_model.state_dict(),\n",
    "            'optimizer': optim.state_dict(),\n",
    "            'lr_scheduler': lr_schdlr.state_dict(),\n",
    "            'prob_list': prob_list,\n",
    "            'queue': {'queue_feats':queue_feats, 'queue_probs':queue_probs, 'queue_ptr':queue_ptr},\n",
    "            'metrics': {'best_acc': best_acc, 'best_epoch': best_epoch},\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(save_obj, os.path.join(output_dir, 'ckp.latest'))\n",
    "    _, _ = evaluate(model, ema_model, emb_model, dlval)\n",
    "    if 'cifar' in args[\"dataset\"].lower():\n",
    "        predictions = predict_cifar(model, ema_model, emb_model, dltrain_x, dltrain_u, dlval)\n",
    "    elif 'nih' in args[\"dataset\"].lower():\n",
    "        predictions = predict_nih(model, ema_model, emb_model, dltrain_x, dltrain_u, dlval)\n",
    "\n",
    "    logger.info(\"***** Generate Predictions *****\")\n",
    "    if not os.path.exists('./artificial_expert_labels/'):\n",
    "        os.makedirs('./artificial_expert_labels/')\n",
    "    pred_file = f'{args[\"exp_dir\"]}_{args[\"dataset\"].lower()}_expert{args[\"ex_strength\"]}.{args[\"seed\"]}@{args[\"n_labeled\"]}_predictions.json'\n",
    "    with open(f'artificial_expert_labels/{pred_file}', 'w') as f:\n",
    "        json.dump(predictions, f)\n",
    "    with open(os.getcwd()[:-len('Embedding-Semi-Supervised')]+f'Learning-to-Defer-Algs/artificial_expert_labels/{pred_file}', 'w') as f:\n",
    "        json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f352c697-048b-4e6e-926b-959b9c4b30ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#Dataset direcotry\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNIH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex_strength\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4323195249\u001b[39m, \u001b[38;5;66;03m#Strength of the expert \u001b[39;00m\n\u001b[1;32m     30\u001b[0m }\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#Setzt Logger fest\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m logger, output_dir \u001b[38;5;241m=\u001b[39m \u001b[43msetup_default_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#logger.info(dict(args[\"_get_kwargs()))\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#tb_logger = SummaryWriter(output_dir)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/home/joli/Masterarbeit/SSL/utils.py:25\u001b[0m, in \u001b[0;36msetup_default_logging\u001b[0;34m(args, default_level, format)\u001b[0m\n\u001b[1;32m     23\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m], args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mex\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex_strength\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_x\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_labeled\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mfolds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, args\u001b[38;5;241m.\u001b[39mexp_dir)\n\u001b[1;32m     27\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "feba799c-2b98-46cd-a3cc-2162f4c5c629",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;66;03m#seed for random behaviors, no seed if negtive\u001b[39;00m\n\u001b[1;32m      3\u001b[0m }\n\u001b[0;32m----> 4\u001b[0m \u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ffb1c-adb1-4a8f-b64c-2f0ec1c76235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joli-env_kernel",
   "language": "python",
   "name": "joli-env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
