{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde69683-8722-4058-9324-43c6bd027cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 06:02:51.972278: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from LinearModel import LinearNN\n",
    "import datasets.cifar as cifar\n",
    "import datasets.nih as nih\n",
    "from utils import accuracy, setup_default_logging, AverageMeter, WarmupCosineLrScheduler\n",
    "from utils import load_from_checkpoint\n",
    "from Expert import CIFAR100Expert, NIHExpert\n",
    "from feature_extractor.embedding_model import EmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05b2f55-e04d-4174-b323-e6c77b63e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a58800-f68e-4545-8e03-3cc4650b9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images of the whole dataset: 4381\n",
      "Loaded image number: 0\n",
      "Loaded image number: 200\n",
      "Loaded image number: 400\n",
      "Loaded image number: 600\n",
      "Loaded image number: 800\n",
      "Loaded image number: 1000\n",
      "Loaded image number: 1200\n",
      "Loaded image number: 1400\n",
      "Loaded image number: 1600\n",
      "Loaded image number: 1800\n",
      "Loaded image number: 2000\n",
      "Loaded image number: 2200\n",
      "Loaded image number: 2400\n",
      "Loaded image number: 2600\n",
      "Loaded image number: 2800\n",
      "Loaded image number: 3000\n",
      "Loaded image number: 3200\n",
      "Loaded image number: 3400\n",
      "Loaded image number: 3600\n",
      "Loaded image number: 3800\n",
      "Loaded image number: 4000\n",
      "Loaded image number: 4200\n",
      "Full length: 4381\n",
      "Loaded image number: 0\n",
      "Loaded image number: 200\n",
      "Loaded image number: 400\n",
      "Loaded image number: 600\n",
      "Loaded image number: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Loaded set number 0\n",
      "Loaded set number 1\n",
      "Loaded set number 2\n",
      "Loaded set number 3\n",
      "Loaded set number 4\n",
      "Loaded set number 5\n",
      "Loaded set number 6\n",
      "Loaded set number 7\n",
      "Loaded set number 8\n",
      "Loaded set number 9\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n"
     ]
    }
   ],
   "source": [
    "import Dataset as ds\n",
    "\n",
    "param = {\n",
    "    \"TARGET\": \"Airspace_Opacity\",\n",
    "    \"PATH\": \"../../Datasets/NIH/\",\n",
    "    \"K\": 10, #Number of folds\n",
    "    \"LABELER_IDS\": [4323195249, 4295232296],\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"alpha\": 1.0, #scaling parameter for the loss function, default=1.0\n",
    "    #\"epochs\": 50,\n",
    "    \"epochs\": 50,\n",
    "    \"patience\": 15, #number of patience steps for early stopping the training\n",
    "    \"expert_type\": \"MLPMixer\", #specify the expert type. For the type of experts available, see-> models -> experts. defualt=predict\n",
    "    \"n_classes\": 2, #K for K class classification\n",
    "    \"K\": 10, #\n",
    "    \n",
    "    \"TRAIN_BATCH_SIZE\": 64,\n",
    "    \"TEST_BATCH_SIZE\": 64,\n",
    "    \"NUM_EXPERTS\": 2,\n",
    "\n",
    "    \"PRELOAD\": True,\n",
    "    \"PREBUILD\": True,\n",
    "    \n",
    "    \"OVERLAP K\": 8,\n",
    "    \"NUMBER LABELS\": 8,\n",
    "}\n",
    "\n",
    "dataManager = ds.DataManager(path = param[\"PATH\"], target = param[\"TARGET\"], param=param, seeds=[0])\n",
    "dataManager.createData()\n",
    "sslDataset = dataManager.getSSLDataset(0)\n",
    "#train_dataloader, val_dataloader, test_dataloader = sslDataset.get_data_loader_for_fold(0)\n",
    "#train_dataloader, val_dataloader, test_dataloader = nih_dataloader.get_data_loader_for_fold(1)\n",
    "#dataloaders = (train_dataloader, val_dataloader, test_dataloader)\n",
    "#sslDataset.get_train_loader_interface(expert=exper(4323195249), batch_size=64, mu = 5, n_iters_per_epoch = 10, L=8, method='comatch', imsize=128, fold_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb22208-cd4e-447a-b434-2a59bf3ecce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(args):\n",
    "    \"\"\"Initialize models\n",
    "\n",
    "    Lineare Modelle, welche später die extrahierten Features übergeben bekommen\n",
    "\n",
    "    :param args: training arguments\n",
    "    :return: tuple\n",
    "        - model: Initialized model\n",
    "        - criteria_x: Supervised loss function\n",
    "        - ema_model: Initialized ema model\n",
    "    \"\"\"\n",
    "    if args[\"dataset\"].lower() == 'cifar100':\n",
    "        feature_dim = 1280\n",
    "    elif args[\"dataset\"].lower() == 'nih':\n",
    "        feature_dim = 512\n",
    "    else:\n",
    "        print(f'Dataset {args[\"dataset\"]} not defined')\n",
    "        sys.exit()\n",
    "    model = LinearNN(num_classes=args[\"n_classes\"], feature_dim=feature_dim, proj=True)\n",
    "\n",
    "    model.train()\n",
    "    model.cuda()  \n",
    "    \n",
    "    if args[\"eval_ema\"]:\n",
    "        ema_model = LinearNN(num_classes=args[\"n_classes\"], feature_dim=feature_dim, proj=True)\n",
    "        for param_q, param_k in zip(model.parameters(), ema_model.parameters()):\n",
    "            param_k.data.copy_(param_q.detach().data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient for eval_net\n",
    "        ema_model.cuda()  \n",
    "        ema_model.eval()\n",
    "    else:\n",
    "        ema_model = None\n",
    "        \n",
    "    criteria_x = nn.CrossEntropyLoss().cuda()\n",
    "    return model, criteria_x, ema_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6bce0b-14ac-459e-abee-c9190e043a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch,\n",
    "                    model,\n",
    "                    ema_model,\n",
    "                    emb_model,\n",
    "                    prob_list,\n",
    "                    criteria_x,\n",
    "                    optim,\n",
    "                    lr_schdlr,\n",
    "                    dltrain_x,\n",
    "                    dltrain_u,\n",
    "                    args,\n",
    "                    n_iters,\n",
    "                    logger,\n",
    "                    queue_feats,\n",
    "                    queue_probs,\n",
    "                    queue_ptr,\n",
    "                    ):\n",
    "    \"\"\"Train one epoch on the train set\n",
    "\n",
    "    :param epoch: Current epoch\n",
    "    :param model: Model\n",
    "    :param ema_model: EMA-Model\n",
    "    :param emb_model: Embedding model\n",
    "    :param prob_list: List of probabilities\n",
    "    :param criteria_x: Supervised loss function\n",
    "    :param optim: Optimizer\n",
    "    :param lr_schdlr: Learning rate scheduler\n",
    "    :param dltrain_x: Data loader for the labeled training instances\n",
    "    :param dltrain_u: Data loader for the unlabeled training instances\n",
    "    :param args: Training arguments\n",
    "    :param n_iters: Number of iterations per epoch\n",
    "    :param logger: Logger\n",
    "    :param queue_feats: Memory bank feature vectors\n",
    "    :param queue_probs: Memory bank probabilities\n",
    "    :param queue_ptr: Memory bank ptr\n",
    "    :return: tuple\n",
    "        - Average supervised loss\n",
    "        - Average unsupervised loss\n",
    "        - Average contrastive loss\n",
    "        - Average mask\n",
    "        - Average number of edges in the pseudo label graph\n",
    "        - Percentage of correct pseudo labels\n",
    "        - Memory bank feature vectors\n",
    "        - Memory bank probabilities\n",
    "        - Memory bank ptr\n",
    "        - List of probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    loss_x_meter = AverageMeter()\n",
    "    loss_u_meter = AverageMeter()\n",
    "    loss_contrast_meter = AverageMeter()\n",
    "    # the number of correct pseudo-labels\n",
    "    n_correct_u_lbs_meter = AverageMeter()\n",
    "    # the number of confident unlabeled data\n",
    "    n_strong_aug_meter = AverageMeter()\n",
    "    mask_meter = AverageMeter()\n",
    "    # the number of edges in the pseudo-label graph\n",
    "    pos_meter = AverageMeter()\n",
    "\n",
    "    \n",
    "    epoch_start = time.time()  # start time\n",
    "    dl_x, dl_u = iter(dltrain_x), iter(dltrain_u)\n",
    "    for it in range(n_iters):\n",
    "        ims_x_weak, lbs_x, im_id = next(dl_x)\n",
    "        (ims_u_weak, ims_u_strong0, ims_u_strong1), lbs_u_real, im_id = next(dl_u)\n",
    "\n",
    "        lbs_x = lbs_x.type(torch.LongTensor) \n",
    "        lbs_x = lbs_x.cuda()\n",
    "        lbs_u_real = lbs_u_real.cuda()\n",
    "\n",
    "        # --------------------------------------\n",
    "        bt = ims_x_weak.size(0)\n",
    "        btu = ims_u_weak.size(0)\n",
    "\n",
    "        imgs = torch.cat([ims_x_weak, ims_u_weak, ims_u_strong0, ims_u_strong1], dim=0).cuda()\n",
    "        embedding = emb_model.get_embedding(batch=imgs)\n",
    "        logits, features = model(embedding)\n",
    "\n",
    "        logits_x = logits[:bt]\n",
    "        logits_u_w, logits_u_s0, logits_u_s1 = torch.split(logits[bt:], btu)\n",
    "        \n",
    "        feats_x = features[:bt]\n",
    "        feats_u_w, feats_u_s0, feats_u_s1 = torch.split(features[bt:], btu)\n",
    "\n",
    "        \n",
    "        loss_x = criteria_x(logits_x, lbs_x)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_u_w = logits_u_w.detach()\n",
    "            feats_x = feats_x.detach()\n",
    "            feats_u_w = feats_u_w.detach()\n",
    "            \n",
    "            probs = torch.softmax(logits_u_w, dim=1)            \n",
    "            # DA\n",
    "            prob_list.append(probs.mean(0))\n",
    "            if len(prob_list)>32:\n",
    "                prob_list.pop(0)\n",
    "            prob_avg = torch.stack(prob_list, dim=0).mean(0)\n",
    "            probs = probs / prob_avg\n",
    "            probs = probs / probs.sum(dim=1, keepdim=True)   \n",
    "\n",
    "            probs_orig = probs.clone()\n",
    "            \n",
    "            if epoch>0 or it>args[\"queue_batch\"]: # memory-smoothing \n",
    "                A = torch.exp(torch.mm(feats_u_w, queue_feats.t())/args[\"temperature\"])       \n",
    "                A = A/A.sum(1,keepdim=True)                    \n",
    "                probs = args[\"alpha\"]*probs + (1-args[\"alpha\"])*torch.mm(A, queue_probs)               \n",
    "            \n",
    "            scores, lbs_u_guess = torch.max(probs, dim=1)\n",
    "            mask = scores.ge(args[\"thr\"]).float() \n",
    "                   \n",
    "            feats_w = torch.cat([feats_u_w,feats_x],dim=0)   \n",
    "            onehot = torch.zeros(bt,args[\"n_classes\"]).cuda().scatter(1,lbs_x.view(-1,1),1)\n",
    "            probs_w = torch.cat([probs_orig,onehot],dim=0)\n",
    "            \n",
    "            # update memory bank\n",
    "            n = bt+btu   \n",
    "            queue_feats[queue_ptr:queue_ptr + n,:] = feats_w\n",
    "            queue_probs[queue_ptr:queue_ptr + n,:] = probs_w      \n",
    "            queue_ptr = (queue_ptr+n)%args[\"queue_size\"]\n",
    "\n",
    "            \n",
    "        # embedding similarity\n",
    "        sim = torch.exp(torch.mm(feats_u_s0, feats_u_s1.t())/args[\"temperature\"]) \n",
    "        sim_probs = sim / sim.sum(1, keepdim=True)\n",
    "        \n",
    "        # pseudo-label graph with self-loop\n",
    "        Q = torch.mm(probs, probs.t())       \n",
    "        Q.fill_diagonal_(1)    \n",
    "        pos_mask = (Q>=args[\"contrast_th\"]).float()\n",
    "            \n",
    "        Q = Q * pos_mask\n",
    "        Q = Q / Q.sum(1, keepdim=True)\n",
    "        \n",
    "        # contrastive loss\n",
    "        loss_contrast = - (torch.log(sim_probs + 1e-7) * Q).sum(1)\n",
    "        loss_contrast = loss_contrast.mean()  \n",
    "        \n",
    "        # unsupervised classification loss\n",
    "        loss_u = - torch.sum((F.log_softmax(logits_u_s0,dim=1) * probs),dim=1) * mask                \n",
    "        loss_u = loss_u.mean()\n",
    "        \n",
    "        loss = loss_x + args[\"lam_u\"] * loss_u + args[\"lam_c\"] * loss_contrast\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        lr_schdlr.step()\n",
    "\n",
    "        if args[\"eval_ema\"]:\n",
    "            with torch.no_grad():\n",
    "                ema_model_update(model, ema_model, args[\"ema_m\"])\n",
    "                \n",
    "        loss_x_meter.update(loss_x.item())\n",
    "        loss_u_meter.update(loss_u.item())\n",
    "        loss_contrast_meter.update(loss_contrast.item())\n",
    "        mask_meter.update(mask.mean().item())       \n",
    "        pos_meter.update(pos_mask.sum(1).float().mean().item())\n",
    "        \n",
    "        corr_u_lb = (lbs_u_guess == lbs_u_real).float() * mask\n",
    "        n_correct_u_lbs_meter.update(corr_u_lb.sum().item())\n",
    "        n_strong_aug_meter.update(mask.sum().item())\n",
    "\n",
    "        if (it + 1) % 64 == 0:\n",
    "            t = time.time() - epoch_start\n",
    "\n",
    "            lr_log = [pg['lr'] for pg in optim.param_groups]\n",
    "            lr_log = sum(lr_log) / len(lr_log)\n",
    "\n",
    "            logger.info(\"{}-x{}-s{}, {} | epoch:{}, iter: {}. loss_u: {:.3f}. loss_x: {:.3f}. loss_c: {:.3f}. \"\n",
    "                        \"n_correct_u: {:.2f}/{:.2f}. Mask:{:.3f}. num_pos: {:.1f}. LR: {:.3f}. Time: {:.2f}\".format(\n",
    "                args[\"dataset\"], args[\"n_labeled\"], args[\"seed\"], args[\"exp_dir\"], epoch, it + 1, loss_u_meter.avg, loss_x_meter.avg, loss_contrast_meter.avg, n_correct_u_lbs_meter.avg, n_strong_aug_meter.avg, mask_meter.avg, pos_meter.avg, lr_log, t))\n",
    "            epoch_start = time.time()\n",
    "\n",
    "    return loss_x_meter.avg, loss_u_meter.avg, loss_contrast_meter.avg, mask_meter.avg, pos_meter.avg, n_correct_u_lbs_meter.avg/n_strong_aug_meter.avg, queue_feats, queue_probs, queue_ptr, prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2647367-aa07-4c8c-a8ea-363d8acc9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, ema_model, emb_model, dataloader):\n",
    "    \"\"\"Evaluate model on train or validation set\n",
    "\n",
    "    :param model: Model\n",
    "    :param ema_model: EMA-Model\n",
    "    :param emb_model: Embedding model\n",
    "    :param dataloader: Data loader for the evaluation set\n",
    "    :return: tuple\n",
    "        - Accuracy of the model\n",
    "        - Accuracy of the ema_model\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    top1_meter = AverageMeter()\n",
    "    ema_top1_meter = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ims, lbs, im_id in dataloader:\n",
    "            ims = ims.cuda()\n",
    "            lbs = lbs.cuda()\n",
    "\n",
    "            embedding = emb_model.get_embedding(batch=ims)\n",
    "            logits, _ = model(embedding)\n",
    "            scores = torch.softmax(logits, dim=1)\n",
    "            preds += torch.argmax(scores, dim=1).cpu().tolist()\n",
    "            targets += lbs.cpu().tolist()\n",
    "            top1 = accuracy(scores, lbs, (1, ))\n",
    "            top1_meter.update(top1.item())\n",
    "            \n",
    "            if ema_model is not None:\n",
    "                embedding = emb_model.get_embedding(batch=ims)\n",
    "                logits, _ = ema_model(embedding)\n",
    "                scores = torch.softmax(logits, dim=1)\n",
    "                top1 = accuracy(scores, lbs, (1, ))\n",
    "                ema_top1_meter.update(top1.item())\n",
    "    return top1_meter.avg, ema_top1_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179d3940-2865-4f7f-a420-5046c3558e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ema_model_update(model, ema_model, ema_m):\n",
    "    \"\"\"Momentum update of evaluation model (exponential moving average)\n",
    "\n",
    "    :param model: Model\n",
    "    :param ema_model: EMA-Model\n",
    "    :param ema_m: Ema parameter\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for param_train, param_eval in zip(model.parameters(), ema_model.parameters()):\n",
    "        param_eval.copy_(param_eval * ema_m + param_train.detach() * (1-ema_m))\n",
    "\n",
    "    for buffer_train, buffer_eval in zip(model.buffers(), ema_model.buffers()):\n",
    "        buffer_eval.copy_(buffer_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85889983-5c01-4610-b8d9-fc3b9b019acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class exper:\n",
    "    def __init__(self, id):\n",
    "        self.labeler_id = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2055bfd1-c863-4aa5-8e0e-02a67ad1bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = {\n",
    "        \"root\": \"\", #Dataset direcotry\n",
    "        \"dataset\": \"NIH\", #\n",
    "        \"wresnet_k\": 2, #width factor of wide resnet\n",
    "        \"wresnet_n\": 28, #depth of wide resnet\n",
    "        \"n_classes\": 2, #number of classes in dataset\n",
    "        \"n_labeled\": 12, #number of labeled samples for training\n",
    "        \"n_epoches\": 10, #number of training epoches\n",
    "        \"batchsize\": 16, #train batch size of labeled samples\n",
    "        \"mu\": 7, #factor of train batch size of unlabeled samples\n",
    "        \"n_imgs_per_epoch\": 32768, #number of training images for each epoch\n",
    "        #\"n_imgs_per_epoch\": 4381,\n",
    "        \"eval_ema\": True, #whether to use ema model for evaluation\n",
    "        \"ema_m\": 0.999, #\n",
    "        \"lam_u\": 1., #coefficient of unlabeled loss\n",
    "        \"lr\": 0.03, #learning rate for training\n",
    "        \"weight_decay\": 5e-4, #weight decay\n",
    "        \"momentum\": 0.9, #momentum for optimizer\n",
    "        \"seed\": 2, #seed for random behaviors, no seed if negtive\n",
    "        \"temperature\": 0.2, #softmax temperature\n",
    "        \"low_dim\": 64, #\n",
    "        \"lam_c\": 1, #coefficient of contrastive loss\n",
    "        \"contrast_th\": 0.8, #pseudo label graph threshold\n",
    "        \"thr\": 0.95, #pseudo label threshold\n",
    "        \"alpha\": 0.9, #\n",
    "        \"queue_batch\": 5, #number of batches stored in memory bank\n",
    "        \"exp_dir\": \"EmbeddingCM_bin\", #experiment id\n",
    "        #\"ex_strength\": 4323195249, #Strength of the expert \n",
    "        \"ex_strength\": 4295232296\n",
    "    }\n",
    "\n",
    "    #Setzt Logger fest\n",
    "    logger, output_dir = setup_default_logging(args)\n",
    "    logger.info(dict(args))\n",
    "    \n",
    "    tb_logger = SummaryWriter(output_dir)\n",
    "\n",
    "    print(args[\"seed\"])\n",
    "\n",
    "    #Seed init\n",
    "    if args[\"seed\"] >= 0:\n",
    "        set_seed(args[\"seed\"])\n",
    "\n",
    "    #Calculates number of iterations\n",
    "    n_iters_per_epoch = args[\"n_imgs_per_epoch\"] // args[\"batchsize\"]  # 1024\n",
    "    n_iters_all = n_iters_per_epoch * args[\"n_epoches\"]  # 1024 * 200\n",
    "\n",
    "    path = \"../../../Datasets/NIH/\"\n",
    "\n",
    "    #Erstellt das Modell\n",
    "    model, criteria_x, ema_model = set_model(args)\n",
    "    #Lädt das trainierte eingebettete Modell\n",
    "    emb_model = EmbeddingModel(os.getcwd(), args[\"dataset\"])\n",
    "    logger.info(\"Total params: {:.2f}M\".format(\n",
    "        sum(p.numel() for p in model.parameters()) / 1e6))\n",
    "\n",
    "    #if 'cifar' in args[\"dataset.lower\"]():\n",
    "    #    expert = CIFAR100Expert(20, int(args[\"ex_strength\"]), 1, 0, 123)\n",
    "    #    dltrain_x, dltrain_u = cifar.get_train_loader(\n",
    "    #        args[\"dataset, expert\"], args[\"batchsize\"], args[\"mu\"], n_iters_per_epoch\"], L=args[\"n_labeled\"], root=args[\"root\"],\n",
    "    #        method='comatch')\n",
    "    #    dlval = cifar.get_val_loader(args[\"dataset\"], expert, batch_size=64, num_workers=2)\n",
    "    if 'nih' in args[\"dataset\"].lower(): #Erstellt den Experten mit seiner ID\n",
    "        expert = NIHExpert(int(args[\"ex_strength\"]), 2)\n",
    "        exp = exper(int(args[\"ex_strength\"]))\n",
    "        #dltrain_x, dltrain_u = nih.get_train_loader( \n",
    "        #    expert, args[\"batchsize\"], args[\"mu, n_iters_per_epoch\"], L=args[\"n_labeled\"], method='comatch')\n",
    "        #dlval = nih.get_val_loader(expert, batch_size=64, num_workers=4)\n",
    "        #dlval = nih.get_test_loader(expert, batch_size=64, num_workers=4)\n",
    "        \n",
    "        dltrain_x, dltrain_u = sslDataset.get_train_loader_interface( \n",
    "            exp, args[\"batchsize\"], args[\"mu\"], n_iters_per_epoch, L=args[\"n_labeled\"], method='comatch')\n",
    "        dlval = sslDataset.get_val_loader_interface(expert, batch_size=64, num_workers=4)\n",
    "        dlval = sslDataset.get_test_loader_interface(expert, batch_size=64, num_workers=4)\n",
    "\n",
    "    wd_params, non_wd_params = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bn' in name:\n",
    "            non_wd_params.append(param)  \n",
    "        else:\n",
    "            wd_params.append(param)\n",
    "    param_list = [\n",
    "        {'params': wd_params}, {'params': non_wd_params, 'weight_decay': 0}]\n",
    "    optim = torch.optim.SGD(param_list, lr=args[\"lr\"], weight_decay=args[\"weight_decay\"],\n",
    "                            momentum=args[\"momentum\"], nesterov=True)\n",
    "\n",
    "    lr_schdlr = WarmupCosineLrScheduler(optim, n_iters_all, warmup_iter=0)\n",
    "    \n",
    "    model, ema_model, optim, lr_schdlr, start_epoch, metrics, prob_list, queue = \\\n",
    "        load_from_checkpoint(output_dir, model, ema_model, optim, lr_schdlr)\n",
    "\n",
    "    # memory bank\n",
    "    args[\"queue_size\"] = args[\"queue_batch\"]*(args[\"mu\"]+1)*args[\"batchsize\"]\n",
    "    if queue is not None:\n",
    "        queue_feats = queue['queue_feats']\n",
    "        queue_probs = queue['queue_probs']\n",
    "        queue_ptr = queue['queue_ptr']\n",
    "    else:\n",
    "        queue_feats = torch.zeros(args[\"queue_size\"], args[\"low_dim\"]).cuda()\n",
    "        queue_probs = torch.zeros(args[\"queue_size\"], args[\"n_classes\"]).cuda()\n",
    "        queue_ptr = 0\n",
    "\n",
    "    train_args = dict(\n",
    "        model=model,\n",
    "        ema_model=ema_model,\n",
    "        emb_model=emb_model,\n",
    "        prob_list=prob_list,\n",
    "        criteria_x=criteria_x,\n",
    "        optim=optim,\n",
    "        lr_schdlr=lr_schdlr,\n",
    "        dltrain_x=dltrain_x,\n",
    "        dltrain_u=dltrain_u,\n",
    "        args=args,\n",
    "        n_iters=n_iters_per_epoch,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    best_acc = -1\n",
    "    best_epoch = 0\n",
    "\n",
    "    if metrics is not None:\n",
    "        best_acc = metrics['best_acc']\n",
    "        best_epoch = metrics['best_epoch']\n",
    "    logger.info('-----------start training--------------')\n",
    "    for epoch in range(start_epoch, args[\"n_epoches\"]):\n",
    "        \n",
    "        loss_x, loss_u, loss_c, mask_mean, num_pos, guess_label_acc, queue_feats, queue_probs, queue_ptr, prob_list = \\\n",
    "        train_one_epoch(epoch, **train_args, queue_feats=queue_feats,queue_probs=queue_probs,queue_ptr=queue_ptr)\n",
    "\n",
    "        top1, ema_top1 = evaluate(model, ema_model, emb_model, dlval)\n",
    "\n",
    "\n",
    "        tb_logger.add_scalar('loss_x', loss_x, epoch)\n",
    "        tb_logger.add_scalar('loss_u', loss_u, epoch)\n",
    "        tb_logger.add_scalar('loss_c', loss_c, epoch)\n",
    "        tb_logger.add_scalar('guess_label_acc', guess_label_acc, epoch)\n",
    "        tb_logger.add_scalar('test_acc', top1, epoch)\n",
    "        tb_logger.add_scalar('test_ema_acc', ema_top1, epoch)\n",
    "        tb_logger.add_scalar('mask', mask_mean, epoch)\n",
    "        tb_logger.add_scalar('num_pos', num_pos, epoch)\n",
    "\n",
    "        if best_acc < top1:\n",
    "            best_acc = top1\n",
    "            best_epoch = epoch\n",
    "\n",
    "        logger.info(\"Epoch {}. Acc: {:.4f}. Ema-Acc: {:.4f}. best_acc: {:.4f} in epoch{}\".\n",
    "                    format(epoch, top1, ema_top1, best_acc, best_epoch))\n",
    "        \n",
    "        save_obj = {\n",
    "            'model': model.state_dict(),\n",
    "            'ema_model': ema_model.state_dict(),\n",
    "            'optimizer': optim.state_dict(),\n",
    "            'lr_scheduler': lr_schdlr.state_dict(),\n",
    "            'prob_list': prob_list,\n",
    "            'queue': {'queue_feats':queue_feats, 'queue_probs':queue_probs, 'queue_ptr':queue_ptr},\n",
    "            'metrics': {'best_acc': best_acc, 'best_epoch': best_epoch},\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(save_obj, os.path.join(output_dir, 'ckp.latest'))\n",
    "    _, _ = evaluate(model, ema_model, emb_model, dlval)\n",
    "\n",
    "    return emb_model, model\n",
    "    \n",
    "    if 'cifar' in args[\"dataset\"].lower():\n",
    "        predictions = predict_cifar(model, ema_model, emb_model, dltrain_x, dltrain_u, dlval)\n",
    "    elif 'nih' in args[\"dataset\"].lower():\n",
    "        predictions = predict_nih(model, ema_model, emb_model, dltrain_x, dltrain_u, dlval)\n",
    "\n",
    "    logger.info(\"***** Generate Predictions *****\")\n",
    "    if not os.path.exists('./artificial_expert_labels/'):\n",
    "        os.makedirs('./artificial_expert_labels/')\n",
    "    pred_file = f'{args[\"exp_dir\"]}_{args[\"dataset\"].lower()}_expert{args[\"ex_strength\"]}.{args[\"seed\"]}@{args[\"n_labeled\"]}_predictions.json'\n",
    "    with open(f'artificial_expert_labels/{pred_file}', 'w') as f:\n",
    "        json.dump(predictions, f)\n",
    "    with open(os.getcwd()[:-len('Embedding-Semi-Supervised')]+f'Learning-to-Defer-Algs/artificial_expert_labels/{pred_file}', 'w') as f:\n",
    "        json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f352c697-048b-4e6e-926b-959b9c4b30ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIH\n",
      "2023-07-03 07:52:40,704 - INFO - train -   {'root': '', 'dataset': 'NIH', 'wresnet_k': 2, 'wresnet_n': 28, 'n_classes': 2, 'n_labeled': 12, 'n_epoches': 10, 'batchsize': 16, 'mu': 7, 'n_imgs_per_epoch': 32768, 'eval_ema': True, 'ema_m': 0.999, 'lam_u': 1.0, 'lr': 0.03, 'weight_decay': 0.0005, 'momentum': 0.9, 'seed': 2, 'temperature': 0.2, 'low_dim': 64, 'lam_c': 1, 'contrast_th': 0.8, 'thr': 0.95, 'alpha': 0.9, 'queue_batch': 5, 'exp_dir': 'EmbeddingCM_bin', 'ex_strength': 4295232296}\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/joli/joli-env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Resnet-18 checkpoint\n",
      "None\n",
      "Loaded Model resnet18\n",
      "2023-07-03 07:52:41,003 - INFO - train -   Total params: 0.30M\n",
      "Index: 0\n",
      "Labels: 8\n",
      "Index: 0\n",
      "Index: 0\n",
      "Found latest checkpoint at NIH/EmbeddingCM_bin/ex4295232296_x12_seed2/ckp.latest\n",
      "Continuing in epoch 11\n",
      "2023-07-03 07:52:41,527 - INFO - train -   -----------start training--------------\n"
     ]
    }
   ],
   "source": [
    "emb_model, model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e34ffb1c-adb1-4a8f-b64c-2f0ec1c76235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearNN(\n",
       "  (linear_layers): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (l2norm): Normalize()\n",
       "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (relu_mlp): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af01ef3d-4193-449b-a52e-84dc7c1039c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resnet(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7e45b-e288-4036-896e-83031f855c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joli-env_kernel",
   "language": "python",
   "name": "joli-env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
