{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bf74b-0389-4fba-9d69-712ac6c786a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from absl import flags\n",
    "from absl import app\n",
    "\n",
    "from feature_extractor.utils import save_to_logs, get_train_dir\n",
    "from feature_extractor.emb_model_lib import EmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd71abf-a04e-41e0-86eb-b3572f739174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561de50e-5c03-4722-b614-9001dff0405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"L2D\": { # Parameter for Learning to defer\n",
    "        \"maxLabels\": 16,\n",
    "    },\n",
    "    \"TARGET\": \"Airspace_Opacity\",\n",
    "    \"PATH\": \"../Datasets/NIH/\",\n",
    "    \"K\": 10, #Number of folds\n",
    "    \"LABELER_IDS\": [4323195249, 4295232296],\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"alpha\": 1.0, #scaling parameter for the loss function, default=1.0\n",
    "    \"epochs\": 150,\n",
    "    \"patience\": 50, #number of patience steps for early stopping the training\n",
    "    \"expert_type\": \"MLPMixer\", #specify the expert type. For the type of experts available, see-> models -> experts. defualt=predict\n",
    "    \"n_classes\": 2, #K for K class classification\n",
    "    \"k\": 0, #\n",
    "    \"n_experts\": 2, #\n",
    "    \"lr\": 0.001, #learning rate\n",
    "    \"weight_decay\": 5e-4, #\n",
    "    \"warmup_epochs\": 5, #\n",
    "    \"loss_type\": \"softmax\", #surrogate loss type for learning to defer\n",
    "    \"ckp_dir\": \"./Models\", #directory name to save the checkpoints\n",
    "    \"experiment_name\": \"multiple_experts\", #specify the experiment name. Checkpoints will be saved with this name\n",
    "    #\n",
    "    \"TRAIN_BATCH_SIZE\": 64,\n",
    "    \"TEST_BATCH_SIZE\": 64,\n",
    "    \"NUM_EXPERTS\": 2,\n",
    "\n",
    "    \"GT\": True, # Determines if the classifier gets all data with GT Label or only the labeld data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b56f93a-4581-4ae8-aa4d-9419ffb247a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run = neptune.init_run(\\n    project=config_neptune[\"project\"],\\n    api_token=config_neptune[\"api_token\"],\\n    custom_run_id=\"AI\"\\n)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neptune\n",
    "\n",
    "import json\n",
    "\n",
    "with open('neptune_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config_neptune = config[\"neptune\"]\n",
    "\"\"\"run = neptune.init_run(\n",
    "    project=config_neptune[\"project\"],\n",
    "    api_token=config_neptune[\"api_token\"],\n",
    "    custom_run_id=\"AI\"\n",
    ")\"\"\"\n",
    "\n",
    "basic_Dataset = ds.BasicDataset(param[\"PATH\"], param[\"TARGET\"])\n",
    "nih_dataloader = ds.NIH_K_Fold_Dataloader(\n",
    "            dataset = basic_Dataset,\n",
    "            k = param[\"K\"],\n",
    "            labelerIds = param[\"LABELER_IDS\"],\n",
    "            train_batch_size = param[\"TRAIN_BATCH_SIZE\"],\n",
    "            test_batch_size = param[\"TEST_BATCH_SIZE\"],\n",
    "            param = param\n",
    "        )\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = nih_dataloader.get_data_loader_for_fold(1)\n",
    "dataloaders = (train_dataloader, val_dataloader, test_dataloader)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c892c57-f6ed-47b3-9a23-e355bbcaa96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset': \"nih\",\n",
    "    'model': \"resnet18\",\n",
    "    'num_classes': 2,\n",
    "    'batch': 128,\n",
    "    'lr': 0.001,\n",
    "}\n",
    "\n",
    "path = \"../../../Datasets/NIH/\"\n",
    "\n",
    "wkdir = os.getcwd()\n",
    "sys.path.append(wkdir)\n",
    "    \n",
    "# get training directory\n",
    "train_dir = get_train_dir(wkdir, args, 'emb_net')\n",
    "\n",
    "print(\"Train dir: \" + train_dir)\n",
    "# initialize summary writer for tensorboard\n",
    "writer = SummaryWriter(train_dir + 'logs/')\n",
    "\n",
    "# initialize base model\n",
    "emb_model = EmbeddingModel(args, wkdir, writer, dataloaders, param)\n",
    "# try to load previous training runs\n",
    "start_epoch = emb_model.load_from_checkpoint(mode='latest')\n",
    "# train model\n",
    "for epoch in range(start_epoch, 10):\n",
    "    # train one epoch\n",
    "    loss = emb_model.train_one_epoch(epoch)\n",
    "    # get validation accuracy\n",
    "    valid_acc = emb_model.get_test_accuracy(return_acc=True)\n",
    "    print(f'loss: {loss}')\n",
    "    # save logs to json\n",
    "    save_to_logs(train_dir, valid_acc, loss.item())\n",
    "    # save model to checkpoint\n",
    "    emb_model.save_to_checkpoint(epoch, loss, valid_acc)\n",
    "# get test accuracy\n",
    "emb_model.get_test_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joli-env_kernel",
   "language": "python",
   "name": "joli-env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
