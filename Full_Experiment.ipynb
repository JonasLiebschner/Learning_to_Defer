{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c923862-4c78-46d2-87e8-a617e07c32d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 14:03:30.255359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import sklearn\n",
    "import copy\n",
    "\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "#import Verma.experts as vexp\n",
    "import Verma.losses as vlos\n",
    "from Verma.utils import AverageMeter, accuracy\n",
    "import Verma.resnet50 as vres\n",
    "from AL.utils import *\n",
    "from AL.metrics import *\n",
    "\n",
    "import Dataset.Dataset as ds\n",
    "\n",
    "import ssl_functions as ssl\n",
    "import active_learning as al\n",
    "from active_learning import NIHExpertDatasetMemory\n",
    "\n",
    "import expert as expert_module\n",
    "import verma as verm\n",
    "import hemmer as hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9104208e-aaa1-40e2-b3c6-1058f92fd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, fold=None, text=None):\n",
    "    if fold is not None and text is not None:\n",
    "        s = text + f\" + {seed} + {fold}\"\n",
    "        seed = int(hashlib.sha256(s.encode('utf-8')).hexdigest(), 16) % 10**8\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e071303-6599-454d-a430-c2b39165f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "import json\n",
    "\n",
    "with open('neptune_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config_neptune = config[\"neptune\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc80118e-ff9d-491c-b393-94569d4e4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def cleanTrainDir(path):\n",
    "    shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6272b79-d261-46c6-bc8e-d5b6fa8dc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertModelSSL_AL(dataManager, expert, labelerId, param=None, seed=None, fold=None, learning_mod=\"ssl\", prediction_type=\"target\"):\n",
    "\n",
    "    if param[\"SETTING\"] == \"SSL_AL_SSL\":\n",
    "        learning_type = \"ssl\"\n",
    "    elif param[\"SETTING\"] == \"SSL_AL\":\n",
    "        learning_type = \"sl\"\n",
    "\n",
    "    assert learning_type != \"\", \"Need to define how experts should be trained with new AL data (sl or ssl)\"\n",
    "    \n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "    train_dataset = ds.NIHDataset(expert_train, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "    usedFilenames = sslDataset.getLabeledFilenames(labelerId, fold)\n",
    "    \n",
    "    # initialize data, Erhält alle Indizes der Daten\n",
    "    all_indices = list(range(len(train_dataset.getAllIndices())))\n",
    "    all_data_filenames = np.array(train_dataset.getAllFilenames())[all_indices]\n",
    "    all_data_y = np.array(train_dataset.getAllTargets())[all_indices]\n",
    "\n",
    "    used_indices = [index for index in all_indices if all_data_filenames[index] in usedFilenames]\n",
    "    indices = used_indices\n",
    "\n",
    "    print(\"Len overlapping used indices: \" + str(len(used_indices)))\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    met = al.testExpert(expert, val_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "    metrics[\"Val\"] = {\n",
    "        \"Start\": met,\n",
    "    }\n",
    "\n",
    "    met = al.testExpert(expert, test_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "    metrics[\"Test\"] = {\n",
    "        \"Start\": met,\n",
    "    }\n",
    "\n",
    "    metrics[\"Train\"] = {}\n",
    "\n",
    "    set_seed(seed, fold, text=\"\")\n",
    "    \n",
    "    Intial_random_set = indices\n",
    "    indices_labeled  = Intial_random_set\n",
    "    indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
    "\n",
    "    # Lädt die Datasets für die beschrifteten und unbeschrifteten Daten\n",
    "    dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], None , [0]*len(indices_unlabeled), indices_unlabeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    dataset_val_unlabeled = NIHExpertDatasetMemory(None, val_dataset.getAllFilenames(), np.array(val_dataset.getAllTargets()), expert.predict , [1]*len(val_dataset.getAllIndices()), val_dataset.getAllIndices(), param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    \n",
    "    # Lädt die Dataloaders\n",
    "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    dataLoaderValUnlabeled = DataLoader(dataset=dataset_val_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    for round in range(param[\"AL\"][\"ROUNDS\"]):\n",
    "\n",
    "        print(f'\\n \\n Round {round} \\n \\n')\n",
    "\n",
    "        # get points where expert model is least confident on\n",
    "        indices_confidence = al.get_least_confident_points(expert, dataLoaderTrainUnlabeled, param[\"AL\"][\"LABELS_PER_ROUND\"], mod=\"ssl\")\n",
    "        indices_labeled  = indices_labeled + list(indices_confidence) \n",
    "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
    "\n",
    "        dataset_train_labeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_labeled], all_data_y[indices_labeled], expert.predict , [1]*len(indices_labeled), indices_labeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "        dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], expert.predict , [0]*len(indices_unlabeled), indices_unlabeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "\n",
    "        dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        if learning_type == \"ssl\": #If the experts should be trained with ssl\n",
    "            sslDataset = dataManager.getSSLDataset(seed)\n",
    "            sslDataset.addNewLabels(all_data_filenames[list(indices_confidence)], fold, expert.labelerId)\n",
    "            emb_model, model = ssl.getExpertModelSSL(labelerId=expert.labelerId, sslDataset=sslDataset, seed=seed, fold_idx=fold, n_labeled=None, embedded_model=None, param=param, neptune_param=param[\"NEPTUNE\"], added_epochs=(round+1)*3)\n",
    "            expert.setModel(expert_module.SSLModel(emb_model, model), mod=\"SSL\")\n",
    "\n",
    "\n",
    "            #TODO: Test experts and get metrics\n",
    "            n_images = param[\"AL\"][\"INITIAL_SIZE\"] + (round+1)*param[\"AL\"][\"LABELS_PER_ROUND\"]\n",
    "\n",
    "            train_metrics = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderTrainLabeled, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, step=\"Train\", param=param, mod=\"ssl\", prediction_type=\"target\", print_result=False)\n",
    "            val_metrics = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderValUnlabeled, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, step=\"Val\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "\n",
    "            metrics[\"Train\"][n_images] = {\n",
    "                \"train_metrics\": train_metrics,\n",
    "                \"val_metrics\": val_metrics,\n",
    "            }\n",
    "\n",
    "        elif learning_type == \"sl\": #supervised learning\n",
    "        \n",
    "            # train model on labeled data\n",
    "            dataloaders = (dataLoaderTrainLabeled, dataLoaderValUnlabeled)\n",
    "            n_images = param[\"AL\"][\"INITIAL_SIZE\"] + (round+1)*param[\"AL\"][\"LABELS_PER_ROUND\"]\n",
    "            train_metrics, val_metrics = al.run_expert(model=None, expert=expert, epochs=param[\"AL\"][\"EPOCH_TRAIN\"], dataloaders=dataloaders, param=param, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, mod=\"ssl\", prediction_type=\"target\")\n",
    "        \n",
    "            metrics[\"Train\"][n_images] = {\n",
    "                \"train_metrics\": train_metrics,\n",
    "                \"val_metrics\": val_metrics,\n",
    "            }\n",
    "    \n",
    "    dataset_test_unlabeled = NIHExpertDatasetMemory(None, test_dataset.getAllFilenames(), np.array(test_dataset.getAllTargets()), expert.predict , [1]*len(test_dataset.getAllIndices()), test_dataset.getAllIndices(), param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    dataLoaderVal = DataLoader(dataset=dataset_test_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    met_test = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderVal, id=expert.labelerId, seed=seed, fold=fold, n_images=param[\"AL\"][\"INITIAL_SIZE\"] + param[\"AL\"][\"ROUNDS\"]*param[\"AL\"][\"LABELS_PER_ROUND\"], step=\"Test\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "\n",
    "    met = al.testExpert(expert, val_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "    metrics[\"Val\"][\"End\"] = met\n",
    "\n",
    "    met = al.testExpert(expert, test_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "    metrics[\"Test\"][\"End\"] = met\n",
    "    \n",
    "    #metrics[\"Test\"] = met\n",
    "    print(\"AL finished\")\n",
    "    return met_test, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc438de-b6b0-463c-95ae-c62afdbbc6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertModelsSSL_AL(dataManager, experts, param, seed, fold, learning_mod=\"ssl\", prediction_type=\"target\"):\n",
    "\n",
    "    if param[\"SETTING\"] == \"SSL_AL_SSL\":\n",
    "        learning_type = \"ssl\"\n",
    "    elif param[\"SETTING\"] == \"SSL_AL\":\n",
    "        learning_type = \"sl\"\n",
    "\n",
    "    assert learning_type != \"\", \"Need to define how experts should be trained with new AL data (sl or ssl)\"\n",
    "\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "    train_dataset = ds.NIHDataset(expert_train, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "    usedFilenames = []\n",
    "    for labelerId in param[\"LABELER_IDS\"]:\n",
    "        temp = usedFilenames + sslDataset.getLabeledFilenames(labelerId, fold)\n",
    "    usedFilenames = temp\n",
    "    \n",
    "    \n",
    "    # initialize data, Erhält alle Indizes der Daten\n",
    "    all_indices = list(range(len(train_dataset.getAllIndices())))\n",
    "    all_data_filenames = np.array(train_dataset.getAllFilenames())[all_indices]\n",
    "    all_data_y = np.array(train_dataset.getAllTargets())[all_indices]\n",
    "\n",
    "    unused_indices = [index for index in all_indices if all_data_filenames[index] not in usedFilenames]\n",
    "    \n",
    "    metrics = {}\n",
    "    for labelerId, expert in experts.items():\n",
    "        metrics[labelerId] = {}\n",
    "\n",
    "        met = al.testExpert(expert, val_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "        metrics[labelerId][\"Val\"] = {\n",
    "            \"Start\": met,\n",
    "        }\n",
    "\n",
    "        met = al.testExpert(expert, test_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "        metrics[labelerId][\"Test\"] = {\n",
    "            \"Start\": met,\n",
    "        }\n",
    "\n",
    "        metrics[labelerId][\"Train\"] = {}\n",
    "\n",
    "    set_seed(seed, fold, text=\"\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    indices_unlabeled = unused_indices\n",
    "    indices_labeled = list(set(all_indices) - set(indices_unlabeled))\n",
    "\n",
    "    dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], experts[param[\"LABELER_IDS\"][0]].predict , [0]*len(indices_unlabeled), indices_unlabeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    for round in range(param[\"AL\"][\"ROUNDS\"]):\n",
    "\n",
    "        print(f'\\n \\n Round {round} \\n \\n')\n",
    "\n",
    "        #Try to get better Points\n",
    "        if param[\"MOD\"] == \"disagreement\":\n",
    "            indices_qbq = al.getQbQPoints(experts, dataLoaderTrainUnlabeled, param[\"AL\"][\"LABELS_PER_ROUND\"], mod=\"ssl\", param=param)\n",
    "        if param[\"MOD\"] == \"disagreement_diff\":\n",
    "            indices_qbq = al.getQbQPointsDifference(experts, dataLoaderTrainUnlabeled, param[\"AL\"][\"LABELS_PER_ROUND\"], mod=\"ssl\", param=param)\n",
    "        \n",
    "        #indices_labeled  = indices_labeled + list(indices_confidence) \n",
    "        indices_labeled  = indices_labeled + list(indices_qbq) \n",
    "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))     \n",
    "        \n",
    "        # train model on labeled data\n",
    "        for labelerId, expert in experts.items():\n",
    "\n",
    "            #Val Dataset, needed for SSL and AL\n",
    "            dataset_val_unlabeled = NIHExpertDatasetMemory(None, val_dataset.getAllFilenames(), np.array(val_dataset.getAllTargets()), expert.predict , [1]*len(val_dataset.getAllIndices()), val_dataset.getAllIndices(), param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "            dataLoaderValUnlabeled = DataLoader(dataset=dataset_val_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "            #Create train dataset\n",
    "            dataset_train_labeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_labeled], all_data_y[indices_labeled], expert.predict , [1]*len(indices_labeled), indices_labeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "            dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "            if learning_type == \"ssl\": #If the experts should be trained with ssl\n",
    "                sslDataset = dataManager.getSSLDataset(seed)\n",
    "                sslDataset.addNewLabels(all_data_filenames[list(indices_qbq)], fold, labelerId)\n",
    "                emb_model, model = ssl.getExpertModelSSL(labelerId=labelerId, sslDataset=sslDataset, seed=seed, fold_idx=fold, n_labeled=None, embedded_model=None, param=param, neptune_param=param[\"NEPTUNE\"], added_epochs=(round+1)*2)\n",
    "                experts[labelerId].setModel(expert_module.SSLModel(emb_model, model), mod=\"SSL\")\n",
    "\n",
    "\n",
    "                #TODO: Test experts and get metrics\n",
    "                n_images = param[\"AL\"][\"INITIAL_SIZE\"] + (round+1)*param[\"AL\"][\"LABELS_PER_ROUND\"]\n",
    "\n",
    "                train_metrics = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderTrainLabeled, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, step=\"Train\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "                val_metrics = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderValUnlabeled, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, step=\"Val\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "\n",
    "                metrics[labelerId][\"Train\"][n_images] = {\n",
    "                    \"train_metrics\": train_metrics,\n",
    "                    \"val_metrics\": val_metrics,\n",
    "                }\n",
    "\n",
    "                \n",
    "            elif learning_type == \"sl\": # If the experts sould be trained with supervised learning\n",
    "\n",
    "                dataloaders = (dataLoaderTrainLabeled, dataLoaderValUnlabeled)\n",
    "                n_images = param[\"AL\"][\"INITIAL_SIZE\"] + (round+1)*param[\"AL\"][\"LABELS_PER_ROUND\"]\n",
    "                train_metrics, val_metrics = al.run_expert(model=None, expert=expert, epochs=param[\"AL\"][\"EPOCH_TRAIN\"], dataloaders=dataloaders, param=param, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, mod=\"ssl\", prediction_type=\"target\")\n",
    "\n",
    "                metrics[labelerId][\"Train\"][n_images] = {\n",
    "                    \"train_metrics\": train_metrics,\n",
    "                    \"val_metrics\": val_metrics\n",
    "                }\n",
    "        \n",
    "        dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], expert.predict , [0]*len(indices_unlabeled), indices_unlabeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    dataset_test_unlabeled = NIHExpertDatasetMemory(None, test_dataset.getAllFilenames(), np.array(test_dataset.getAllTargets()), expert.predict , [1]*len(test_dataset.getAllIndices()), test_dataset.getAllIndices(), param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    dataLoaderVal = DataLoader(dataset=dataset_test_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    met_test = {}\n",
    "    for labelerId, expert in experts.items():\n",
    "        temp = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderVal, id=expert.labelerId, seed=seed, fold=fold, n_images=param[\"AL\"][\"INITIAL_SIZE\"] + param[\"AL\"][\"ROUNDS\"]*param[\"AL\"][\"LABELS_PER_ROUND\"], step=\"Test\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "        met_test[expert.labelerId] = temp\n",
    "\n",
    "        met = al.testExpert(expert, val_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "        metrics[labelerId][\"Val\"][\"End\"] = met\n",
    "\n",
    "        met = al.testExpert(expert, test_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "        metrics[labelerId][\"Test\"][\"End\"] = met\n",
    "        \n",
    "    return met_test, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be36f3a5-859c-4d0a-956c-8ec5b8f57383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsSSL_AL(dataManager, param, fold, seed):\n",
    "\n",
    "    mod = \"ssl\"\n",
    "    prediction_type = param[\"EXPERT_PREDICT\"]\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "\n",
    "    sslDataset.createLabeledIndices(labelerIds=param[\"LABELER_IDS\"], n_L=param[\"AL\"][\"INITIAL_SIZE\"], k=round(param[\"AL\"][\"INITIAL_SIZE\"]*param[\"OVERLAP\"]/100), seed=seed, sample_equal=param[\"SAMPLE_EQUAL\"])\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = sslDataset.get_data_loader_for_fold(fold)\n",
    "    dataloaders = (train_dataloader, val_dataloader, test_dataloader)\n",
    "\n",
    "    embedded_model = ssl.create_embedded_model(dataloaders, param, param[\"NEPTUNE\"], fold=fold, seed=seed)\n",
    "\n",
    "    experts = {}\n",
    "    for labelerId in param[\"LABELER_IDS\"]:\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId)\n",
    "        emb_model, model = ssl.getExpertModelSSL(labelerId=labelerId, sslDataset=sslDataset, seed=seed, fold_idx=fold, n_labeled=None, embedded_model=embedded_model, param=param, neptune_param=param[\"NEPTUNE\"])\n",
    "        nih_expert.setModel(expert_module.SSLModel(emb_model, model), mod=\"SSL\")\n",
    "        experts[labelerId] = nih_expert\n",
    "    metrics = {}\n",
    "    if param[\"MOD\"] == \"confidence\":\n",
    "        for i, labelerId in enumerate(param[\"LABELER_IDS\"]):\n",
    "            met, metrics_return = getExpertModelSSL_AL(dataManager=dataManager, expert=experts[labelerId], labelerId=labelerId, param=param, seed=seed, fold=fold, learning_mod=\"ssl\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "            metrics[labelerId] = metrics_return\n",
    "    elif param[\"MOD\"] == \"disagreement\" or param[\"MOD\"] == \"disagreement_diff\":\n",
    "        met, metrics = getExpertModelsSSL_AL(dataManager, experts, param, seed, fold, learning_mod=\"ssl\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "        \n",
    "    return experts, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff5079e-b28c-42c8-a5e0-379f31fdd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsSSL(dataManager, param, fold, seed):\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "\n",
    "    mod = \"ssl\"\n",
    "    prediction_type = param[\"EXPERT_PREDICT\"]\n",
    "\n",
    "    sslDataset.createLabeledIndices(labelerIds=param[\"LABELER_IDS\"], n_L=param[\"LABELED\"], k=round(param[\"LABELED\"]*param[\"OVERLAP\"]/100), seed=seed, sample_equal=param[\"SAMPLE_EQUAL\"])\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = sslDataset.get_data_loader_for_fold(fold)\n",
    "    dataloaders = (train_dataloader, val_dataloader, test_dataloader)\n",
    "\n",
    "    ssl.create_embedded_model(dataloaders, param, param[\"NEPTUNE\"], fold=fold, seed=seed)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    experts = {}\n",
    "    for labelerId in param[\"LABELER_IDS\"]:\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId)\n",
    "        emb_model, model = ssl.getExpertModelSSL(labelerId=labelerId, sslDataset=sslDataset, seed=seed, fold_idx=fold, n_labeled=None, embedded_model=None, param=param, neptune_param=param[\"NEPTUNE\"])\n",
    "        nih_expert.setModel(expert_module.SSLModel(emb_model, model), mod=\"SSL\")\n",
    "        experts[labelerId] = nih_expert\n",
    "\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "\n",
    "    val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    metrics = {}\n",
    "    for labelerId, expert in experts.items():\n",
    "        metrics[labelerId] = {}\n",
    "\n",
    "        met = al.testExpert(expert, val_dataset, image_container, param, mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "        metrics[labelerId][\"Val\"] = {\n",
    "            \"End\": met,\n",
    "        }\n",
    "\n",
    "        met = al.testExpert(expert, test_dataset, image_container, param, mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "        metrics[labelerId][\"Test\"] = {\n",
    "            \"End\": met\n",
    "        }\n",
    "        \n",
    "    return experts, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f339e5-b0be-466d-b520-ac2f9680c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupEmbeddedModel(dataManager, param, fold, seed):\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "\n",
    "    mod = \"ssl\"\n",
    "    prediction_type = param[\"EXPERT_PREDICT\"]\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = sslDataset.get_data_loader_for_fold(fold)\n",
    "    dataloaders = (train_dataloader, val_dataloader, test_dataloader)\n",
    "\n",
    "    ssl.create_embedded_model(dataloaders, param, param[\"NEPTUNE\"], fold=fold, seed=seed)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f63a5829-4507-4ce4-bb06-c99ea126c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsAL(dataManager, param, fold_idx, seed):\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold_idx)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "    expert_train_dataset = ds.NIHDataset(expert_train, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    expert_val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    expert_test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    setupEmbeddedModel(dataManager, param, fold_idx, seed)\n",
    "    #Get init labeled indices with k same images and n-k different images\n",
    "    #k=None means random indieces\n",
    "    k = param[\"OVERLAP\"]\n",
    "    all_indices = list(range(len(expert_train_dataset.getAllIndices())))\n",
    "    #If no k is set than it selects one randomly\n",
    "    k = round(param[\"AL\"][\"INITIAL_SIZE\"]*k/100)\n",
    "    if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "        run[\"param/overlap_k\"] = k\n",
    "    indices = al.sampleIndices(n = param[\"AL\"][\"INITIAL_SIZE\"], k = k, all_indices = all_indices, experten = list(param[\"LABELER_IDS\"]), seed = seed, fold=fold_idx)\n",
    "\n",
    "    if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "        run[f\"Seed_{seed}/Fold_{fold_idx}/Experts/Indices\"] = indices\n",
    "\n",
    "    print(\"Random indices:\")\n",
    "    print(indices)\n",
    "\n",
    "    experts = {}\n",
    "    metrics = {}\n",
    "    for i, labelerId in enumerate(list(param[\"LABELER_IDS\"])):\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId)\n",
    "        experts[labelerId] = nih_expert\n",
    "        if param[\"MOD\"] == \"confidence\":\n",
    "            expert_model, met_test, metric = al.getExpertModel(indices[labelerId], expert_train_dataset, expert_val_dataset, expert_test_dataset, nih_expert, param, seed, fold_idx, image_container=image_container, learning_mod=\"al\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "            nih_expert.setModel(expert_model, mod=\"AL\")\n",
    "            metrics[labelerId] = metric\n",
    "    if param[\"MOD\"] == \"disagreement\" or param[\"MOD\"]==\"disagreement_diff\":\n",
    "        expert_models, met, metrics = al.getExpertModels(indices, experts, expert_train_dataset, expert_val_dataset, expert_test_dataset, param, seed, fold_idx, mod=param[\"MOD\"], image_container=image_container, learning_mod=\"al\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "        for labelerId, expert in experts.items():\n",
    "            expert.setModel(expert_models[labelerId], mod=\"AL\")\n",
    "\n",
    "    return experts, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d265f97f-07f8-493d-8de5-aa0d60b99eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsNormal(dataManager, param, fold_idx, seed):\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold_idx)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "    expert_train_dataset = ds.NIHDataset(expert_train, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    expert_val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    expert_test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    setupEmbeddedModel(dataManager, param, fold_idx, seed)\n",
    "    \n",
    "    #Get init labeled indices with k same images and n-k different images\n",
    "    #k=None means random indieces\n",
    "    k = param[\"OVERLAP\"]\n",
    "    all_indices = list(range(len(expert_train_dataset.getAllIndices())))\n",
    "    #If no k is set than it selects one randomly\n",
    "    k = round(param[\"LABELED\"]*k/100)\n",
    "    if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "        run[\"param/overlap_k\"] = k\n",
    "    indices = al.sampleIndices(n = param[\"LABELED\"], k = k, all_indices = all_indices, experten = list(param[\"LABELER_IDS\"]), seed = seed, fold=fold_idx)\n",
    "\n",
    "    if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "        run[f\"Seed_{seed}/Fold_{fold_idx}/Experts/Indices\"] = indices\n",
    "\n",
    "    print(\"Random indices:\")\n",
    "    print(indices)\n",
    "\n",
    "    experts = {}\n",
    "    #Create the experts\n",
    "    metrics = {}\n",
    "    for i, labelerId in enumerate(list(param[\"LABELER_IDS\"])):\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId)\n",
    "        experts[labelerId] = nih_expert\n",
    "\n",
    "        model, met, metric = al.getExpertModelNormal(indices[labelerId], expert_train_dataset, expert_val_dataset, expert_test_dataset, nih_expert, param, seed, fold_idx, image_container=image_container, learning_mod=\"al\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "        nih_expert.setModel(model, mod=\"AL\")\n",
    "        metrics[labelerId] = metric\n",
    "\n",
    "    return experts, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39124366-b8e9-4c8d-89d2-f06a5b698ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsPerfect(dataManager, param, fold, seed):\n",
    "\n",
    "    experts = {}\n",
    "    for i, labelerId in enumerate(list(param[\"LABELER_IDS\"])):\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId)\n",
    "        experts[labelerId] = nih_expert\n",
    "\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "\n",
    "    mod = \"perfect\"\n",
    "    prediction_type = param[\"EXPERT_PREDICT\"]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "\n",
    "    val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    metrics = {}\n",
    "    for labelerId, expert in experts.items():\n",
    "        metrics[labelerId] = {}\n",
    "\n",
    "        met = al.testExpert(expert, val_dataset, image_container, param, mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "        metrics[labelerId][\"Val\"] = {\n",
    "            \"End\": met,\n",
    "        }\n",
    "\n",
    "        met = al.testExpert(expert, test_dataset, image_container, param, mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "        metrics[labelerId][\"Test\"] = {\n",
    "            \"End\": met\n",
    "        }\n",
    "\n",
    "    return experts, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1de88e94-d4cf-475b-bd7b-8c3f0c2eb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExperts(dataManager, param, seed, fold):\n",
    "      \n",
    "    #Creates expert models for the choosen method\n",
    "    if param[\"SETTING\"] == \"PERFECT\":\n",
    "        experts, metrics = getExpertsPerfect(dataManager, param, fold, seed)\n",
    "    if param[\"SETTING\"] == \"AL\":\n",
    "        experts, metrics = getExpertsAL(dataManager, param, fold, seed)\n",
    "    elif param[\"SETTING\"] == \"SSL\":\n",
    "        experts, metrics = getExpertsSSL(dataManager, param, fold, seed)\n",
    "    elif param[\"SETTING\"] == \"SSL_AL\" or param[\"SETTING\"] == \"SSL_AL_SSL\":\n",
    "        experts, metrics = getExpertsSSL_AL(dataManager, param, fold, seed)\n",
    "    elif param[\"SETTING\"] == \"NORMAL\":\n",
    "        experts, metrics = getExpertsNormal(dataManager, param, fold, seed)\n",
    "\n",
    "    return experts, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36779aaf-d0a9-4116-8888-d629701db78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2D_Verma(train_loader, val_loader, test_loader, full_dataloader, expert_fns, param, seed, fold_idx, experts):\n",
    "    num_experts = len(expert_fns)\n",
    "            \n",
    "    model = model = vres.ResNet50_defer(int(param[\"n_classes\"]) + num_experts)\n",
    "\n",
    "    metrics_train_all, metrics_val_all, metrics_test, metrics_full, metrics_pretrain_all = verm.train(model, train_loader, val_loader, test_loader, expert_fns, param, seed=seed, experts=experts, \n",
    "                                                                                fold=fold_idx, full_dataloader=full_dataloader, param=param)\n",
    "\n",
    "    return metrics_train_all, metrics_val_all, metrics_test, metrics_full, metrics_pretrain_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39c730b2-f947-44cf-95f5-aaf92081a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_run(dataManager, run_param):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    expert_metrics = {}\n",
    "    verma_metrics = {}\n",
    "    hemmer_metrics = {}\n",
    "\n",
    "    for seed in run_param[\"SEEDS\"]:\n",
    "        print(\"Seed: \" + str(seed))\n",
    "        \n",
    "\n",
    "        expert_metrics[seed] = {}\n",
    "        verma_metrics[seed] = {}\n",
    "        hemmer_metrics[seed] = {}\n",
    "\n",
    "        if os.path.isdir('SSL_Working'):\n",
    "            cleanTrainDir(\"SSL_Working\")\n",
    "\n",
    "        #for fold_idx in range(run_param[\"K\"]):\n",
    "        for fold_idx in range(4):\n",
    "\n",
    "            if seed != \"\":\n",
    "                set_seed(seed, fold_idx, text=\"\")\n",
    "\n",
    "            print(\"/n\")\n",
    "            print(f\"Seed: {seed} - Fold: {fold_idx} \\n\")\n",
    "\n",
    "            if os.path.isdir('SSL_Working/NIH/EmbeddingCM_bin'):\n",
    "                cleanTrainDir(\"SSL_Working/NIH/EmbeddingCM_bin\")\n",
    "\n",
    "            neptune = {\n",
    "                \"SEED\": seed,\n",
    "                \"FOLD\": fold_idx,\n",
    "            }\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            experts, expert_metric = getExperts(dataManager, run_param, seed, fold_idx)\n",
    "            expert_metrics[seed][fold_idx] = expert_metric\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            #print(f\"Got {len(experts)} experts\")\n",
    "\n",
    "            nih_dataloader = dataManager.getKFoldDataloader(seed=seed)\n",
    "\n",
    "            train_loader, val_loader, test_loader = nih_dataloader.get_data_loader_for_fold(fold_idx)\n",
    "            full_dataloader = nih_dataloader.getFullDataloader()\n",
    "\n",
    "            expert_fns = []\n",
    "            print(run_param[\"SETTING\"])\n",
    "            for labelerId, expert in experts.items():\n",
    "                if run_param[\"SETTING\"] == \"AL\":\n",
    "                    expert.init_model_predictions(full_dataloader, mod=\"AL\")\n",
    "                    expert_fns.append(expert.predict_model_predefined_al)\n",
    "                elif run_param[\"SETTING\"] == \"SSL\":\n",
    "                    expert.init_model_predictions(full_dataloader, mod=\"SSL\")\n",
    "                    expert_fns.append(expert.predict_model_predefined_ssl)\n",
    "                elif (run_param[\"SETTING\"] == \"SSL_AL\" or run_param[\"SETTING\"] == \"SSL_AL_SSL\"):\n",
    "                    expert.init_model_predictions(full_dataloader, mod=\"SSL\")\n",
    "                    expert_fns.append(expert.predict_model_predefined_ssl)\n",
    "                elif run_param[\"SETTING\"] == \"NORMAL\":\n",
    "                    expert.init_model_predictions(full_dataloader, mod=\"AL\")\n",
    "                    expert_fns.append(expert.predict_model_predefined_al)\n",
    "                elif run_param[\"SETTING\"] == \"PERFECT\":\n",
    "                    expert_fns.append(expert.predict)\n",
    "\n",
    "            metrics_train_all, metrics_val_all, metrics_test_all, metrics_full_all, metrics_pretrain_all = L2D_Verma(train_loader, val_loader, test_loader, full_dataloader, expert_fns, run_param, seed, fold_idx, experts=experts)\n",
    "\n",
    "            verma_metrics[seed][fold_idx] = {\n",
    "                \"train\": metrics_train_all,\n",
    "                \"val\": metrics_val_all,\n",
    "                \"test\": metrics_test_all,\n",
    "                \"full\": metrics_full_all,\n",
    "                \"pretrain\": metrics_pretrain_all,\n",
    "            }\n",
    "            \n",
    "            system_accuracy, classifier_coverage, all_train_metrics, all_val_metrics, all_test_metrics, all_full_metrics = hm.L2D_Hemmer(train_loader, val_loader, test_loader, full_dataloader, expert_fns, run_param, seed, fold_idx, experts)\n",
    "\n",
    "            hemmer_metrics[seed][fold_idx] = {\n",
    "                \"train\": all_train_metrics,\n",
    "                \"val\": all_val_metrics,\n",
    "                \"test\": all_test_metrics,\n",
    "                \"full\": all_full_metrics,\n",
    "            }\n",
    "\n",
    "            \n",
    "    return expert_metrics, verma_metrics, hemmer_metrics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20baf936-e327-4025-94e4-9a06d2cbacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(param):\n",
    "    run_param = copy.deepcopy(param)\n",
    "\n",
    "    expert_metrics_all = []\n",
    "\n",
    "    with open('Metrics_Folder/Metrics_97.pickle', 'rb') as handle:\n",
    "        expert_metrics_all = pickle.load(handle)\n",
    "\n",
    "    runs = [{i:run[i] for i in run if i not in [\"expert metrics\", \"verma\", \"hemmer\"]} for run in expert_metrics_all]\n",
    "\n",
    "    count = 98\n",
    "\n",
    "    #Every pair of labeler ids\n",
    "    for labeler_ids in param[\"LABELER_IDS\"]:\n",
    "        run_param[\"LABELER_IDS\"] = labeler_ids\n",
    "        run_param[\"labeler_ids\"] = convert_ids_to_string(labeler_ids)\n",
    "        \n",
    "\n",
    "        dataManager = ds.DataManager(path=param[\"PATH\"], target=param[\"TARGET\"], param=run_param, seeds=param[\"SEEDS\"])\n",
    "        dataManager.createData()\n",
    "\n",
    "        for init_size in param[\"AL\"][\"INITIAL_SIZE\"]:\n",
    "            run_param[\"AL\"][\"INITIAL_SIZE\"] = init_size\n",
    "\n",
    "            for labels_per_round in param[\"AL\"][\"LABELS_PER_ROUND\"]:\n",
    "                run_param[\"AL\"][\"LABELS_PER_ROUND\"] = labels_per_round\n",
    "\n",
    "                for rounds in param[\"AL\"][\"ROUNDS\"]:\n",
    "                    run_param[\"AL\"][\"ROUNDS\"] = rounds\n",
    "\n",
    "                    labeled = init_size + rounds * labels_per_round\n",
    "\n",
    "                    run_param[\"LABELED\"] = labeled\n",
    "\n",
    "                    if (labeled >= 128): #Prevents from large amount of data\n",
    "                        continue\n",
    "\n",
    "                    for cost in param[\"AL\"][\"COST\"]:\n",
    "                        run_param[\"AL\"][\"COST\"] = cost\n",
    "                        run_param[\"AL\"][\"cost\"] = convert_cost_to_string(cost)\n",
    "\n",
    "                        for overlap in param[\"OVERLAP\"]:\n",
    "                            run_param[\"OVERLAP\"] = overlap\n",
    "\n",
    "                            for setting in param[\"SETTING\"]:\n",
    "                                run_param[\"SETTING\"] = setting\n",
    "                        \n",
    "                                for mod in param[\"MOD\"]:\n",
    "                                    run_param[\"MOD\"] = mod\n",
    "\n",
    "                                    if ((setting == \"AL\"  or setting==\"SSL_AL\" or setting==\"SSL_AL_SSL\") and (mod not in [\"confidence\", \"disagreement\", \"disagreement_diff\"])):\n",
    "                                        continue\n",
    "\n",
    "                                    if (setting == \"SSL\" and mod != \"ssl\"):\n",
    "                                        continue\n",
    "\n",
    "                                    if (setting == \"NORMAL\" and mod != \"normal\"):\n",
    "                                        continue\n",
    "\n",
    "                                    for expert_predict in param[\"EXPERT_PREDICT\"]:\n",
    "                                        run_param[\"EXPERT_PREDICT\"] = expert_predict\n",
    "\n",
    "                                        if ((setting == \"SSL\" or setting == \"SSL_AL\" or setting == \"SSL_AL_SSL\") and (expert_predict == \"right\")):\n",
    "                                            continue\n",
    "\n",
    "                                        if (expert_predict == \"target\") and (cost != param[\"AL\"][\"COST\"][0]):\n",
    "                                            continue\n",
    "                                        if (expert_predict == \"target\"):\n",
    "                                            run_param[\"AL\"][\"cost\"] = convert_cost_to_string((0, 0))\n",
    "\n",
    "                                        for sample_equal in param[\"SAMPLE_EQUAL\"]:\n",
    "                                            run_param[\"SAMPLE_EQUAL\"] = sample_equal\n",
    "\n",
    "                                            for epochs_pretrain in param[\"epochs_pretrain\"]:\n",
    "                                                run_param[\"epochs_pretrain\"] = epochs_pretrain\n",
    "\n",
    "                                                metrics_save = {}\n",
    "                                                metrics_save[\"labeler_ids\"] = labeler_ids\n",
    "                                                metrics_save[\"init_size\"] = init_size\n",
    "                                                metrics_save[\"labels_per_round\"] = labels_per_round\n",
    "                                                metrics_save[\"rounds\"] = rounds\n",
    "                                                metrics_save[\"labeled\"] = labeled\n",
    "                                                metrics_save[\"cost\"] = cost\n",
    "                                                metrics_save[\"overlap\"] = overlap\n",
    "                                                metrics_save[\"setting\"] = setting\n",
    "                                                metrics_save[\"mod\"] = mod\n",
    "                                                metrics_save[\"expert_predict\"] = expert_predict\n",
    "                                                metrics_save[\"sample_equal\"] = sample_equal\n",
    "                                                metrics_save[\"epochs_pretrain\"] = epochs_pretrain\n",
    "\n",
    "                                                if metrics_save in runs:\n",
    "                                                    continue\n",
    "                                    \n",
    "                            \n",
    "                                                NEPTUNE = param[\"NEPTUNE\"][\"NEPTUNE\"]\n",
    "                                                if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "                                                    global run\n",
    "                                                    run = neptune.init_run(\n",
    "                                                        project=config_neptune[\"project\"],\n",
    "                                                        api_token=config_neptune[\"api_token\"],\n",
    "                                                        #custom_run_id=\"AL_\" + \n",
    "                                                    )\n",
    "                                                    run[\"param\"] = run_param\n",
    "                                                    run_param[\"NEPTUNE\"][\"RUN\"] = run\n",
    "\n",
    "                                                print(\"\\n #####################################################################################\")\n",
    "                                                print(\"\\n \\n \\n NEW RUN \\n\")\n",
    "                                                print(\"Initial size: \" + str(init_size))\n",
    "                                                print(\"Batch size: \" + str(labels_per_round))\n",
    "                                                print(\"Max rounds: \" + str(rounds))\n",
    "                                                print(\"Labeled: \" + str(labeled))\n",
    "                                                print(\"Cost: \" + str(cost))\n",
    "                                                print(\"Setting: \" + str(setting))\n",
    "                                                print(\"Mod: \" + str(mod))\n",
    "                                                print(\"Overlap: \" + str(overlap))\n",
    "                                                print(\"Prediction Type \" + str(expert_predict))\n",
    "                                                print(\"Sample equal \" + str(sample_equal))\n",
    "                                                print(\"epochs_pretrain \" + str(epochs_pretrain))\n",
    "\n",
    "                                            \n",
    "\n",
    "\n",
    "                                                start_time = time.time()\n",
    "                                                expert_metrics, verma_metrics, hemmer_metrics = one_run(dataManager, \n",
    "                                                                                                                                                                                                 run_param)\n",
    "                                                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "                                                metrics_save[\"expert metrics\"] = expert_metrics\n",
    "                                                metrics_save[\"verma\"] = verma_metrics\n",
    "                                                metrics_save[\"hemmer\"] = hemmer_metrics\n",
    "                                                expert_metrics_all.append(metrics_save)\n",
    "                                                with open(f'Metrics_Folder/Metrics_{count}.pickle', 'wb') as handle:\n",
    "                                                    pickle.dump(expert_metrics_all, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                                count += 1\n",
    "                                                if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "                                                    run[\"metrics\"] = metrics_save\n",
    "\n",
    "                                                    run.stop()\n",
    "\n",
    "    return expert_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f63b284-8933-4876-8875-3ec3c9b35a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cost_to_string(tp):\n",
    "    return \"(\" + str(tp[0]) + \", \" + str(tp[1]) + \")\"\n",
    "\n",
    "def convert_ids_to_string(ids):\n",
    "    return f\"{ids[0]}, {ids[1]}\"\n",
    "\n",
    "def convert_list_to_string(li):\n",
    "    result = \"[\"\n",
    "    for el in li[:-2]:\n",
    "        result = result + str(el)\n",
    "    result = result + \"]\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9b435-2f48-4591-abfb-d484668b4534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dde2cce-c1ce-488a-9f5c-f9560d0d9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"PATH\": \"../Datasets/NIH/\",\n",
    "    \"TARGET\": \"Airspace_Opacity\",\n",
    "    \"LABELER_IDS\": [[4323195249, 4295232296]],\n",
    "    \"K\": 10, #Number of folds\n",
    "    \"SEEDS\": [1, 2, 3], #Seeds for the experiments\n",
    "    \"GT\": True, # Determines if the classifier gets all data with GT Label or only the labeld data\n",
    "    \"MOD\": [\"confidence\", \"disagreement\", \"disagreement_diff\", \"ssl\", \"normal\"], #Determines the experiment modus\n",
    "    #\"MOD\": [\"confidence\", \"ssl\", \"normal\"],\n",
    "\n",
    "    \"OVERLAP\": [0, 100],\n",
    "    \"SAMPLE_EQUAL\": [False, True],\n",
    "    #\"INITAL_SIZE\": [8, 16, 32],\n",
    "    #\"ROUNDS\": [2, 4, 8],\n",
    "    #\"LABELS_PER_ROUND\": [4, 8, 16],\n",
    "\n",
    "    \"SETTING\": [\"AL\", \"SSL\", \"SSL_AL\", \"NORMAL\", \"SSL_AL_SSL\"],\n",
    "    #\"SETTING\": [\"SSL_AL\"],\n",
    "\n",
    "    \"NUM_EXPERTS\": 2,\n",
    "    \"NUM_CLASSES\": 2,\n",
    "\n",
    "    \"EXPERT_PREDICT\": [\"right\", \"target\"],\n",
    "\n",
    "    \"AL\": { #Parameter for Active Learning\n",
    "        \"INITIAL_SIZE\": [8, 16, 32], #\n",
    "        \"EPOCH_TRAIN\": 40, #\n",
    "        \"n_dataset\": 2, #Number Classes\n",
    "        \"BATCH_SIZE\": 4,\n",
    "        \"BATCH_SIZE_VAL\": 32,\n",
    "        \"ROUNDS\": [2, 4, 8],\n",
    "        \"LABELS_PER_ROUND\": [4, 8, 16],\n",
    "        \"EPOCHS_DEFER\": 10,\n",
    "        \"COST\": [(0, 0), (5, 0)], #Cost for Cost sensitiv learning\n",
    "        #\"TRAIN REJECTOR\": False,\n",
    "        \"PRELOAD\": True,\n",
    "        \"PREPROCESS\": True,\n",
    "        \n",
    "    },\n",
    "    \"SSL\": {\n",
    "        \"PREBUILD\": False,\n",
    "        #\"TRAIN_BATCH_SIZE\": 128,\n",
    "        \"TRAIN_BATCH_SIZE\": 254,\n",
    "        \"TEST_BATCH_SIZE\": 254,\n",
    "        \"N_EPOCHS\": 5, #number of training epoches\n",
    "        \"BATCHSIZE\": 16, #train batch size of labeled samples\n",
    "        #\"N_IMGS_PER_EPOCH\": 32768, #number of training images for each epoch\n",
    "        \"N_IMGS_PER_EPOCH\": 4381*1, #number of training images for each epoch\n",
    "    },\n",
    "    \"L2D\": { # Parameter for Learning to defer\n",
    "        \"TRAIN_BATCH_SIZE\": 128,\n",
    "        \"TEST_BATCH_SIZE\": 128,\n",
    "        \"PRELOAD\": True,\n",
    "        \"PREBUILD\": True,\n",
    "        \"EPOCHS\": 50,\n",
    "        \"VERMA\": {},\n",
    "        \"HEMMER\": {\n",
    "            \"EPOCHS\": 50,\n",
    "            \"LR\": 5e-3,\n",
    "            \"USE_LR_SCHEDULER\": False,\n",
    "            \"DROPOUT\": 0.00,\n",
    "            \"NUM_HIDDEN_UNITS\": 30,\n",
    "        },\n",
    "        \n",
    "    },\n",
    "    \"NEPTUNE\": {\n",
    "        \"NEPTUNE\": True,\n",
    "    },\n",
    "    \"EMBEDDED\": {\n",
    "        \"ARGS\": {\n",
    "            'dataset': \"nih\",\n",
    "            'model': \"resnet50\",\n",
    "            'num_classes': 2,\n",
    "            'batch': 128,\n",
    "            'lr': 0.001,\n",
    "        },\n",
    "        \"EPOCHS\": 30,\n",
    "    },\n",
    "    \n",
    "    \n",
    "    \"epochs_pretrain\": [0],\n",
    "    \"batch_size\": 64,\n",
    "    \"alpha\": 1.0, #scaling parameter for the loss function, default=1.0\n",
    "    \"epochs\": 50,\n",
    "    \"patience\": 25, #number of patience steps for early stopping the training\n",
    "    \"expert_type\": \"MLPMixer\", #specify the expert type. For the type of experts available, see-> models -> experts. defualt=predict\n",
    "    \"n_classes\": 2, #K for K class classification\n",
    "    \"k\": 0, #\n",
    "    \"n_experts\": 2, #\n",
    "    \"lr\": 0.001, #learning rate\n",
    "    \"weight_decay\": 5e-4, #\n",
    "    \"warmup_epochs\": 5, #\n",
    "    #\"loss_type\": \"softmax\", #surrogate loss type for learning to defer\n",
    "    \"loss_type\": \"ova\",\n",
    "    \"ckp_dir\": \"./Models\", #directory name to save the checkpoints\n",
    "    \"experiment_name\": \"multiple_experts\", #specify the experiment name. Checkpoints will be saved with this name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "373cf849-73ea-48c8-8beb-f874308f0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5ad03-8442-4120-80cb-b0daa8902444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images of the whole dataset: 4381\n",
      "Loaded image number: 0\n",
      "Loaded image number: 200\n",
      "Loaded image number: 400\n",
      "Loaded image number: 600\n",
      "Loaded image number: 800\n",
      "Loaded image number: 1000\n",
      "Loaded image number: 1200\n",
      "Loaded image number: 1400\n",
      "Loaded image number: 1600\n",
      "Loaded image number: 1800\n",
      "Loaded image number: 2000\n",
      "Loaded image number: 2200\n",
      "Loaded image number: 2400\n",
      "Loaded image number: 2600\n",
      "Loaded image number: 2800\n",
      "Loaded image number: 3000\n",
      "Loaded image number: 3200\n",
      "Loaded image number: 3400\n",
      "Loaded image number: 3600\n",
      "Loaded image number: 3800\n",
      "Loaded image number: 4000\n",
      "Loaded image number: 4200\n",
      "Full length: 4381\n",
      "Loaded image number: 0\n",
      "Loaded image number: 200\n",
      "Loaded image number: 400\n",
      "Loaded image number: 600\n",
      "Loaded image number: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Loaded set number 0\n",
      "Loaded set number 1\n",
      "Loaded set number 2\n",
      "Loaded set number 3\n",
      "Loaded set number 4\n",
      "Loaded set number 5\n",
      "Loaded set number 6\n",
      "Loaded set number 7\n",
      "Loaded set number 8\n",
      "Loaded set number 9\n",
      "Full length: 4381\n",
      "Loaded image number: 0\n",
      "Loaded image number: 200\n",
      "Loaded image number: 400\n",
      "Loaded image number: 600\n",
      "Loaded image number: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Loaded set number 0\n",
      "Loaded set number 1\n",
      "Loaded set number 2\n",
      "Loaded set number 3\n",
      "Loaded set number 4\n",
      "Loaded set number 5\n",
      "Loaded set number 6\n",
      "Loaded set number 7\n",
      "Loaded set number 8\n",
      "Loaded set number 9\n",
      "Full length: 4381\n",
      "Loaded image number: 0\n",
      "Loaded image number: 200\n",
      "Loaded image number: 400\n",
      "Loaded image number: 600\n",
      "Loaded image number: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/home/joli/joli-env/lib/python3.9/site-packages/neptune/internal/utils/git.py:56: UserWarning: GitPython could not be initialized\n",
      "  warnings.warn(\"GitPython could not be initialized\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Loaded set number 0\n",
      "Loaded set number 1\n",
      "Loaded set number 2\n",
      "Loaded set number 3\n",
      "Loaded set number 4\n",
      "Loaded set number 5\n",
      "Loaded set number 6\n",
      "Loaded set number 7\n",
      "Loaded set number 8\n",
      "Loaded set number 9\n",
      "https://app.neptune.ai/jonasl/masterarbeit/e/MAS-463\n",
      "\n",
      " #####################################################################################\n",
      "\n",
      " \n",
      " \n",
      " NEW RUN \n",
      "\n",
      "Initial size: 8\n",
      "Batch size: 4\n",
      "Max rounds: 2\n",
      "Labeled: 16\n",
      "Cost: (0, 0)\n",
      "Setting: SSL_AL_SSL\n",
      "Mod: confidence\n",
      "Overlap: 0\n",
      "Prediction Type target\n",
      "Sample equal False\n",
      "epochs_pretrain 0\n",
      "Seed: 1\n",
      "/n\n",
      "Seed: 1 - Fold: 0 \n",
      "\n",
      "Train dir: /home/joli/Masterarbeit/SSL_Working/NIH/emb_net@dataset-nih-model-resnet50-num_classes-2/\n",
      "load Resnet-50 checkpoint\n",
      "load Resnet-50 pretrained on ImageNet\n",
      "Loaded Model resnet50\n",
      "24\n",
      "No Checkpoint found\n",
      "Starting new from epoch 1\n",
      "ii 12 Epoch 1: |█████████████████████-------------------| 54.2% Complete\n",
      "Test-Accuracy: 0.459954233409611 \n",
      "Test-Acc-Class [0.58810573 0.32142857]\n",
      "loss: 0.6936516761779785\n",
      "ii 12 Epoch 2: |█████████████████████-------------------| 54.2% Complete\n",
      "Test-Accuracy: 0.5205949656750573 \n",
      "Test-Acc-Class [0.67400881 0.3547619 ]\n",
      "loss: 0.676196813583374\n"
     ]
    }
   ],
   "source": [
    "expert_metrics_all = run_experiment(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74125d0e-7586-4ca5-9d54-43f8c0bd8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Store data (serialize)\n",
    "with open('Metrics.pickle', 'wb') as handle:\n",
    "    pickle.dump(expert_metrics_all, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load data (deserialize)\n",
    "#with open('filename.pickle', 'rb') as handle:\n",
    "#    unserialized_data = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joli-env_kernel",
   "language": "python",
   "name": "joli-env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
