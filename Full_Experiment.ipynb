{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c923862-4c78-46d2-87e8-a617e07c32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import sklearn\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "#import Verma.experts as vexp\n",
    "import Verma.losses as vlos\n",
    "from Verma.utils import AverageMeter, accuracy\n",
    "import Verma.resnet50 as vres\n",
    "from AL.utils import *\n",
    "from AL.metrics import *\n",
    "\n",
    "import Dataset.Dataset as ds\n",
    "\n",
    "import ssl_functions as ssl\n",
    "import active_learning as al\n",
    "from active_learning import NIHExpertDatasetMemory\n",
    "\n",
    "import expert as expert_module\n",
    "import verma as verm\n",
    "import hemmer as hm\n",
    "\n",
    "import neptune\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9104208e-aaa1-40e2-b3c6-1058f92fd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, fold=None, text=None):\n",
    "    if fold is not None and text is not None:\n",
    "        s = text + f\" + {seed} + {fold}\"\n",
    "        seed = int(hashlib.sha256(s.encode('utf-8')).hexdigest(), 16) % 10**8\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e071303-6599-454d-a430-c2b39165f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "import json\n",
    "\n",
    "with open('neptune_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config_neptune = config[\"neptune\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc80118e-ff9d-491c-b393-94569d4e4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def cleanTrainDir(path):\n",
    "    shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e6272b79-d261-46c6-bc8e-d5b6fa8dc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertModelSSL_AL(dataManager, expert, labelerId, param=None, seed=None, fold=None, learning_mod=\"ssl\", prediction_type=\"target\"):\n",
    "\n",
    "    if param[\"SETTING\"] == \"SSL_AL_SSL\":\n",
    "        learning_type = \"ssl\"\n",
    "    elif param[\"SETTING\"] == \"SSL_AL\":\n",
    "        learning_type = \"sl\"\n",
    "\n",
    "    assert learning_type != \"\", \"Need to define how experts should be trained with new AL data (sl or ssl)\"\n",
    "    \n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "    train_dataset = ds.NIHDataset(expert_train, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "    usedFilenames = sslDataset.getLabeledFilenames(labelerId, fold)\n",
    "    \n",
    "    # initialize data, Erhält alle Indizes der Daten\n",
    "    all_indices = list(range(len(train_dataset.getAllIndices())))\n",
    "    all_data_filenames = np.array(train_dataset.getAllFilenames())[all_indices]\n",
    "    all_data_y = np.array(train_dataset.getAllTargets())[all_indices]\n",
    "\n",
    "    used_indices = [index for index in all_indices if all_data_filenames[index] in usedFilenames]\n",
    "    indices = used_indices\n",
    "\n",
    "    print(\"Len overlapping used indices: \" + str(len(used_indices)))\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    met = al.testExpert(expert, val_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "    metrics[\"Val\"] = {\n",
    "        \"Start\": met,\n",
    "    }\n",
    "\n",
    "    met = al.testExpert(expert, test_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "    metrics[\"Test\"] = {\n",
    "        \"Start\": met,\n",
    "    }\n",
    "\n",
    "    metrics[\"Train\"] = {}\n",
    "\n",
    "    set_seed(seed, fold, text=\"\")\n",
    "    \n",
    "    Intial_random_set = indices\n",
    "    indices_labeled  = Intial_random_set\n",
    "    indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
    "\n",
    "    # Lädt die Datasets für die beschrifteten und unbeschrifteten Daten\n",
    "    dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], None , [0]*len(indices_unlabeled), indices_unlabeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    dataset_val_unlabeled = NIHExpertDatasetMemory(None, val_dataset.getAllFilenames(), np.array(val_dataset.getAllTargets()), expert.predict , [1]*len(val_dataset.getAllIndices()), val_dataset.getAllIndices(), param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    \n",
    "    # Lädt die Dataloaders\n",
    "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "    dataLoaderValUnlabeled = DataLoader(dataset=dataset_val_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "    \n",
    "    for round in range(param[\"AL\"][\"ROUNDS\"]):\n",
    "\n",
    "        print(f'\\n \\n Round {round} \\n \\n')\n",
    "\n",
    "        # get points where expert model is least confident on\n",
    "        indices_confidence = al.get_least_confident_points(expert, dataLoaderTrainUnlabeled, param[\"AL\"][\"LABELS_PER_ROUND\"], mod=\"ssl\")\n",
    "        indices_labeled  = indices_labeled + list(indices_confidence) \n",
    "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
    "\n",
    "        dataset_train_labeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_labeled], all_data_y[indices_labeled], expert.predict , [1]*len(indices_labeled), indices_labeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "        dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], expert.predict , [0]*len(indices_unlabeled), indices_unlabeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "\n",
    "        dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "\n",
    "        if learning_type == \"ssl\": #If the experts should be trained with ssl\n",
    "            sslDataset = dataManager.getSSLDataset(seed)\n",
    "            sslDataset.addNewLabels(all_data_filenames[list(indices_confidence)], fold, expert.labelerId)\n",
    "            emb_model, model = ssl.getExpertModelSSL(labelerId=expert.labelerId, sslDataset=sslDataset, seed=seed, fold_idx=fold, n_labeled=None, embedded_model=None, param=param, neptune_param=param[\"NEPTUNE\"], added_epochs=(round+1)*param[\"AL\"][\"SSL_EPOCHS\"])\n",
    "            expert.setModel(expert_module.SSLModel(emb_model, model), mod=\"SSL\")\n",
    "\n",
    "\n",
    "            #TODO: Test experts and get metrics\n",
    "            n_images = param[\"AL\"][\"INITIAL_SIZE\"] + (round+1)*param[\"AL\"][\"LABELS_PER_ROUND\"]\n",
    "\n",
    "            train_metrics = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderTrainLabeled, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, step=\"Train\", param=param, mod=\"ssl\", prediction_type=\"target\", print_result=False)\n",
    "            val_metrics = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderValUnlabeled, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, step=\"Val\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "\n",
    "            metrics[\"Train\"][n_images] = {\n",
    "                \"train_metrics\": train_metrics,\n",
    "                \"val_metrics\": val_metrics,\n",
    "            }\n",
    "\n",
    "        elif learning_type == \"sl\": #supervised learning\n",
    "        \n",
    "            # train model on labeled data\n",
    "            dataloaders = (dataLoaderTrainLabeled, dataLoaderValUnlabeled)\n",
    "            n_images = param[\"AL\"][\"INITIAL_SIZE\"] + (round+1)*param[\"AL\"][\"LABELS_PER_ROUND\"]\n",
    "            train_metrics, val_metrics = al.run_expert(model=None, expert=expert, epochs=param[\"AL\"][\"EPOCH_TRAIN\"], dataloaders=dataloaders, param=param, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, mod=\"ssl\", prediction_type=\"target\")\n",
    "        \n",
    "            metrics[\"Train\"][n_images] = {\n",
    "                \"train_metrics\": train_metrics,\n",
    "                \"val_metrics\": val_metrics,\n",
    "            }\n",
    "    \n",
    "    dataset_test_unlabeled = NIHExpertDatasetMemory(None, test_dataset.getAllFilenames(), np.array(test_dataset.getAllTargets()), expert.predict , [1]*len(test_dataset.getAllIndices()), test_dataset.getAllIndices(), param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    dataLoaderVal = DataLoader(dataset=dataset_test_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "    met_test = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderVal, id=expert.labelerId, seed=seed, fold=fold, n_images=param[\"AL\"][\"INITIAL_SIZE\"] + param[\"AL\"][\"ROUNDS\"]*param[\"AL\"][\"LABELS_PER_ROUND\"], step=\"Test\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "\n",
    "    met = al.testExpert(expert, val_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "    metrics[\"Val\"][\"End\"] = met\n",
    "\n",
    "    met = al.testExpert(expert, test_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "    metrics[\"Test\"][\"End\"] = met\n",
    "    \n",
    "    #metrics[\"Test\"] = met\n",
    "    print(\"AL finished\")\n",
    "    return met_test, metrics, all_data_filenames[indices_labeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cbc438de-b6b0-463c-95ae-c62afdbbc6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertModelsSSL_AL(dataManager, experts, param, seed, fold, learning_mod=\"ssl\", prediction_type=\"target\"):\n",
    "\n",
    "    if param[\"SETTING\"] == \"SSL_AL_SSL\":\n",
    "        learning_type = \"ssl\"\n",
    "    elif param[\"SETTING\"] == \"SSL_AL\":\n",
    "        learning_type = \"sl\"\n",
    "\n",
    "    assert learning_type != \"\", \"Need to define how experts should be trained with new AL data (sl or ssl)\"\n",
    "\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "    train_dataset = ds.NIHDataset(expert_train, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "    usedFilenames = []\n",
    "    for labelerId in param[\"LABELER_IDS\"]:\n",
    "        temp = usedFilenames + sslDataset.getLabeledFilenames(labelerId, fold)\n",
    "    usedFilenames = temp\n",
    "    \n",
    "    \n",
    "    # initialize data, Erhält alle Indizes der Daten\n",
    "    all_indices = list(range(len(train_dataset.getAllIndices())))\n",
    "    all_data_filenames = np.array(train_dataset.getAllFilenames())[all_indices]\n",
    "    all_data_y = np.array(train_dataset.getAllTargets())[all_indices]\n",
    "\n",
    "    unused_indices = [index for index in all_indices if all_data_filenames[index] not in usedFilenames]\n",
    "    \n",
    "    metrics = {}\n",
    "    for labelerId, expert in experts.items():\n",
    "        metrics[labelerId] = {}\n",
    "\n",
    "        met = al.testExpert(expert, val_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "        metrics[labelerId][\"Val\"] = {\n",
    "            \"Start\": met,\n",
    "        }\n",
    "\n",
    "        met = al.testExpert(expert, test_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "        metrics[labelerId][\"Test\"] = {\n",
    "            \"Start\": met,\n",
    "        }\n",
    "\n",
    "        metrics[labelerId][\"Train\"] = {}\n",
    "\n",
    "    set_seed(seed, fold, text=\"\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    indices_unlabeled = unused_indices\n",
    "    indices_labeled = list(set(all_indices) - set(indices_unlabeled))\n",
    "\n",
    "    dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], experts[param[\"LABELER_IDS\"][0]].predict , [0]*len(indices_unlabeled), indices_unlabeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "    \n",
    "    for round in range(param[\"AL\"][\"ROUNDS\"]):\n",
    "\n",
    "        print(f'\\n \\n Round {round} \\n \\n')\n",
    "\n",
    "        #Try to get better Points\n",
    "        if param[\"MOD\"] == \"disagreement\":\n",
    "            indices_qbq = al.getQbQPoints(experts, dataLoaderTrainUnlabeled, param[\"AL\"][\"LABELS_PER_ROUND\"], mod=\"ssl\", param=param)\n",
    "        if param[\"MOD\"] == \"disagreement_diff\":\n",
    "            indices_qbq = al.getQbQPointsDifference(experts, dataLoaderTrainUnlabeled, param[\"AL\"][\"LABELS_PER_ROUND\"], mod=\"ssl\", param=param)\n",
    "        \n",
    "        #indices_labeled  = indices_labeled + list(indices_confidence) \n",
    "        indices_labeled  = indices_labeled + list(indices_qbq) \n",
    "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))     \n",
    "        \n",
    "        # train model on labeled data\n",
    "        for labelerId, expert in experts.items():\n",
    "\n",
    "            #Val Dataset, needed for SSL and AL\n",
    "            dataset_val_unlabeled = NIHExpertDatasetMemory(None, val_dataset.getAllFilenames(), np.array(val_dataset.getAllTargets()), expert.predict , [1]*len(val_dataset.getAllIndices()), val_dataset.getAllIndices(), param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "            dataLoaderValUnlabeled = DataLoader(dataset=dataset_val_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "\n",
    "            #Create train dataset\n",
    "            dataset_train_labeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_labeled], all_data_y[indices_labeled], expert.predict , [1]*len(indices_labeled), indices_labeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "            dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "\n",
    "            if learning_type == \"ssl\": #If the experts should be trained with ssl\n",
    "                sslDataset = dataManager.getSSLDataset(seed)\n",
    "                sslDataset.addNewLabels(all_data_filenames[list(indices_qbq)], fold, labelerId)\n",
    "                emb_model, model = ssl.getExpertModelSSL(labelerId=labelerId, sslDataset=sslDataset, seed=seed, fold_idx=fold, n_labeled=None, embedded_model=None, param=param, neptune_param=param[\"NEPTUNE\"], added_epochs=(round+1)*param[\"AL\"][\"SSL_EPOCHS\"])\n",
    "                experts[labelerId].setModel(expert_module.SSLModel(emb_model, model), mod=\"SSL\")\n",
    "\n",
    "\n",
    "                #TODO: Test experts and get metrics\n",
    "                n_images = param[\"AL\"][\"INITIAL_SIZE\"] + (round+1)*param[\"AL\"][\"LABELS_PER_ROUND\"]\n",
    "\n",
    "                train_metrics = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderTrainLabeled, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, step=\"Train\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "                val_metrics = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderValUnlabeled, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, step=\"Val\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "\n",
    "                metrics[labelerId][\"Train\"][n_images] = {\n",
    "                    \"train_metrics\": train_metrics,\n",
    "                    \"val_metrics\": val_metrics,\n",
    "                }\n",
    "\n",
    "                \n",
    "            elif learning_type == \"sl\": # If the experts sould be trained with supervised learning\n",
    "\n",
    "                dataloaders = (dataLoaderTrainLabeled, dataLoaderValUnlabeled)\n",
    "                n_images = param[\"AL\"][\"INITIAL_SIZE\"] + (round+1)*param[\"AL\"][\"LABELS_PER_ROUND\"]\n",
    "                train_metrics, val_metrics = al.run_expert(model=None, expert=expert, epochs=param[\"AL\"][\"EPOCH_TRAIN\"], dataloaders=dataloaders, param=param, id=expert.labelerId, seed=seed, fold=fold, n_images=n_images, mod=\"ssl\", prediction_type=\"target\")\n",
    "\n",
    "                metrics[labelerId][\"Train\"][n_images] = {\n",
    "                    \"train_metrics\": train_metrics,\n",
    "                    \"val_metrics\": val_metrics\n",
    "                }\n",
    "        \n",
    "        dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], expert.predict , [0]*len(indices_unlabeled), indices_unlabeled, param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "    \n",
    "    dataset_test_unlabeled = NIHExpertDatasetMemory(None, test_dataset.getAllFilenames(), np.array(test_dataset.getAllTargets()), expert.predict , [1]*len(test_dataset.getAllIndices()), test_dataset.getAllIndices(), param=param, preload=param[\"AL\"][\"PRELOAD\"], image_container=image_container)\n",
    "    dataLoaderVal = DataLoader(dataset=dataset_test_unlabeled, batch_size=param[\"AL\"][\"BATCH_SIZE\"], shuffle=True, num_workers=param[\"num_worker\"], pin_memory=True)\n",
    "    met_test = {}\n",
    "    for labelerId, expert in experts.items():\n",
    "        temp = al.metrics_print_expert(model=None, expert=expert, data_loader=dataLoaderVal, id=expert.labelerId, seed=seed, fold=fold, n_images=param[\"AL\"][\"INITIAL_SIZE\"] + param[\"AL\"][\"ROUNDS\"]*param[\"AL\"][\"LABELS_PER_ROUND\"], step=\"Test\", param=param, mod=\"ssl\", prediction_type=\"target\")\n",
    "        met_test[expert.labelerId] = temp\n",
    "\n",
    "        met = al.testExpert(expert, val_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "        metrics[labelerId][\"Val\"][\"End\"] = met\n",
    "\n",
    "        met = al.testExpert(expert, test_dataset, image_container, param, learning_mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "        metrics[labelerId][\"Test\"][\"End\"] = met\n",
    "        \n",
    "    return met_test, metrics, all_data_filenames[indices_labeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be36f3a5-859c-4d0a-956c-8ec5b8f57383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsSSL_AL(dataManager, param, fold, seed):\n",
    "\n",
    "    mod = \"ssl\"\n",
    "    prediction_type = param[\"EXPERT_PREDICT\"]\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "\n",
    "    sslDataset.createLabeledIndices(labelerIds=param[\"LABELER_IDS\"], n_L=param[\"AL\"][\"INITIAL_SIZE\"], k=round(param[\"AL\"][\"INITIAL_SIZE\"]*param[\"OVERLAP\"]/100), seed=seed, sample_equal=param[\"SAMPLE_EQUAL\"])\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = sslDataset.get_data_loader_for_fold(fold)\n",
    "    dataloaders = (train_dataloader, val_dataloader, test_dataloader)\n",
    "\n",
    "    embedded_model = ssl.create_embedded_model(dataloaders, param, param[\"NEPTUNE\"], fold=fold, seed=seed)\n",
    "\n",
    "    indices = {}\n",
    "    experts = {}\n",
    "    for labelerId in param[\"LABELER_IDS\"]:\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId, modus=\"ssl_al\")\n",
    "        emb_model, model = ssl.getExpertModelSSL(labelerId=labelerId, sslDataset=sslDataset, seed=seed, fold_idx=fold, n_labeled=None, embedded_model=embedded_model, param=param, neptune_param=param[\"NEPTUNE\"])\n",
    "        nih_expert.setModel(expert_module.SSLModel(emb_model, model), mod=\"SSL\")\n",
    "        experts[labelerId] = nih_expert\n",
    "        indices[labelerId] = sslDataset.getLabeledFilenames(labelerId, fold)\n",
    "    metrics = {}\n",
    "    indices_labeled = {}\n",
    "    if param[\"MOD\"] == \"confidence\":\n",
    "        for i, labelerId in enumerate(param[\"LABELER_IDS\"]):\n",
    "            met, metrics_return, labeled = getExpertModelSSL_AL(dataManager=dataManager, expert=experts[labelerId], labelerId=labelerId, param=param, seed=seed, fold=fold, learning_mod=\"ssl\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "            metrics[labelerId] = metrics_return\n",
    "            indices_labeled[labelerId] = labeled\n",
    "    elif param[\"MOD\"] == \"disagreement\" or param[\"MOD\"] == \"disagreement_diff\":\n",
    "        met, metrics, indices_labeled = getExpertModelsSSL_AL(dataManager, experts, param, seed, fold, learning_mod=\"ssl\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "        \n",
    "    return experts, metrics, {\"starting labels\": indices, \"al labels\": indices_labeled}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dff5079e-b28c-42c8-a5e0-379f31fdd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsSSL(dataManager, param, fold, seed):\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "\n",
    "    mod = \"ssl\"\n",
    "    prediction_type = param[\"EXPERT_PREDICT\"]\n",
    "\n",
    "    sslDataset.createLabeledIndices(labelerIds=param[\"LABELER_IDS\"], n_L=param[\"LABELED\"], k=round(param[\"LABELED\"]*param[\"OVERLAP\"]/100), seed=seed, sample_equal=param[\"SAMPLE_EQUAL\"])\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = sslDataset.get_data_loader_for_fold(fold)\n",
    "    dataloaders = (train_dataloader, val_dataloader, test_dataloader)\n",
    "\n",
    "    ssl.create_embedded_model(dataloaders, param, param[\"NEPTUNE\"], fold=fold, seed=seed)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    indices = {}\n",
    "\n",
    "    experts = {}\n",
    "    for labelerId in param[\"LABELER_IDS\"]:\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId, modus=\"ssl\")\n",
    "        emb_model, model = ssl.getExpertModelSSL(labelerId=labelerId, sslDataset=sslDataset, seed=seed, fold_idx=fold, n_labeled=None, embedded_model=None, param=param, neptune_param=param[\"NEPTUNE\"])\n",
    "        nih_expert.setModel(expert_module.SSLModel(emb_model, model), mod=\"SSL\")\n",
    "        experts[labelerId] = nih_expert\n",
    "        indices[labelerId] = sslDataset.getLabeledFilenames(labelerId, fold)\n",
    "\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "\n",
    "    val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    metrics = {}\n",
    "    for labelerId, expert in experts.items():\n",
    "        metrics[labelerId] = {}\n",
    "\n",
    "        met = al.testExpert(expert, val_dataset, image_container, param, mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "        metrics[labelerId][\"Val\"] = {\n",
    "            \"End\": met,\n",
    "        }\n",
    "\n",
    "        met = al.testExpert(expert, test_dataset, image_container, param, mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "        metrics[labelerId][\"Test\"] = {\n",
    "            \"End\": met\n",
    "        }\n",
    "        \n",
    "    return experts, metrics, {\"starting labels\": indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "44f339e5-b0be-466d-b520-ac2f9680c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupEmbeddedModel(dataManager, param, fold, seed):\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "\n",
    "    mod = \"ssl\"\n",
    "    prediction_type = param[\"EXPERT_PREDICT\"]\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = sslDataset.get_data_loader_for_fold(fold)\n",
    "    dataloaders = (train_dataloader, val_dataloader, test_dataloader)\n",
    "\n",
    "    ssl.create_embedded_model(dataloaders, param, param[\"NEPTUNE\"], fold=fold, seed=seed)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f63a5829-4507-4ce4-bb06-c99ea126c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsAL(dataManager, param, fold_idx, seed):\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold_idx)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "    expert_train_dataset = ds.NIHDataset(expert_train, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    expert_val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    expert_test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    setupEmbeddedModel(dataManager, param, fold_idx, seed)\n",
    "    #Get init labeled indices with k same images and n-k different images\n",
    "    #k=None means random indieces\n",
    "    k = param[\"OVERLAP\"]\n",
    "    all_indices = list(range(len(expert_train_dataset.getAllIndices())))\n",
    "    #If no k is set than it selects one randomly\n",
    "    k = round(param[\"AL\"][\"INITIAL_SIZE\"]*k/100)\n",
    "    if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "        run[\"param/overlap_k\"] = k\n",
    "    indices = al.sampleIndices(n = param[\"AL\"][\"INITIAL_SIZE\"], k = k, all_indices = all_indices, experten = list(param[\"LABELER_IDS\"]), seed = seed, fold=fold_idx)\n",
    "\n",
    "    if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "        run[f\"Seed_{seed}/Fold_{fold_idx}/Experts/Indices\"] = indices\n",
    "\n",
    "    print(\"Random indices:\")\n",
    "    print(indices)\n",
    "\n",
    "    labeld_filenames = {}\n",
    "\n",
    "    indeces_al = {}\n",
    "\n",
    "    experts = {}\n",
    "    metrics = {}\n",
    "    for i, labelerId in enumerate(list(param[\"LABELER_IDS\"])):\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId, modus=\"al\")\n",
    "        experts[labelerId] = nih_expert\n",
    "        print(\"DELETE ME\")\n",
    "        print(\"Drawn indices\")\n",
    "        print(indices)\n",
    "        print(f\"Len of all filenames: {len(expert_train_dataset.getAllFilenames())}\")\n",
    "        print(\"All indices\")\n",
    "        print(expert_train_dataset.getAllIndices())\n",
    "        labeld_filenames[labelerId] = np.array(expert_train_dataset.getAllFilenames())[indices[labelerId]]\n",
    "        if param[\"MOD\"] == \"confidence\":\n",
    "            expert_model, met_test, metric, indices_labeled = al.getExpertModel(indices[labelerId], expert_train_dataset, expert_val_dataset, expert_test_dataset, nih_expert, param, seed, fold_idx, image_container=image_container, learning_mod=\"al\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "            nih_expert.setModel(expert_model, mod=\"AL\")\n",
    "            metrics[labelerId] = metric\n",
    "            indeces_al[labelerId] = indices_labeled\n",
    "    if param[\"MOD\"] == \"disagreement\" or param[\"MOD\"]==\"disagreement_diff\":\n",
    "        expert_models, met, metrics, indeces_al = al.getExpertModels(indices, experts, expert_train_dataset, expert_val_dataset, expert_test_dataset, param, seed, fold_idx, mod=param[\"MOD\"], image_container=image_container, learning_mod=\"al\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "        for labelerId, expert in experts.items():\n",
    "            expert.setModel(expert_models[labelerId], mod=\"AL\")\n",
    "\n",
    "    return experts, metrics, {\"starting labels\": labeld_filenames, \"al labels\": indeces_al}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d265f97f-07f8-493d-8de5-aa0d60b99eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsNormal(dataManager, param, fold_idx, seed):\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold_idx)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "    expert_train_dataset = ds.NIHDataset(expert_train, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    expert_val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    expert_test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    setupEmbeddedModel(dataManager, param, fold_idx, seed)\n",
    "    \n",
    "    #Get init labeled indices with k same images and n-k different images\n",
    "    #k=None means random indieces\n",
    "    k = param[\"OVERLAP\"]\n",
    "    all_indices = list(range(len(expert_train_dataset.getAllIndices())))\n",
    "    #If no k is set than it selects one randomly\n",
    "    k = round(param[\"LABELED\"]*k/100)\n",
    "    if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "        run[\"param/overlap_k\"] = k\n",
    "    indices = al.sampleIndices(n = param[\"LABELED\"], k = k, all_indices = all_indices, experten = list(param[\"LABELER_IDS\"]), seed = seed, fold=fold_idx)\n",
    "\n",
    "    if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "        run[f\"Seed_{seed}/Fold_{fold_idx}/Experts/Indices\"] = indices\n",
    "\n",
    "    print(\"Random indices:\")\n",
    "    print(indices)\n",
    "\n",
    "    labeled_filenames = {}\n",
    "\n",
    "    experts = {}\n",
    "    #Create the experts\n",
    "    metrics = {}\n",
    "    for i, labelerId in enumerate(list(param[\"LABELER_IDS\"])):\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId, modus=\"normal\")\n",
    "        experts[labelerId] = nih_expert\n",
    "        labeld_filenames[labelerId] = np.array(expert_train_dataset.getAllFilenames())[indices[labelerId]]\n",
    "\n",
    "        model, met, metric = al.getExpertModelNormal(indices[labelerId], expert_train_dataset, expert_val_dataset, expert_test_dataset, nih_expert, param, seed, fold_idx, image_container=image_container, learning_mod=\"al\", prediction_type=param[\"EXPERT_PREDICT\"])\n",
    "        nih_expert.setModel(model, mod=\"AL\")\n",
    "        metrics[labelerId] = metric\n",
    "\n",
    "    return experts, metrics, {\"starting labels\": labeled_filenames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39124366-b8e9-4c8d-89d2-f06a5b698ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertsPerfect(dataManager, param, fold, seed):\n",
    "\n",
    "    experts = {}\n",
    "    for i, labelerId in enumerate(list(param[\"LABELER_IDS\"])):\n",
    "        nih_expert = expert_module.Expert(dataset = dataManager.getBasicDataset(), labeler_id=labelerId, modus=\"perfect\")\n",
    "        experts[labelerId] = nih_expert\n",
    "\n",
    "\n",
    "    sslDataset = dataManager.getSSLDataset(seed)\n",
    "\n",
    "    mod = \"perfect\"\n",
    "    prediction_type = param[\"EXPERT_PREDICT\"]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    nih_dataloader = dataManager.getKFoldDataloader(seed)\n",
    "    expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(fold)\n",
    "    image_container = nih_dataloader.get_ImageContainer()\n",
    "\n",
    "    val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "    test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param, image_container=image_container)\n",
    "\n",
    "    metrics = {}\n",
    "    for labelerId, expert in experts.items():\n",
    "        metrics[labelerId] = {}\n",
    "\n",
    "        met = al.testExpert(expert, val_dataset, image_container, param, mod, prediction_type, seed, fold, data_name=\"Val\")\n",
    "        metrics[labelerId][\"Val\"] = {\n",
    "            \"End\": met,\n",
    "        }\n",
    "\n",
    "        met = al.testExpert(expert, test_dataset, image_container, param, mod, prediction_type, seed, fold, data_name=\"Test\")\n",
    "        metrics[labelerId][\"Test\"] = {\n",
    "            \"End\": met\n",
    "        }\n",
    "\n",
    "    return experts, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1de88e94-d4cf-475b-bd7b-8c3f0c2eb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExperts(dataManager, param, seed, fold):\n",
    "      \n",
    "    #Creates expert models for the choosen method\n",
    "    if param[\"SETTING\"] == \"PERFECT\":\n",
    "        experts, metrics = getExpertsPerfect(dataManager, param, fold, seed)\n",
    "    if param[\"SETTING\"] == \"AL\":\n",
    "        experts, metrics, labeled_filenames = getExpertsAL(dataManager, param, fold, seed)\n",
    "    elif param[\"SETTING\"] == \"SSL\":\n",
    "        experts, metrics, labeled_filenames = getExpertsSSL(dataManager, param, fold, seed)\n",
    "    elif param[\"SETTING\"] == \"SSL_AL\" or param[\"SETTING\"] == \"SSL_AL_SSL\":\n",
    "        experts, metrics, labeled_filenames = getExpertsSSL_AL(dataManager, param, fold, seed)\n",
    "    elif param[\"SETTING\"] == \"NORMAL\":\n",
    "        experts, metrics, labeled_filenames = getExpertsNormal(dataManager, param, fold, seed)\n",
    "\n",
    "    return experts, metrics, labeled_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "36779aaf-d0a9-4116-8888-d629701db78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2D_Verma(train_loader, val_loader, test_loader, full_dataloader, expert_fns, param, seed, fold_idx, experts):\n",
    "    num_experts = len(expert_fns)\n",
    "            \n",
    "    model = model = vres.ResNet50_defer(int(param[\"n_classes\"]) + num_experts)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Use \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    metrics_train_all, metrics_val_all, metrics_test, metrics_full, metrics_pretrain_all = verm.train(model, train_loader, val_loader, test_loader, expert_fns, param, seed=seed, experts=experts, \n",
    "                                                                                fold=fold_idx, full_dataloader=full_dataloader, param=param)\n",
    "\n",
    "    return metrics_train_all, metrics_val_all, metrics_test, metrics_full, metrics_pretrain_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "39c730b2-f947-44cf-95f5-aaf92081a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_run(dataManager, run_param, all_metrics, print_text, run_metrics, count, current_index=None):\n",
    "    \"\"\"\n",
    "    Computes all seed-fold combinations for one parameter combination and saves the metrics into a file\n",
    "    \n",
    "    Param:\n",
    "        dataManager: DataManager for all data\n",
    "        run_param: dict of all relevant parameters for this run\n",
    "        all_metrics: list which contains all already computed results\n",
    "        print_text: output text to print the current paramater combination\n",
    "        run_metrics: core parameters for this run (which vary over different runs)\n",
    "        count: integer to identify the save file (and number of runs)\n",
    "        current_index: index of the current run in all_metrics, if it exists\n",
    "    \"\"\"\n",
    "\n",
    "    #Get device for cuda training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #To ensure to only print the run text only one time\n",
    "    printed = False\n",
    "\n",
    "    #Metrics for this run\n",
    "    expert_metrics = {}\n",
    "    verma_metrics = {}\n",
    "    hemmer_metrics = {}\n",
    "\n",
    "    #Checks if there is data for this run in the save files\n",
    "    if current_index is not None:\n",
    "        #Load the current metrics\n",
    "        print(f\"Current index: {current_index}\")\n",
    "        current_metric = all_metrics[current_index]\n",
    "\n",
    "        #Save the already computed metrics in the working directories\n",
    "        expert_metrics = current_metric[\"expert metrics\"]\n",
    "        verma_metrics = current_metric[\"verma\"]\n",
    "        hemmer_metrics = current_metric[\"hemmer\"]\n",
    "    #If not, create new element in list of all metrics\n",
    "    else:\n",
    "        all_metrics.append(run_metrics)\n",
    "        \n",
    "\n",
    "    #Iterate over all seeds\n",
    "    for seed in run_param[\"SEEDS\"]:\n",
    "\n",
    "        #If this seed is not already in the save file\n",
    "        if seed not in expert_metrics.keys():\n",
    "            print(f\"New seed: {seed}\")\n",
    "            expert_metrics[seed] = {}\n",
    "            verma_metrics[seed] = {}\n",
    "            hemmer_metrics[seed] = {}\n",
    "\n",
    "        #Iterate over the folds\n",
    "        #for fold_idx in range(run_param[\"K\"]):\n",
    "        for fold_idx in range(2):\n",
    "\n",
    "            #Check if the seed-fold combination is already in the save files\n",
    "            if fold_idx in expert_metrics[seed].keys():\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Keys: {expert_metrics[seed].keys()}\")\n",
    "                print(f\"New fold: {fold_idx}\")\n",
    "\n",
    "            #Print run text if at least one computation is made for this parameter combination (run)\n",
    "            if not printed:\n",
    "                print(print_text)\n",
    "                printed = True\n",
    "\n",
    "            \n",
    "            if run_param[\"cluster\"]: #Keep the embedded model in cluster training\n",
    "                if os.path.isdir(f'{run_param[\"Parent_PATH\"]}/SSL_Working/SSL'):\n",
    "                    cleanTrainDir(f'{run_param[\"Parent_PATH\"]}/SSL_Working/SSL')\n",
    "            else: #delete everything if space is limited\n",
    "                if os.path.isdir(f'{run_param[\"Parent_PATH\"]}/SSL_Working'):\n",
    "                    cleanTrainDir(f'{run_param[\"Parent_PATH\"]}/SSL_Working')\n",
    "\n",
    "            if seed != \"\":\n",
    "                set_seed(seed, fold_idx, text=\"\")\n",
    "\n",
    "            print(\"/n\")\n",
    "            print(f\"Seed: {seed} - Fold: {fold_idx} \\n\")\n",
    "\n",
    "            #if os.path.isdir(f'{run_param[\"Parent_PATH\"]}/SSL_Working/NIH/EmbeddingCM_bin'):\n",
    "            #    cleanTrainDir(f'{run_param[\"Parent_PATH\"]}SSL_Working/NIH/EmbeddingCM_bin')\n",
    "\n",
    "            neptune = {\n",
    "                \"SEED\": seed,\n",
    "                \"FOLD\": fold_idx,\n",
    "            }\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            experts, expert_metric, labeled_filenames = getExperts(dataManager, run_param, seed, fold_idx)\n",
    "            expert_metrics[seed][fold_idx] = expert_metric\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            #print(f\"Got {len(experts)} experts\")\n",
    "\n",
    "            nih_dataloader = dataManager.getKFoldDataloader(seed=seed)\n",
    "\n",
    "            train_loader, val_loader, test_loader = nih_dataloader.get_data_loader_for_fold(fold_idx)\n",
    "            full_dataloader = nih_dataloader.getFullDataloader()\n",
    "\n",
    "            expert_fns = []\n",
    "            print(run_param[\"SETTING\"])\n",
    "            for labelerId, expert in experts.items():\n",
    "                if run_param[\"SETTING\"] == \"AL\":\n",
    "                    expert.init_model_predictions(full_dataloader, mod=\"AL\")\n",
    "                    expert_fns.append(expert.predict_model_predefined_al)\n",
    "                elif run_param[\"SETTING\"] == \"SSL\":\n",
    "                    expert.init_model_predictions(full_dataloader, mod=\"SSL\")\n",
    "                    expert_fns.append(expert.predict_model_predefined_ssl)\n",
    "                elif (run_param[\"SETTING\"] == \"SSL_AL\" or run_param[\"SETTING\"] == \"SSL_AL_SSL\"):\n",
    "                    expert.init_model_predictions(full_dataloader, mod=\"SSL\")\n",
    "                    expert_fns.append(expert.predict_model_predefined_ssl)\n",
    "                elif run_param[\"SETTING\"] == \"NORMAL\":\n",
    "                    expert.init_model_predictions(full_dataloader, mod=\"AL\")\n",
    "                    expert_fns.append(expert.predict_model_predefined_al)\n",
    "                elif run_param[\"SETTING\"] == \"PERFECT\":\n",
    "                    expert_fns.append(expert.predict)\n",
    "\n",
    "            print(\"DELETE ME\")\n",
    "            return experts, dataManager, labeled_filenames\n",
    "\n",
    "            metrics_train_all, metrics_val_all, metrics_test_all, metrics_full_all, metrics_pretrain_all = L2D_Verma(train_loader, val_loader, test_loader, full_dataloader, expert_fns, run_param, seed, fold_idx, experts=experts)\n",
    "\n",
    "            verma_metrics[seed][fold_idx] = {\n",
    "                \"train\": metrics_train_all,\n",
    "                \"val\": metrics_val_all,\n",
    "                \"test\": metrics_test_all,\n",
    "                \"full\": metrics_full_all,\n",
    "                \"pretrain\": metrics_pretrain_all,\n",
    "            }\n",
    "            \n",
    "            system_accuracy, classifier_coverage, all_train_metrics, all_val_metrics, all_test_metrics, all_full_metrics = hm.L2D_Hemmer(train_loader, val_loader, test_loader, full_dataloader, expert_fns, run_param, seed, fold_idx, experts)\n",
    "\n",
    "            hemmer_metrics[seed][fold_idx] = {\n",
    "                \"train\": all_train_metrics,\n",
    "                \"val\": all_val_metrics,\n",
    "                \"test\": all_test_metrics,\n",
    "                \"full\": all_full_metrics,\n",
    "            }\n",
    "\n",
    "            run_metrics[\"expert metrics\"] = expert_metrics\n",
    "            run_metrics[\"verma\"] = verma_metrics\n",
    "            run_metrics[\"hemmer\"] = hemmer_metrics\n",
    "\n",
    "            #Write only into new file if a new run was computed\n",
    "            temp_count = count\n",
    "            if current_index is not None:\n",
    "                all_metrics[current_index] = run_metrics\n",
    "                temp_count = count - 1\n",
    "            else:\n",
    "                all_metrics[-1] = run_metrics\n",
    "            with open(f'{run_param[\"Parent_PATH\"]}/Metrics_Folder/Metrics_{temp_count}.pickle', 'wb') as handle:\n",
    "                pickle.dump(all_metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return expert_metrics, verma_metrics, hemmer_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20baf936-e327-4025-94e4-9a06d2cbacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(param):\n",
    "    run_param = copy.deepcopy(param)\n",
    "\n",
    "    runs = None\n",
    "\n",
    "    expert_metrics_all = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    list_of_files = glob.glob(f'{param[\"Parent_PATH\"]}/Metrics_Folder/*') # * means all if need specific format then *.csv\n",
    "    \n",
    "    if len(list_of_files) >= 1:\n",
    "        latest_file = max(list_of_files, key=os.path.getctime)\n",
    "      \n",
    "        print(f\"Open metrics file: {latest_file}\")\n",
    "\n",
    "        with open(latest_file, 'rb') as handle:\n",
    "            expert_metrics_all = pickle.load(handle)\n",
    "\n",
    "        runs = [{i:run[i] for i in run if i not in [\"expert metrics\", \"verma\", \"hemmer\"]} for run in expert_metrics_all]\n",
    "\n",
    "        if \"pickle\" in latest_file:\n",
    "\n",
    "            count = int(latest_file.split(\"/\")[-1][8:-7]) + 1\n",
    "\n",
    "    #Every pair of labeler ids\n",
    "    for labeler_ids in param[\"LABELER_IDS\"]:\n",
    "        run_param[\"LABELER_IDS\"] = labeler_ids\n",
    "        run_param[\"labeler_ids\"] = convert_ids_to_string(labeler_ids)\n",
    "        \n",
    "\n",
    "        dataManager = ds.DataManager(path=param[\"PATH\"], target=param[\"TARGET\"], param=run_param, seeds=param[\"SEEDS\"])\n",
    "        dataManager.createData()\n",
    "\n",
    "        for init_size in param[\"AL\"][\"INITIAL_SIZE\"]:\n",
    "            run_param[\"AL\"][\"INITIAL_SIZE\"] = init_size\n",
    "\n",
    "            for labels_per_round in param[\"AL\"][\"LABELS_PER_ROUND\"]:\n",
    "                run_param[\"AL\"][\"LABELS_PER_ROUND\"] = labels_per_round\n",
    "\n",
    "                for rounds in param[\"AL\"][\"ROUNDS\"]:\n",
    "                    run_param[\"AL\"][\"ROUNDS\"] = rounds\n",
    "\n",
    "                    labeled = init_size + rounds * labels_per_round\n",
    "\n",
    "                    run_param[\"LABELED\"] = labeled\n",
    "\n",
    "                    if (labeled >= 128): #Prevents from large amount of data\n",
    "                        continue\n",
    "\n",
    "                    for cost in param[\"AL\"][\"COST\"]:\n",
    "                        run_param[\"AL\"][\"COST\"] = cost\n",
    "                        run_param[\"AL\"][\"cost\"] = convert_cost_to_string(cost)\n",
    "\n",
    "                        for overlap in param[\"OVERLAP\"]:\n",
    "                            run_param[\"OVERLAP\"] = overlap\n",
    "\n",
    "                            for setting in param[\"SETTING\"]:\n",
    "                                run_param[\"SETTING\"] = setting\n",
    "                        \n",
    "                                for mod in param[\"MOD\"]:\n",
    "                                    run_param[\"MOD\"] = mod\n",
    "\n",
    "                                    if ((setting == \"AL\"  or setting==\"SSL_AL\" or setting==\"SSL_AL_SSL\") and (mod not in [\"confidence\", \"disagreement\", \"disagreement_diff\"])):\n",
    "                                        continue\n",
    "\n",
    "                                    if (setting == \"SSL\" and mod != \"ssl\"):\n",
    "                                        continue\n",
    "\n",
    "                                    if (setting == \"NORMAL\" and mod != \"normal\"):\n",
    "                                        continue\n",
    "\n",
    "                                    for expert_predict in param[\"EXPERT_PREDICT\"]:\n",
    "                                        run_param[\"EXPERT_PREDICT\"] = expert_predict\n",
    "\n",
    "                                        if ((setting == \"SSL\" or setting == \"SSL_AL\" or setting == \"SSL_AL_SSL\") and (expert_predict == \"right\")):\n",
    "                                            continue\n",
    "\n",
    "                                        if (expert_predict == \"target\") and (cost != param[\"AL\"][\"COST\"][0]):\n",
    "                                            continue\n",
    "                                        if (expert_predict == \"target\"):\n",
    "                                            run_param[\"AL\"][\"cost\"] = convert_cost_to_string((0, 0))\n",
    "\n",
    "                                        for sample_equal in param[\"SAMPLE_EQUAL\"]:\n",
    "                                            run_param[\"SAMPLE_EQUAL\"] = sample_equal\n",
    "\n",
    "                                            for epochs_pretrain in param[\"epochs_pretrain\"]:\n",
    "                                                run_param[\"epochs_pretrain\"] = epochs_pretrain\n",
    "\n",
    "                                                metrics_save = {}\n",
    "                                                metrics_save[\"labeler_ids\"] = labeler_ids\n",
    "                                                metrics_save[\"init_size\"] = init_size\n",
    "                                                metrics_save[\"labels_per_round\"] = labels_per_round\n",
    "                                                metrics_save[\"rounds\"] = rounds\n",
    "                                                metrics_save[\"labeled\"] = labeled\n",
    "                                                metrics_save[\"cost\"] = cost\n",
    "                                                metrics_save[\"overlap\"] = overlap\n",
    "                                                metrics_save[\"setting\"] = setting\n",
    "                                                metrics_save[\"mod\"] = mod\n",
    "                                                metrics_save[\"expert_predict\"] = expert_predict\n",
    "                                                metrics_save[\"sample_equal\"] = sample_equal\n",
    "                                                metrics_save[\"epochs_pretrain\"] = epochs_pretrain\n",
    "\n",
    "                                                \n",
    "                                                current_index = None\n",
    "                                                \n",
    "                                                #Compute the current index\n",
    "                                                if runs is not None:\n",
    "                                                    #If this parameter compination is in the already done runs\n",
    "                                                    if metrics_save in runs:\n",
    "                                                        #Get index of this combination\n",
    "                                                        current_index = runs.index(metrics_save)\n",
    "                                                        print(f\"Current index: {current_index}\")\n",
    "                            \n",
    "                                                NEPTUNE = param[\"NEPTUNE\"][\"NEPTUNE\"]\n",
    "                                                if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "                                                    global run\n",
    "                                                    run = neptune.init_run(\n",
    "                                                        project=config_neptune[\"project\"],\n",
    "                                                        api_token=config_neptune[\"api_token\"],\n",
    "                                                        #custom_run_id=\"AL_\" + \n",
    "                                                    )\n",
    "                                                    run[\"param\"] = run_param\n",
    "                                                    run_param[\"NEPTUNE\"][\"RUN\"] = run\n",
    "\n",
    "                                                print_text = f\"\"\"\\n \\n \\n #############################################################\n",
    "                                                NEW RUN\n",
    "\n",
    "                                                Initial size: {init_size}\n",
    "                                                Batch size AL: {labels_per_round}\n",
    "                                                Max rounds: {rounds}\n",
    "                                                Labeled images: {labeled}\n",
    "                                                Cost: {cost}\n",
    "                                                Setting: {setting}\n",
    "                                                Mod: {mod}\n",
    "                                                Overlap: {overlap}\n",
    "                                                Prediction Type: {expert_predict}\n",
    "                                                Sample equal: {sample_equal}\n",
    "                                                Epochs pretrain: {epochs_pretrain}\n",
    "                                                \"\"\"\n",
    "\n",
    "                                                start_time = time.time()\n",
    "                                                #dataManager, run_param, all_metrics, print_text, run_metrics, count, current_index=None\n",
    "                                                #expert_metrics, verma_metrics, hemmer_metrics = one_run(dataManager, run_param, expert_metrics_all.copy(), print_text, metrics_save,\n",
    "                                                #                                                       count, current_index)\n",
    "\n",
    "                                                print(\"DELETE ME\")\n",
    "                                                return one_run(dataManager, run_param, expert_metrics_all.copy(), print_text, metrics_save, count, current_index)\n",
    "                                                \n",
    "                                                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "                                                metrics_save[\"expert metrics\"] = expert_metrics\n",
    "                                                metrics_save[\"verma\"] = verma_metrics\n",
    "                                                metrics_save[\"hemmer\"] = hemmer_metrics\n",
    "                                                ensure_count = 0 #Helps to save into the correct file if metrics are added to a run\n",
    "                                                if current_index is not None:\n",
    "                                                    expert_metrics_all[current_index] = metrics_save\n",
    "                                                    ensure_count = 1\n",
    "                                                else:\n",
    "                                                    expert_metrics_all.append(metrics_save)\n",
    "                                                with open(f'{param[\"Parent_PATH\"]}/Metrics_Folder/Metrics_{count - ensure_count}.pickle', 'wb') as handle:\n",
    "                                                    pickle.dump(expert_metrics_all, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                                if current_index is None:\n",
    "                                                    count += 1\n",
    "                                                if param[\"NEPTUNE\"][\"NEPTUNE\"]:\n",
    "                                                    run[\"metrics\"] = metrics_save\n",
    "\n",
    "                                                    run.stop()\n",
    "                                                return\n",
    "\n",
    "    return expert_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f63b284-8933-4876-8875-3ec3c9b35a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cost_to_string(tp):\n",
    "    return \"(\" + str(tp[0]) + \", \" + str(tp[1]) + \")\"\n",
    "\n",
    "def convert_ids_to_string(ids):\n",
    "    return f\"{ids[0]}, {ids[1]}\"\n",
    "\n",
    "def convert_list_to_string(li):\n",
    "    result = \"[\"\n",
    "    for el in li[:-2]:\n",
    "        result = result + str(el)\n",
    "    result = result + \"]\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87b9b435-2f48-4591-abfb-d484668b4534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joli/Masterarbeit'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6dde2cce-c1ce-488a-9f5c-f9560d0d9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    path = args[0]\n",
    "\n",
    "    num_worker = 4\n",
    "    if len(args) >= 2:\n",
    "        num_worker = int(args[1])\n",
    "\n",
    "    if \"liebschner\" not in path and \"joli\" not in path:\n",
    "        return\n",
    "\n",
    "    param = {\n",
    "        \"PATH\": f\"{path}/Datasets/NIH/\",\n",
    "        \"Parent_PATH\": path,\n",
    "        \"TARGET\": \"Airspace_Opacity\",\n",
    "        #\"LABELER_IDS\": [[4323195249, 4295232296]],\n",
    "        \"LABELER_IDS\": [[4295349121, 4295342357]],\n",
    "        \"K\": 10, #Number of folds\n",
    "        #\"SEEDS\": [1, 2, 3, 4, 42], #Seeds for the experiments\n",
    "        \"SEEDS\": [1], #Seeds for the experiments\n",
    "        \"GT\": True, # Determines if the classifier gets all data with GT Label or only the labeld data\n",
    "        #\"MOD\": [\"confidence\", \"disagreement\", \"disagreement_diff\", \"ssl\", \"normal\"], #Determines the experiment modus\n",
    "        \"MOD\": [\"confidence\"],\n",
    "\n",
    "        \"OVERLAP\": [0, 100],\n",
    "        \"SAMPLE_EQUAL\": [False, True],\n",
    "\n",
    "        #\"SETTING\": [\"AL\", \"SSL\", \"SSL_AL\", \"NORMAL\", \"SSL_AL_SSL\"],\n",
    "        \"SETTING\": [\"SSL_AL\"],\n",
    "\n",
    "        \"NUM_EXPERTS\": 2,\n",
    "        \"NUM_CLASSES\": 2,\n",
    "\n",
    "        \"EXPERT_PREDICT\": [\"right\", \"target\"],\n",
    "\n",
    "        \"AL\": { #Parameter for Active Learning\n",
    "            \"INITIAL_SIZE\": [16, 32], #\n",
    "            \"EPOCH_TRAIN\": 40, #\n",
    "            \"n_dataset\": 2, #Number Classes\n",
    "            \"BATCH_SIZE\": 4,\n",
    "            \"BATCH_SIZE_VAL\": 32,\n",
    "            \"ROUNDS\": [2, 4, 8],\n",
    "            \"LABELS_PER_ROUND\": [4, 8, 16],\n",
    "            \"EPOCHS_DEFER\": 10,\n",
    "            \"COST\": [(0, 0), (5, 0)], #Cost for Cost sensitiv learning\n",
    "            #\"TRAIN REJECTOR\": False,\n",
    "            \"PRELOAD\": True,\n",
    "            \"PREPROCESS\": True,\n",
    "            \"SSL_EPOCHS\": 3\n",
    "        \n",
    "        },\n",
    "        \"SSL\": {\n",
    "            \"PREBUILD\": False,\n",
    "            #\"TRAIN_BATCH_SIZE\": 128,\n",
    "            \"TRAIN_BATCH_SIZE\": 254,\n",
    "            \"TEST_BATCH_SIZE\": 254,\n",
    "            \"N_EPOCHS\": 5, #number of training epoches\n",
    "            \"BATCHSIZE\": 16, #train batch size of labeled samples\n",
    "            #\"N_IMGS_PER_EPOCH\": 32768, #number of training images for each epoch\n",
    "            \"N_IMGS_PER_EPOCH\": 4381*1, #number of training images for each epoch\n",
    "        },\n",
    "        \"L2D\": { # Parameter for Learning to defer\n",
    "            \"TRAIN_BATCH_SIZE\": 128,\n",
    "            \"TEST_BATCH_SIZE\": 128,\n",
    "            \"PRELOAD\": True,\n",
    "            \"PREBUILD\": True,\n",
    "            \"EPOCHS\": 50,\n",
    "            \"VERMA\": {},\n",
    "            \"HEMMER\": {\n",
    "                \"EPOCHS\": 50,\n",
    "                \"LR\": 5e-3,\n",
    "                \"USE_LR_SCHEDULER\": False,\n",
    "                \"DROPOUT\": 0.00,\n",
    "                \"NUM_HIDDEN_UNITS\": 30,\n",
    "            },\n",
    "        \n",
    "        },\n",
    "        \"NEPTUNE\": {\n",
    "            \"NEPTUNE\": False,\n",
    "        },\n",
    "        \"EMBEDDED\": {\n",
    "            \"ARGS\": {\n",
    "                'dataset': \"nih\",\n",
    "                'model': \"resnet50\",\n",
    "                'num_classes': 2,\n",
    "                'batch': 128,\n",
    "                'lr': 0.001,\n",
    "            },\n",
    "            \"EPOCHS\": 30,\n",
    "        },\n",
    "    \n",
    "    \n",
    "        \"epochs_pretrain\": [0],\n",
    "        \"batch_size\": 64,\n",
    "        \"alpha\": 1.0, #scaling parameter for the loss function, default=1.0\n",
    "        \"epochs\": 50,\n",
    "        \"patience\": 35, #number of patience steps for early stopping the training\n",
    "        \"expert_type\": \"MLPMixer\", #specify the expert type. For the type of experts available, see-> models -> experts. defualt=predict\n",
    "        \"n_classes\": 2, #K for K class classification\n",
    "        \"k\": 0, #\n",
    "        \"n_experts\": 2, #\n",
    "        \"lr\": 0.001, #learning rate\n",
    "        \"weight_decay\": 5e-4, #\n",
    "        \"warmup_epochs\": 5, #\n",
    "        #\"loss_type\": \"softmax\", #surrogate loss type for learning to defer\n",
    "        \"loss_type\": \"ova\",\n",
    "        \"ckp_dir\": f\"{path}/Models\", #directory name to save the checkpoints\n",
    "        \"experiment_name\": \"multiple_experts\", #specify the experiment name. Checkpoints will be saved with this name\n",
    "\n",
    "        #Params for cluster training\n",
    "        \"num_worker\": num_worker,\n",
    "        \"cluster\": False\n",
    "    }\n",
    "\n",
    "    return run_experiment(param)\n",
    "\n",
    "    expert_metrics_all = run_experiment(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947663b-90a5-4562-ac8c-a9b90dea8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "experts, dataManager, labeled_filenames = main([\"/home/joli\", 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39b1fb-ae54-4687-931a-041131a6bc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3df2ebcf-eecd-4004-8941-bc9c6902a3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Image ID</th>\n",
       "      <th>GT</th>\n",
       "      <th>4295367682</th>\n",
       "      <th>4295232296</th>\n",
       "      <th>4323195249</th>\n",
       "      <th>4295194124</th>\n",
       "      <th>4325222456</th>\n",
       "      <th>4295354140</th>\n",
       "      <th>4295206903</th>\n",
       "      <th>...</th>\n",
       "      <th>4295246212</th>\n",
       "      <th>4295354117</th>\n",
       "      <th>4326615889</th>\n",
       "      <th>4322936986</th>\n",
       "      <th>4295369079</th>\n",
       "      <th>4295344101</th>\n",
       "      <th>4325221191</th>\n",
       "      <th>4295349028</th>\n",
       "      <th>4295393403</th>\n",
       "      <th>4295354708</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>00000013_008.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>00000013_026.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>00000032_002.png</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>00000032_009.png</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>00000032_011.png</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4376</th>\n",
       "      <td>30709</td>\n",
       "      <td>00030709_002.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377</th>\n",
       "      <td>30719</td>\n",
       "      <td>00030719_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>30752</td>\n",
       "      <td>00030752_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379</th>\n",
       "      <td>30759</td>\n",
       "      <td>00030759_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>30794</td>\n",
       "      <td>00030794_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4381 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Patient ID          Image ID  GT  4295367682  4295232296  4323195249  \\\n",
       "0             13  00000013_008.png   1         1.0         1.0         1.0   \n",
       "1             13  00000013_026.png   0        -1.0        -1.0         0.0   \n",
       "2             32  00000032_002.png   1        -1.0        -1.0         1.0   \n",
       "3             32  00000032_009.png   1        -1.0         1.0         1.0   \n",
       "4             32  00000032_011.png   1        -1.0         1.0         1.0   \n",
       "...          ...               ...  ..         ...         ...         ...   \n",
       "4376       30709  00030709_002.png   0         0.0        -1.0         0.0   \n",
       "4377       30719  00030719_000.png   0        -1.0         0.0         0.0   \n",
       "4378       30752  00030752_000.png   0        -1.0        -1.0         0.0   \n",
       "4379       30759  00030759_000.png   0        -1.0        -1.0         0.0   \n",
       "4380       30794  00030794_000.png   0        -1.0         0.0         0.0   \n",
       "\n",
       "      4295194124  4325222456  4295354140  4295206903  ...  4295246212  \\\n",
       "0           -1.0        -1.0        -1.0        -1.0  ...        -1.0   \n",
       "1            0.0         0.0        -1.0        -1.0  ...        -1.0   \n",
       "2            0.0        -1.0         0.0        -1.0  ...        -1.0   \n",
       "3           -1.0        -1.0        -1.0         1.0  ...        -1.0   \n",
       "4           -1.0        -1.0        -1.0        -1.0  ...        -1.0   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "4376         0.0        -1.0        -1.0        -1.0  ...        -1.0   \n",
       "4377        -1.0        -1.0        -1.0        -1.0  ...        -1.0   \n",
       "4378         0.0         0.0        -1.0        -1.0  ...        -1.0   \n",
       "4379        -1.0        -1.0        -1.0        -1.0  ...        -1.0   \n",
       "4380        -1.0        -1.0        -1.0         0.0  ...        -1.0   \n",
       "\n",
       "      4295354117  4326615889  4322936986  4295369079  4295344101  4325221191  \\\n",
       "0           -1.0        -1.0        -1.0        -1.0        -1.0        -1.0   \n",
       "1           -1.0        -1.0        -1.0        -1.0        -1.0        -1.0   \n",
       "2           -1.0        -1.0        -1.0        -1.0        -1.0        -1.0   \n",
       "3           -1.0        -1.0        -1.0        -1.0        -1.0        -1.0   \n",
       "4           -1.0        -1.0        -1.0        -1.0        -1.0        -1.0   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "4376        -1.0        -1.0        -1.0        -1.0        -1.0        -1.0   \n",
       "4377        -1.0        -1.0        -1.0        -1.0        -1.0        -1.0   \n",
       "4378        -1.0        -1.0        -1.0        -1.0        -1.0        -1.0   \n",
       "4379        -1.0         0.0        -1.0        -1.0        -1.0        -1.0   \n",
       "4380        -1.0        -1.0        -1.0        -1.0        -1.0        -1.0   \n",
       "\n",
       "      4295349028  4295393403  4295354708  \n",
       "0           -1.0        -1.0        -1.0  \n",
       "1           -1.0        -1.0        -1.0  \n",
       "2           -1.0        -1.0        -1.0  \n",
       "3           -1.0        -1.0        -1.0  \n",
       "4           -1.0        -1.0        -1.0  \n",
       "...          ...         ...         ...  \n",
       "4376        -1.0        -1.0        -1.0  \n",
       "4377        -1.0        -1.0        -1.0  \n",
       "4378        -1.0        -1.0        -1.0  \n",
       "4379        -1.0        -1.0        -1.0  \n",
       "4380        -1.0        -1.0        -1.0  \n",
       "\n",
       "[4381 rows x 25 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicDataset = dataManager.getBasicDataset()\n",
    "basicData = basicDataset.getData().copy()\n",
    "basicData.head()\n",
    "labelerIds = basicData.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c7998aeb-3a99-4ec9-98cd-7cf4e2153c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4295367682</th>\n",
       "      <th>4295232296</th>\n",
       "      <th>4323195249</th>\n",
       "      <th>4295194124</th>\n",
       "      <th>4325222456</th>\n",
       "      <th>4295354140</th>\n",
       "      <th>4295206903</th>\n",
       "      <th>4326829894</th>\n",
       "      <th>4295376896</th>\n",
       "      <th>4295349121</th>\n",
       "      <th>...</th>\n",
       "      <th>4295246212</th>\n",
       "      <th>4295354117</th>\n",
       "      <th>4326615889</th>\n",
       "      <th>4322936986</th>\n",
       "      <th>4295369079</th>\n",
       "      <th>4295344101</th>\n",
       "      <th>4325221191</th>\n",
       "      <th>4295349028</th>\n",
       "      <th>4295393403</th>\n",
       "      <th>4295354708</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4295367682</th>\n",
       "      <td>536</td>\n",
       "      <td>9</td>\n",
       "      <td>536</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295232296</th>\n",
       "      <td>9</td>\n",
       "      <td>853</td>\n",
       "      <td>852</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>256</td>\n",
       "      <td>253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4323195249</th>\n",
       "      <td>536</td>\n",
       "      <td>852</td>\n",
       "      <td>2320</td>\n",
       "      <td>936</td>\n",
       "      <td>162</td>\n",
       "      <td>212</td>\n",
       "      <td>593</td>\n",
       "      <td>295</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>255</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295194124</th>\n",
       "      <td>319</td>\n",
       "      <td>162</td>\n",
       "      <td>936</td>\n",
       "      <td>938</td>\n",
       "      <td>64</td>\n",
       "      <td>122</td>\n",
       "      <td>149</td>\n",
       "      <td>8</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325222456</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>64</td>\n",
       "      <td>162</td>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295354140</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>212</td>\n",
       "      <td>122</td>\n",
       "      <td>6</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295206903</th>\n",
       "      <td>74</td>\n",
       "      <td>256</td>\n",
       "      <td>593</td>\n",
       "      <td>149</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>594</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326829894</th>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>295</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295376896</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295349121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1674</td>\n",
       "      <td>...</td>\n",
       "      <td>425</td>\n",
       "      <td>758</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295342357</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1635</td>\n",
       "      <td>...</td>\n",
       "      <td>521</td>\n",
       "      <td>901</td>\n",
       "      <td>419</td>\n",
       "      <td>298</td>\n",
       "      <td>26</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295359843</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295246212</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>425</td>\n",
       "      <td>...</td>\n",
       "      <td>521</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295354117</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>758</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>940</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326615889</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>265</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>456</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322936986</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>323</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295369079</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295344101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325221191</th>\n",
       "      <td>90</td>\n",
       "      <td>98</td>\n",
       "      <td>255</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295349028</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295393403</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295354708</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            4295367682  4295232296  4323195249  4295194124  4325222456  \\\n",
       "4295367682         536           9         536         319           0   \n",
       "4295232296           9         853         852         162           0   \n",
       "4323195249         536         852        2320         936         162   \n",
       "4295194124         319         162         936         938          64   \n",
       "4325222456           0           0         162          64         162   \n",
       "4295354140           0          58         212         122           6   \n",
       "4295206903          74         256         593         149          69   \n",
       "4326829894           0         253         295           8           0   \n",
       "4295376896           0           0         100         101           0   \n",
       "4295349121           0           0           0           0           0   \n",
       "4295342357          20           0         332           0           0   \n",
       "4295359843           0           0           0           0           0   \n",
       "4295246212           0           0           0           0           0   \n",
       "4295354117           0           0          28           0           0   \n",
       "4326615889           0           0          32           0           0   \n",
       "4322936986          24           0         190           0           0   \n",
       "4295369079           0           0           0           0           0   \n",
       "4295344101           0           0          64           0           0   \n",
       "4325221191          90          98         255          11          23   \n",
       "4295349028           0           0          20           0           0   \n",
       "4295393403           0          12          12           0           0   \n",
       "4295354708           0           0           0           0           0   \n",
       "\n",
       "            4295354140  4295206903  4326829894  4295376896  4295349121  ...  \\\n",
       "4295367682           0          74           0           0           0  ...   \n",
       "4295232296          58         256         253           0           0  ...   \n",
       "4323195249         212         593         295         100           0  ...   \n",
       "4295194124         122         149           8         101           0  ...   \n",
       "4325222456           6          69           0           0           0  ...   \n",
       "4295354140         212           0           0           0           0  ...   \n",
       "4295206903           0         594          31           0           0  ...   \n",
       "4326829894           0          31         296           0           0  ...   \n",
       "4295376896           0           0           0         101           0  ...   \n",
       "4295349121           0           0           0           0        1674  ...   \n",
       "4295342357           0           0           0           0        1635  ...   \n",
       "4295359843           0           0           0           0         193  ...   \n",
       "4295246212           0           0           0           0         425  ...   \n",
       "4295354117           0           0           0           0         758  ...   \n",
       "4326615889           0           0           0           0         265  ...   \n",
       "4322936986           0           0           0           0           0  ...   \n",
       "4295369079           0           0           0           0           0  ...   \n",
       "4295344101           0           0           0           0           0  ...   \n",
       "4325221191          23          10           0           0           0  ...   \n",
       "4295349028           0           0           0           0          39  ...   \n",
       "4295393403           0           0           0           0           0  ...   \n",
       "4295354708           0           0           0           0          23  ...   \n",
       "\n",
       "            4295246212  4295354117  4326615889  4322936986  4295369079  \\\n",
       "4295367682           0           0           0          24           0   \n",
       "4295232296           0           0           0           0           0   \n",
       "4323195249           0          28          32         190           0   \n",
       "4295194124           0           0           0           0           0   \n",
       "4325222456           0           0           0           0           0   \n",
       "4295354140           0           0           0           0           0   \n",
       "4295206903           0           0           0           0           0   \n",
       "4326829894           0           0           0           0           0   \n",
       "4295376896           0           0           0           0           0   \n",
       "4295349121         425         758         265           0           0   \n",
       "4295342357         521         901         419         298          26   \n",
       "4295359843          20           0           0          54           0   \n",
       "4295246212         521          53          12           0           0   \n",
       "4295354117          53         940         118           0           0   \n",
       "4326615889          12         118         456          16           0   \n",
       "4322936986           0           0          16         323          26   \n",
       "4295369079           0           0           0          26          26   \n",
       "4295344101           0           0           4          33           0   \n",
       "4325221191           0           0           0           0           0   \n",
       "4295349028           9           0           0           0           0   \n",
       "4295393403           0           0           0           0           0   \n",
       "4295354708           0          15          46           0           0   \n",
       "\n",
       "            4295344101  4325221191  4295349028  4295393403  4295354708  \n",
       "4295367682           0          90           0           0           0  \n",
       "4295232296           0          98           0          12           0  \n",
       "4323195249          64         255          20          12           0  \n",
       "4295194124           0          11           0           0           0  \n",
       "4325222456           0          23           0           0           0  \n",
       "4295354140           0          23           0           0           0  \n",
       "4295206903           0          10           0           0           0  \n",
       "4326829894           0           0           0           0           0  \n",
       "4295376896           0           0           0           0           0  \n",
       "4295349121           0           0          39           0          23  \n",
       "4295342357         101           0          68           0          86  \n",
       "4295359843           0           0           0           0           0  \n",
       "4295246212           0           0           9           0           0  \n",
       "4295354117           0           0           0           0          15  \n",
       "4326615889           4           0           0           0          46  \n",
       "4322936986          33           0           0           0           0  \n",
       "4295369079           0           0           0           0           0  \n",
       "4295344101         101           0           0           0           0  \n",
       "4325221191           0         255           0           0           0  \n",
       "4295349028           0           0          68           0           0  \n",
       "4295393403           0           0           0          12           0  \n",
       "4295354708           0           0           0           0          86  \n",
       "\n",
       "[22 rows x 22 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def equal_labeled_images(data, experts):\n",
    "    \"\"\"\n",
    "    Returns dataframe with only images which are labeled from every given expert\n",
    "    \"\"\"\n",
    "    for expertId in experts:\n",
    "        data = data[data[expertId] != -1].copy()\n",
    "    return data[[\"Patient ID\", \"Image ID\", \"GT\"] + experts]\n",
    "\n",
    "def number_labeled_images(data, experts):\n",
    "    \"\"\"\n",
    "    Returns number of images which every given expert has labeled\n",
    "    \"\"\"\n",
    "    return len(equal_labeled_images(data, experts))\n",
    "\n",
    "def get_pair_labeled(data, Ids):\n",
    "    \"\"\"\n",
    "    Returns matrix which contains the number of labeled images for every pair of experts\n",
    "    \"\"\"\n",
    "    images_dict = {}\n",
    "    for i in Ids:\n",
    "        labeler_dict = {}\n",
    "        for j in Ids:\n",
    "            labeler_dict[j] = number_labeled_images(basicData, [i, j])\n",
    "        images_dict[i] = labeler_dict\n",
    "    return pd.DataFrame(images_dict)\n",
    "\n",
    "pair_labeled = get_pair_labeled(basicData, labelerIds)\n",
    "pair_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a50fe7da-cf4e-4583-ad64-510e94a75ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b09d5232-ea15-4808-b263-d55911ef62a2",
   "metadata": {},
   "source": [
    "### New functions for saving the labels from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16857242-a29c-466c-8bea-cec33c56803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_expert_labels(experts, labeled_images):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac203c46-9db9-45e9-a84b-2517a931aa2d",
   "metadata": {},
   "source": [
    "### New functions for training without gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899113c-5223-4e79-9844-aa53958b4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artificial_gt(dataManager, train_dataloader, experts, fold, seed, method=\"perfect\"):\n",
    "    if method == \"perfect\":\n",
    "        return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff14a20-4e7a-4139-80d7-53968e85f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_df(dataset, experts):\n",
    "    \"\"\"\n",
    "    Creates a df with filename, gt, [artificial expert predictions]\n",
    "    \"\"\"\n",
    "    gt = pd.DataFrame({\"filename\": dataset.getAllFilenames(), \"gt\": dataset.getAllTargets()})\n",
    "    result = gt.copy()\n",
    "    for expert_id, expert in experts.items():\n",
    "        if \"ssl\" in expert.modus:\n",
    "            expert_df = pd.DataFrame({\"filename\": expert.prebuild_filenames_ssl, f\"prediction_{expert_id}\": expert.prebuild_predictions_ssl})\n",
    "        else:\n",
    "            expert_df = pd.DataFrame({\"filename\": expert.prebuild_filenames_al, f\"prediction_{expert_id}\": expert.prebuild_predictions_al})\n",
    "        result = pd.merge(result, expert_df)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "616327c8-4339-4c13-8ca6-effd17c9dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_true_expert_labels(dataset, experts):\n",
    "    \"\"\"\n",
    "    Creates a df with filename, gt, [true expert predictions]\n",
    "    \"\"\"\n",
    "    gt = pd.DataFrame({\"filename\": dataset.getAllFilenames(), \"gt\": dataset.getAllTargets()})\n",
    "    result = gt.copy()\n",
    "    for expert_id, expert in experts.items():\n",
    "        expert_df = expert.predictions.reset_index(names=\"filename\")\n",
    "        result = pd.merge(result, expert_df)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "284ee017-a4fe-4675-affd-406d73304a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_labeled(df, labeled):\n",
    "    return df[df[\"filename\"].isin(labeled)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66d79e-2cec-4127-a54f-36fb491dfc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4614465-5368-4e2c-b0ba-f8420c9c3818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a480fd-7e71-4290-aec6-f7d2126f6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDataset = dataManager.getKFoldDataloader(1).getFullDataloader().dataset\n",
    "fullDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef9b19-523d-4158-b1d2-ec7fcd315cbf",
   "metadata": {},
   "source": [
    "Create df with filename, gt, and artificial expert predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac560434-2db8-49c2-88bf-48c202023cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = create_label_df(fullDataset, experts)\n",
    "label_df.head(7)\n",
    "\n",
    "print(f'Accuracy expert 1: {len(label_df[label_df[\"gt\"] == label_df[\"prediction_4295349121\"]])/len(label_df)*100}%')\n",
    "print(f'Accuracy expert 2: {len(label_df[label_df[\"gt\"] == label_df[\"prediction_4295342357\"]])/len(label_df)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c38a36-7d51-4511-be96-9fa7fc12cf9e",
   "metadata": {},
   "source": [
    "The corresponding real expert predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1ab7535b-b223-4adc-9b45-35065dda0743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gt</th>\n",
       "      <th>4295349121</th>\n",
       "      <th>4295342357</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000119_001.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000134_000.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000135_001.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000156_000.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000156_001.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>00019643_019.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>00019643_020.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>00019643_021.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>00019643_022.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>00019643_024.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1635 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename  gt  4295349121  4295342357\n",
       "0     00000119_001.png   0         0.0         0.0\n",
       "1     00000134_000.png   1         1.0         0.0\n",
       "2     00000135_001.png   1         0.0         0.0\n",
       "3     00000156_000.png   1         1.0         1.0\n",
       "4     00000156_001.png   1         1.0         1.0\n",
       "...                ...  ..         ...         ...\n",
       "1630  00019643_019.png   1         1.0         0.0\n",
       "1631  00019643_020.png   0         0.0         0.0\n",
       "1632  00019643_021.png   0         0.0         0.0\n",
       "1633  00019643_022.png   0         0.0         0.0\n",
       "1634  00019643_024.png   0         0.0         0.0\n",
       "\n",
       "[1635 rows x 4 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_experts = create_true_expert_labels(fullDataset, experts)\n",
    "true_experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03689a58-ee03-48ce-8331-796f511a853b",
   "metadata": {},
   "source": [
    "Processing the list of labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "46ede6cf-224d-43e2-b214-30c693bac300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00012233_001.png', '00001792_003.png', '00015698_001.png',\n",
       "       '00001722_004.png', '00013443_001.png', '00001470_000.png',\n",
       "       '00018546_002.png', '00018557_011.png', '00019260_007.png',\n",
       "       '00013601_003.png', '00012184_002.png', '00012387_001.png',\n",
       "       '00018829_005.png', '00016052_006.png', '00018019_005.png',\n",
       "       '00014558_011.png', '00014715_011.png', '00001449_002.png',\n",
       "       '00016887_000.png', '00019643_007.png', '00019240_005.png',\n",
       "       '00012010_005.png', '00013073_006.png', '00012010_035.png',\n",
       "       '00001722_004.png', '00013844_001.png', '00019576_023.png',\n",
       "       '00019643_004.png', '00018546_006.png', '00012010_026.png',\n",
       "       '00012219_007.png', '00013916_002.png', '00012543_010.png',\n",
       "       '00016561_003.png', '00018250_000.png', '00013613_013.png',\n",
       "       '00018335_012.png', '00019576_060.png', '00013601_022.png',\n",
       "       '00013613_016.png', '00018329_007.png', '00001249_004.png'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.empty(0, dtype=\"str\")\n",
    "for key, item in labeled_list[\"ssl_al\"][\"al labels\"].items():\n",
    "    result = np.append(result, item)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5dd0d-084c-4add-87ae-a370d690ae1e",
   "metadata": {},
   "source": [
    "Filter expert predictions for only labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "6986e8d7-1652-4c3b-93cb-8816494cc1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gt</th>\n",
       "      <th>4295349121</th>\n",
       "      <th>4295342357</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00001249_004.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>00001449_002.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>00001470_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>00001722_004.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>00001792_003.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  gt  4295349121  4295342357\n",
       "21  00001249_004.png   1         1.0         0.0\n",
       "34  00001449_002.png   0         0.0         0.0\n",
       "35  00001470_000.png   0         0.0         0.0\n",
       "64  00001722_004.png   0         0.0         0.0\n",
       "70  00001792_003.png   0         0.0         0.0"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_labeled(true_experts, result).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130cc49-f8d1-499c-b08b-1806ddac3bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b42b7251-9595-42e7-8c3f-f24250a61ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dataset.Dataset.NIHDataset at 0x7f4c2b5a0400>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c70b09a-5a02-4674-9402-329b159a5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_train, expert_val, expert_test = dataManager.getKFoldDataloader(1).get_dataset_for_folder(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edbb4f-92c2-4dcc-8391-fa1eac223257",
   "metadata": {},
   "source": [
    "{4295349121: [440, 112, 28, 357, 759, 108, 463, 256, 675, 703, 101, 33, 663, 16, 198, 183], 4295342357: [701, 770, 503, 32, 776, 577, 733, 248, 384, 186, 729, 249, 591, 268, 174, 93]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4eff7da0-02db-4c90-88c4-4c4a204186ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_list[\"ssl_al\"] = labeled_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e77933d-5fdf-4942-98a2-5af670cb6ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "af56b0a3-6b22-457c-ba20-5dc28498d68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy expert 1: 68.68501529051989%\n",
      "Accuracy expert 2: 56.513761467889914%\n"
     ]
    }
   ],
   "source": [
    "label_df = create_label_df(fullDataset, experts)\n",
    "label_df.head(7)\n",
    "\n",
    "print(f'Accuracy expert 1: {len(label_df[label_df[\"gt\"] == label_df[\"prediction_4295349121\"]])/len(label_df)*100}%')\n",
    "print(f'Accuracy expert 2: {len(label_df[label_df[\"gt\"] == label_df[\"prediction_4295342357\"]])/len(label_df)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "6c29cb38-b39a-4838-a42e-f7ed8178bb6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: empty expression not allowed (2971499285.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[266], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    name = f\"labeler_{}_seed_{}_fold_{}_gt_{param[\"GT\"]}_training_{param[\"SETTING\"]}_mod_{param[\"MOD\"]}_overlap_{param[\"OVERLAP\"]}_sample_equal_{param[\"SAMPLE_EQUAL\"]}_labeled_{param[\"LABELED\"]}_initSize_{param[\"AL\"][\"INITIAL_SIZE\"]}_rounds_{param[\"AL\"][\"ROUNDS\"]}_labelsPerRound_{param[\"AL\"][\"LABELS_PER_ROUND\"]}\"\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: empty expression not allowed\n"
     ]
    }
   ],
   "source": [
    "def save_expert_labels(labels_df, param, path):\n",
    "    if path[-1] != \"/\":\n",
    "        path = path + \"/\"\n",
    "    name = f\"labeler_{}_seed_{}_fold_{}_gt_{param[\"GT\"]}_training_{param[\"SETTING\"]}_mod_{param[\"MOD\"]}_overlap_{param[\"OVERLAP\"]}_sample_equal_{param[\"SAMPLE_EQUAL\"]}_labeled_{param[\"LABELED\"]}_initSize_{param[\"AL\"][\"INITIAL_SIZE\"]}_rounds_{param[\"AL\"][\"ROUNDS\"]}_labelsPerRound_{param[\"AL\"][\"LABELS_PER_ROUND\"]}\"\n",
    "    labels_df.to_csv(f\"{path}labels_{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01430b49-ab03-4c52-a0aa-a87f428b5670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e5be4375-a46c-42e1-aa1f-cd2e1f1a02f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4295349121: array(['00012233_001.png', '00001792_003.png', '00015698_001.png',\n",
       "        '00001722_004.png', '00013443_001.png', '00001470_000.png',\n",
       "        '00018546_002.png', '00018557_011.png', '00019260_007.png',\n",
       "        '00013601_003.png', '00012184_002.png', '00012387_001.png',\n",
       "        '00018829_005.png', '00016052_006.png', '00018019_005.png',\n",
       "        '00014558_011.png', '00014715_011.png', '00001449_002.png',\n",
       "        '00016887_000.png', '00019643_007.png', '00019240_005.png',\n",
       "        '00012010_005.png'], dtype='<U16'),\n",
       " 4295342357: array(['00013073_006.png', '00012010_035.png', '00001722_004.png',\n",
       "        '00013844_001.png', '00019576_023.png', '00019643_004.png',\n",
       "        '00018546_006.png', '00012010_026.png', '00012219_007.png',\n",
       "        '00013916_002.png', '00012543_010.png', '00016561_003.png',\n",
       "        '00018250_000.png', '00013613_013.png', '00018335_012.png',\n",
       "        '00019576_060.png', '00013601_022.png', '00013613_016.png',\n",
       "        '00018329_007.png', '00001249_004.png'], dtype='<U16')}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_list[\"ssl_al\"][\"al labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2abcf545-2c75-478a-9b40-65cf441ed60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00012233_001.png', '00001792_003.png', '00015698_001.png',\n",
       "       '00001722_004.png', '00013443_001.png', '00001470_000.png',\n",
       "       '00018546_002.png', '00018557_011.png', '00019260_007.png',\n",
       "       '00013601_003.png', '00012184_002.png', '00012387_001.png',\n",
       "       '00018829_005.png', '00016052_006.png', '00018019_005.png',\n",
       "       '00014558_011.png', '00014715_011.png', '00001449_002.png',\n",
       "       '00016887_000.png', '00019643_007.png', '00019240_005.png',\n",
       "       '00012010_005.png', '00013073_006.png', '00012010_035.png',\n",
       "       '00001722_004.png', '00013844_001.png', '00019576_023.png',\n",
       "       '00019643_004.png', '00018546_006.png', '00012010_026.png',\n",
       "       '00012219_007.png', '00013916_002.png', '00012543_010.png',\n",
       "       '00016561_003.png', '00018250_000.png', '00013613_013.png',\n",
       "       '00018335_012.png', '00019576_060.png', '00013601_022.png',\n",
       "       '00013613_016.png', '00018329_007.png', '00001249_004.png'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.empty(0, dtype=\"str\")\n",
    "for key, item in labeled_list[\"ssl_al\"][\"al labels\"].items():\n",
    "    result = np.append(result, item)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bf026b2d-1554-4ab6-80f7-11f5e2189427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gt</th>\n",
       "      <th>prediction_4295349121</th>\n",
       "      <th>prediction_4295342357</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00001249_004.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>00001449_002.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>00001470_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>00001722_004.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>00001792_003.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>00012010_005.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>00012010_026.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>00012010_035.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>00012184_002.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>00012219_007.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>00012233_001.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>00012387_001.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>00012543_010.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>00013073_006.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>00013443_001.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>00013601_003.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>00013601_022.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>00013613_013.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>00013613_016.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>00013844_001.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>00013916_002.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>00014558_011.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>00014715_011.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>00015698_001.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>00016052_006.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>00016561_003.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>00016887_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>00018019_005.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>00018250_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>00018329_007.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>00018335_012.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>00018546_002.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>00018546_006.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>00018557_011.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>00018829_005.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>00019240_005.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>00019260_007.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>00019576_023.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>00019576_060.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>00019643_004.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>00019643_007.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename  gt  prediction_4295349121  prediction_4295342357\n",
       "21    00001249_004.png   1                      1                      0\n",
       "34    00001449_002.png   0                      0                      0\n",
       "35    00001470_000.png   0                      0                      0\n",
       "64    00001722_004.png   0                      0                      0\n",
       "70    00001792_003.png   0                      0                      0\n",
       "121   00012010_005.png   1                      1                      0\n",
       "135   00012010_026.png   1                      1                      0\n",
       "142   00012010_035.png   1                      1                      0\n",
       "184   00012184_002.png   0                      0                      0\n",
       "192   00012219_007.png   0                      0                      0\n",
       "197   00012233_001.png   0                      0                      0\n",
       "217   00012387_001.png   0                      0                      0\n",
       "245   00012543_010.png   0                      0                      0\n",
       "354   00013073_006.png   1                      1                      0\n",
       "420   00013443_001.png   0                      0                      0\n",
       "469   00013601_003.png   0                      0                      0\n",
       "484   00013601_022.png   1                      1                      0\n",
       "499   00013613_013.png   1                      1                      1\n",
       "502   00013613_016.png   1                      1                      1\n",
       "526   00013844_001.png   1                      1                      0\n",
       "578   00013916_002.png   0                      0                      0\n",
       "691   00014558_011.png   0                      0                      0\n",
       "738   00014715_011.png   1                      1                      0\n",
       "895   00015698_001.png   0                      0                      0\n",
       "936   00016052_006.png   1                      1                      0\n",
       "978   00016561_003.png   0                      0                      0\n",
       "1058  00016887_000.png   0                      0                      0\n",
       "1228  00018019_005.png   1                      0                      0\n",
       "1263  00018250_000.png   0                      1                      0\n",
       "1278  00018329_007.png   1                      1                      1\n",
       "1285  00018335_012.png   0                      0                      0\n",
       "1334  00018546_002.png   0                      0                      0\n",
       "1336  00018546_006.png   0                      0                      0\n",
       "1349  00018557_011.png   0                      0                      0\n",
       "1410  00018829_005.png   1                      1                      0\n",
       "1489  00019240_005.png   0                      0                      0\n",
       "1495  00019260_007.png   0                      0                      0\n",
       "1575  00019576_023.png   1                      1                      0\n",
       "1604  00019576_060.png   1                      1                      1\n",
       "1617  00019643_004.png   1                      0                      1\n",
       "1620  00019643_007.png   1                      1                      0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_label_df(fullDataset, experts)\n",
    "df[df[\"filename\"].isin(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cde37bcc-c2b1-434f-ba8f-bfe375b2bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDataset():\n",
    "    def __init__(self, data):\n",
    "        self.y = data[\"gt\"]\n",
    "        self.x = data.copy().drop([\"filename\", \"gt\"], axis=1)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return np.array(self.x.iloc[index], dtype=\"float32\"), self.y.iloc[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.y)\n",
    "\n",
    "trainLabels = LabelDataset(df[df[\"filename\"].isin(result)])\n",
    "train_dataloader = DataLoader(datasetLabels, batch_size=8, shuffle=True)\n",
    "\n",
    "valLabels = LabelDataset(create_label_df(fullDataset, experts))\n",
    "val_dataloader = DataLoader(datasetLabels, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "48aa55ac-176e-4668-aac3-1977cc6f1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2),\n",
    "            #nn.Softmax(dim=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = LabelNet()#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "80553ccc-168b-40ea-a33d-536d17c109aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_label_model(model, dataloader):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.7)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for epoch in range(5):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 0 and i != 0:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "            \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "9402017d-2a99-4672-865c-c8b82526bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   101] loss: 0.705\n",
      "[1,   201] loss: 0.694\n",
      "[2,   101] loss: 0.726\n",
      "[2,   201] loss: 0.691\n",
      "[3,   101] loss: 0.724\n",
      "[3,   201] loss: 0.688\n",
      "[4,   101] loss: 0.722\n",
      "[4,   201] loss: 0.689\n",
      "[5,   101] loss: 0.720\n",
      "[5,   201] loss: 0.687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelNet(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4, out_features=4, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LabelNet()\n",
    "train_label_model(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db540b-9387-4503-9acf-56cd92253f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in val_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {total} test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "709b4661-6861-461b-ba55-eaa919d74d99",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[263], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/joli/joli-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/joli/joli-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/home/joli/joli-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/home/joli/joli-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[178], line 7\u001b[0m, in \u001b[0;36mLabelDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39miloc[index], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[index]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a361c7d-1474-4cc3-bbfd-175fc9e5cdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joli-env_kernel",
   "language": "python",
   "name": "joli-env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
