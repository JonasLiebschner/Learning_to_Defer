{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe97cdf-2ce6-44f8-9cc4-286ee98c54f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/joli/joli-env/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390d926d-f1be-4655-9384-aef0905db0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Verma.main_increase_experts_hard_coded as verm\n",
    "import Verma.experts as vexp\n",
    "import Verma.losses as vlos\n",
    "from Verma.utils import AverageMeter, accuracy\n",
    "import Verma.resnet50 as vres\n",
    "from AL.utils import *\n",
    "from AL.metrics import *\n",
    "\n",
    "import NIH.Dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51330197-0c1a-4b7b-ac19-5f83fdbb3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"batch_size\": 64,\n",
    "    \"alpha\": 1.0, #scaling parameter for the loss function, default=1.0\n",
    "    \"epochs\": 150,\n",
    "    \"patience\": 50, #number of patience steps for early stopping the training\n",
    "    \"expert_type\": \"MLPMixer\", #specify the expert type. For the type of experts available, see-> models -> experts. defualt=predict\n",
    "    \"n_classes\": 2, #K for K class classification\n",
    "    \"k\": 0, #\n",
    "    \"n_experts\": 2, #\n",
    "    \"lr\": 0.001, #learning rate\n",
    "    \"weight_decay\": 5e-4, #\n",
    "    \"warmup_epochs\": 5, #\n",
    "    \"loss_type\": \"softmax\", #surrogate loss type for learning to defer\n",
    "    \"ckp_dir\": \"./Models\", #directory name to save the checkpoints\n",
    "    \"experiment_name\": \"multiple_experts\", #specify the experiment name. Checkpoints will be saved with this name\n",
    "    #\n",
    "    \"TRAIN_BATCH_SIZE\": 64,\n",
    "    \"TEST_BATCH_SIZE\": 64,\n",
    "    \"NUM_EXPERTS\": 2,\n",
    "    \"K\": 10,\n",
    "    \"TARGET\": \"Airspace_Opacity\",\n",
    "    \"LABELER_IDS\": [4323195249, 4295232296],\n",
    "    #\n",
    "    \"maxLabels\": 16,\n",
    "    \"PATH\": \"../Datasets/NIH/\"\n",
    "    #\n",
    "    \"Cost\": (9, 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db687a4-4af0-4fc0-81f7-78636323af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec342940-c404-40d9-aa4e-b10a102f569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert:\n",
    "    def __init__(self, dataset, labeler_id, modus=\"perfect\", param=None, nLabels=800, prob=0.5):\n",
    "        self.labelerId = labeler_id\n",
    "        self.dataset = dataset\n",
    "        self.data = dataset.getData()[[\"Image ID\", str(self.labelerId)]]\n",
    "        self.nLabels = nLabels\n",
    "        self.param = param\n",
    "        self.prob = prob\n",
    "        self.modus = modus\n",
    "\n",
    "        if self.modus == \"perfect\":\n",
    "            self.predictions = self.data\n",
    "\n",
    "    def predict(self, img, target, fnames):\n",
    "        \"\"\"\n",
    "        img: the input image\n",
    "        target: the GT label\n",
    "        fname: filename (id for the image)\n",
    "        \"\"\"\n",
    "        return np.array([self.predictions[self.predictions[\"Image ID\"] == image_id][str(self.labelerId)].values for image_id in fnames]).ravel()\n",
    "\n",
    "    def setModel(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def predictModel(self, img, target, fnames):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.unsqueeze(0) \n",
    "        outputs = self.model(img)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted\n",
    "    \n",
    "    def predictImage(self, img):\n",
    "        return self.predictModel(img, None, None)\n",
    "    \n",
    "    def getModel(self):\n",
    "        return self.model\n",
    "    \n",
    "    def saveModel(self, path, name):\n",
    "        torch.save(self.model, PATH + \"/\" + name + \"_\" + str(labeler_id))\n",
    "        \n",
    "    def loadModel(self, path, name):\n",
    "        self.model = torch.load(path + \"/\" + name + \"_\" + str(labeler_id))\n",
    "        model.eval()\n",
    "        \n",
    "    def predictWithModel(self, img, target, filename):\n",
    "        \"\"\"\n",
    "        Checks with the model if the expert would be correct\n",
    "        If it predicts 1 than it returns the true label\n",
    "        If it predicts 0 than is returns the opposit label\n",
    "        \"\"\"\n",
    "        predicted = self.predictModel(self, img, target, fnames)\n",
    "        if predicted == 1:\n",
    "            return target\n",
    "        else:\n",
    "            if target == 1:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f3cd4c1-958c-43b8-9140-f7cbac04ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIHExpertDataset():\n",
    "    def __init__(self, images, filenames, targets, expert_fn, labeled, indices = None, expert_preds = None):\n",
    "        \"\"\"\n",
    "        Original cifar dataset\n",
    "        images: images\n",
    "        targets: labels\n",
    "        expert_fn: expert function\n",
    "        labeled: indicator array if images is labeled\n",
    "        indices: indices in original CIFAR dataset (if this subset is subsampled)\n",
    "        expert_preds: used if expert_fn or have different expert model\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.filenames = filenames\n",
    "        self.targets = np.array(targets)\n",
    "        self.expert_fn = expert_fn\n",
    "        self.labeled = np.array(labeled)\n",
    "        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3]],\n",
    "                                         std=[x / 255.0 for x in [63.0]])\n",
    "        self.transform_test = transforms.Compose([transforms.Resize(128),transforms.ToTensor(), normalize])\n",
    "        if expert_preds is not None:\n",
    "            self.expert_preds = expert_preds\n",
    "        else:\n",
    "            self.expert_preds = np.array(expert_fn(self.images, torch.FloatTensor(targets), self.filenames))\n",
    "        for i in range(len(self.expert_preds)):\n",
    "            if self.labeled[i] == 0:\n",
    "                self.expert_preds[i] = -1 # not labeled by expert\n",
    "        if indices is not None:\n",
    "            self.indices = indices\n",
    "        else:\n",
    "            self.indices = np.array(list(range(len(self.targets))))\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image, label, expert prediction and index in original dataset\"\"\"\n",
    "        label = self.targets[index]\n",
    "        image = self.transform_test(self.images[index])\n",
    "        filename = self.filenames[index]\n",
    "        expert_pred = self.expert_preds[index]\n",
    "        indice = self.indices[index]\n",
    "        labeled = self.labeled[index]\n",
    "        return torch.FloatTensor(image), label, expert_pred, indice, labeled\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "class NIHExpertDatasetMemory():\n",
    "    def __init__(self, images, filenames, targets, expert_fn, labeled, indices = None, expert_preds = None, param=None):\n",
    "        \"\"\"\n",
    "        Original cifar dataset\n",
    "        images: images\n",
    "        targets: labels\n",
    "        expert_fn: expert function\n",
    "        labeled: indicator array if images is labeled\n",
    "        indices: indices in original CIFAR dataset (if this subset is subsampled)\n",
    "        expert_preds: used if expert_fn or have different expert model\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.filenames = filenames\n",
    "        self.targets = np.array(targets)\n",
    "        self.expert_fn = expert_fn\n",
    "        self.labeled = np.array(labeled)\n",
    "        \n",
    "        self.image_ids = filenames\n",
    "        self.preload = False\n",
    "        self.PATH = param[\"PATH\"]\n",
    "        \n",
    "        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3]],\n",
    "                                         std=[x / 255.0 for x in [63.0]])\n",
    "        self.transform_test = transforms.Compose([transforms.Resize(128), transforms.ToTensor(), normalize])\n",
    "        if expert_preds is not None:\n",
    "            self.expert_preds = expert_preds\n",
    "        else:\n",
    "            self.expert_preds = np.array(expert_fn(self.images, torch.FloatTensor(targets), fnames = self.filenames))\n",
    "        for i in range(len(self.expert_preds)):\n",
    "            if self.labeled[i] == 0:\n",
    "                self.expert_preds[i] = -1 # not labeled by expert\n",
    "        if indices is not None:\n",
    "            self.indices = indices\n",
    "        else:\n",
    "            self.indices = np.array(list(range(len(self.targets))))\n",
    "            \n",
    "    def loadImage(self, idx):\n",
    "        \"\"\"\n",
    "        Load one single image\n",
    "        \"\"\"\n",
    "        return Image.open(self.PATH + \"images/\" + self.image_ids[idx]).convert(\"RGB\").resize((244,244))\n",
    "            \n",
    "    def getImage(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the image from index idx\n",
    "        \"\"\"\n",
    "        if self.preload:\n",
    "            return self.images[idx]\n",
    "        else:\n",
    "            return self.loadImage(idx)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image, label, expert prediction and index in original dataset\"\"\"\n",
    "        label = self.targets[index]\n",
    "        img = self.getImage(index)\n",
    "        image = self.transform_test(img)\n",
    "        #image = self.transform_test(self.images[index])\n",
    "        filename = self.filenames[index]\n",
    "        expert_pred = self.expert_preds[index]\n",
    "        indice = self.indices[index]\n",
    "        labeled = self.labeled[index]\n",
    "        return torch.FloatTensor(image), label, expert_pred, indice, labeled, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dee1822-e167-4210-9ffd-c6853de04441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def get_least_confident_points(model, data_loader, budget):\n",
    "    '''\n",
    "    based on entropy score get points, can chagnge, but make sure to get max or min accordingly\n",
    "    '''\n",
    "    uncertainty_estimates = []\n",
    "    indices_all = []\n",
    "    for data in data_loader:\n",
    "        images, labels, expert_preds, indices, _, filenames = data\n",
    "        images, labels, expert_preds = images.to(device), labels.to(device), expert_preds.to(device)\n",
    "        outputs = model(images)\n",
    "        batch_size = outputs.size()[0]  \n",
    "        for i in range(0, batch_size):\n",
    "            output_i =  outputs.data[i].cpu().numpy()\n",
    "            entropy_i = entropy(output_i)\n",
    "            #entropy_i = 1 - max(output_i)\n",
    "            uncertainty_estimates.append(entropy_i)\n",
    "            indices_all.append(indices[i].item())\n",
    "    indices_all = np.array(indices_all)\n",
    "    top_budget_indices = np.argsort(uncertainty_estimates)[-budget:]\n",
    "    actual_indices = indices_all[top_budget_indices]\n",
    "    uncertainty_estimates = np.array(uncertainty_estimates)\n",
    "    return actual_indices\n",
    "import copy\n",
    "EPOCHS_DEFER = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for trial in range(MAX_TRIALS):\n",
    "def getExpertModel(train_dataset, val_dataset, test_dataset, expert, param):\n",
    "    \n",
    "    error_confidence_trials_LCE = []\n",
    "    \n",
    "    #print(f'\\n \\n \\n Trial {trial} \\n \\n \\n ')\n",
    "    # initialize data, Erhält alle Indizes der Daten\n",
    "    all_indices = list(range(len(train_dataset.getAllIndices())))\n",
    "    train_dataset.getAllImagesNP().shape\n",
    "    all_data_x = train_dataset.getAllImagesNP()[all_indices]\n",
    "    all_data_filenames = np.array(train_dataset.getAllFilenames())[all_indices]\n",
    "    all_data_y = np.array(train_dataset.getAllTargets())[all_indices]\n",
    "    \n",
    "    print(\"Complete first data generation\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # Bestimmt die Indizes, welche gelabelt und welche ungelabelt sind\n",
    "    \n",
    "    Intial_random_set = random.sample(all_indices, INITIAL_SIZE)\n",
    "    indices_labeled  = Intial_random_set\n",
    "    indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # Lädt die Datasets für die beschrifteten und unbeschrifteten Daten\n",
    "    #dataset_train_labeled = NIHExpertDataset(all_data_x[indices_labeled], all_data_filenames[indices_labeled], all_data_y[indices_labeled], expert.predict , [1]*len(indices_labeled), indices_labeled)\n",
    "    #dataset_train_unlabeled = NIHExpertDataset(all_data_x[indices_unlabeled], all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], expert.predict , [0]*len(indices_unlabeled), indices_unlabeled)\n",
    "    dataset_train_labeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_labeled], all_data_y[indices_labeled], expert.predict , [1]*len(indices_labeled), indices_labeled, param=param)\n",
    "    dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], expert.predict , [0]*len(indices_unlabeled), indices_unlabeled, param=param)\n",
    "    \n",
    "    \n",
    "    # Lädt die Dataloaders\n",
    "    dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "    \n",
    "    print(\"Complete dataloader generation\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # train expert model on labeled data\n",
    "    # Expertenmodell variabel\n",
    "    model_expert = NetSimple(2, 3, 100, 100, 1000,500).to(device)\n",
    "    #model_expert = NetSimple(1, 3, 50, 50, 500,256).to(device)\n",
    "    # Trainier Modell um Experten vorherzusagen\n",
    "    \n",
    "    param_size = 0\n",
    "    for paramn in model_expert.parameters():\n",
    "        param_size += paramn.nelement() * paramn.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model_expert.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "    \n",
    "    run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled) \n",
    "    \n",
    "    print(\"Expert trained\")\n",
    "\n",
    "    data_sizes = []\n",
    "    error_confidence = []\n",
    "    data_sizes.append(INITIAL_SIZE)\n",
    "    # train model to do classification & Rejector\n",
    "    #model_lce = NetSimple(n_dataset + 1, 3, 100, 100, 1000,500).to(device)\n",
    "    model_lce = NetSimple(n_dataset + 1, 3, 50, 50, 500,256).to(device)\n",
    "\n",
    "    #TODO: Dataloader erstellen\n",
    "    gc.collect()\n",
    "    \n",
    "    train_indices = list(range(len(train_dataset.getAllIndices())))\n",
    "    val_indices = list(range(len(val_dataset.getAllIndices())))\n",
    "    test_indices = list(range(len(test_dataset.getAllIndices())))\n",
    "\n",
    "    #dataset_train = NIHExpertDataset(train_dataset.getAllImagesNP()[train_indices], np.array(train_dataset.getAllFilenames())[train_indices], np.array(train_dataset.getAllTargets())[train_indices], Expert.predict , [1]*len(train_indices))\n",
    "    #dataset_val = NIHExpertDataset(train_dataset.getAllImagesNP()[val_indices], np.array(train_dataset.getAllFilenames())[val_indices], np.array(train_dataset.getAllTargets())[val_indices], Expert.predict , [1]*len(val_indices))\n",
    "    dataset_train = NIHExpertDatasetMemory(None, np.array(train_dataset.getAllFilenames()), np.array(train_dataset.getAllTargets()), expert.predict , [1]*len(train_indices), param=param)\n",
    "    dataset_val = NIHExpertDatasetMemory(None, np.array(val_dataset.getAllFilenames()), np.array(val_dataset.getAllTargets()), expert.predict , [1]*len(val_indices), param=param)\n",
    "    dataset_test = NIHExpertDatasetMemory(None, np.array(test_dataset.getAllFilenames()), np.array(test_dataset.getAllTargets()), expert.predict , [1]*len(test_indices), param=param)\n",
    "    \n",
    "    dataLoaderTrain = DataLoader(dataset=dataset_train, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    dataLoaderVal = DataLoader(dataset=dataset_val, batch_size=BATCH_SIZE, shuffle=False,  num_workers=0, pin_memory=True)\n",
    "    dataLoaderTest = DataLoader(dataset=dataset_test, batch_size=BATCH_SIZE, shuffle=False,  num_workers=0, pin_memory=True)\n",
    "    ##\n",
    "    \n",
    "    run_reject_class(model_lce, EPOCH_TRAIN, dataLoaderTrain, dataLoaderVal)\n",
    "    model_lce_saved = copy.deepcopy(model_lce.state_dict())\n",
    "    \n",
    "    print(\"Rejector trained\")\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    # get expert model predictions on unlabeled data\n",
    "    dataLoaderTrainUnlabeledUnshuffled = DataLoader(dataset=dataset_train_unlabeled, batch_size=BATCH_SIZE, shuffle=False,  num_workers=0, pin_memory=True)\n",
    "    expert_preds_arr = []\n",
    "    for data in dataLoaderTrainUnlabeledUnshuffled:\n",
    "        images, labels, _, _, _, filenames = data\n",
    "        images = images.to(device)\n",
    "        outputs_exp = model_expert(images)\n",
    "        for i in range(outputs_exp.size()[0]):\n",
    "            #pred_exp = np.argmax(outputs_exp.data[i].cpu().numpy())\n",
    "            pred_exp = outputs_exp.data[i].cpu().numpy()\n",
    "            pred_exp = pred_exp[1]\n",
    "            expert_preds_arr.append(pred_exp)\n",
    "    expert_preds_unlabeled = np.array(expert_preds_arr)\n",
    "    expert_preds_labeled = np.array(expert.predict(all_data_x[indices_labeled], torch.FloatTensor(all_data_y[indices_labeled]), all_data_filenames[indices_labeled]))\n",
    "    expert_preds_labeled = ( expert_preds_labeled == all_data_y[indices_labeled]) * 1\n",
    "    expert_preds_combined = np.concatenate(( expert_preds_labeled, expert_preds_unlabeled))\n",
    "    \n",
    "    print(\"Got predictions for all data\")\n",
    "    \n",
    "    # create pseudo-labeled dataset\n",
    "    \"\"\"dataset_train_pseudolabeled = NIHExpertDataset(np.concatenate((all_data_x[indices_labeled], all_data_x[indices_unlabeled])),\n",
    "                                                     np.concatenate((all_data_filenames[indices_labeled], all_data_filenames[indices_unlabeled])),\n",
    "                                                        np.concatenate((all_data_y[indices_labeled] , all_data_y[indices_unlabeled])), \n",
    "                                                     expert.predict , [1]*(len(indices_labeled) + len(indices_unlabeled)), None,\n",
    "                                                        expert_preds_combined)\"\"\"\n",
    "    dataset_train_pseudolabeled = NIHExpertDatasetMemory(None,\n",
    "                                                     np.concatenate((all_data_filenames[indices_labeled], all_data_filenames[indices_unlabeled])),\n",
    "                                                        np.concatenate((all_data_y[indices_labeled] , all_data_y[indices_unlabeled])), \n",
    "                                                     expert.predict , [1]*(len(indices_labeled) + len(indices_unlabeled)), None,\n",
    "                                                        expert_preds_combined, param=param)\n",
    "    \n",
    "    dataLoaderTrainPseudoLabeled = DataLoader(dataset=dataset_train_pseudolabeled, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "\n",
    "    # train model on pseudo-labeled data\n",
    "    run_reject_pseudo(model_lce, n_dataset, expert.predict, EPOCHS_DEFER, 1, dataLoaderTrainPseudoLabeled, dataLoaderTrainLabeled)\n",
    "    \n",
    "    print(\"Model with pseudo labels trained\")\n",
    "    \n",
    "    metrics_confidence = metrics_print(model_lce, expert.predict, n_dataset, dataLoaderTest)\n",
    "    error_confidence.append(metrics_confidence['system accuracy'])\n",
    "    \n",
    "    print(\"Starting with AL\")\n",
    "    for round in range(MAX_ROUNDS):\n",
    "        model_lce.load_state_dict(model_lce_saved)\n",
    "        # get points where expert model is least confident on\n",
    "        print(f'\\n \\n Round {round} \\n \\n')\n",
    "        #indices_confidence =  random.sample(indices_unlabeled, BATCH_SIZE_AL)#\n",
    "        indices_confidence = get_least_confident_points(model_expert, dataLoaderTrainUnlabeled, BATCH_SIZE_AL)\n",
    "        indices_labeled  = indices_labeled + list(indices_confidence) \n",
    "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
    "        \n",
    "        dataset_train_labeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_labeled], all_data_y[indices_labeled], expert.predict , [1]*len(indices_labeled), indices_labeled, param=param)\n",
    "        dataset_train_unlabeled = NIHExpertDatasetMemory(None, all_data_filenames[indices_unlabeled], all_data_y[indices_unlabeled], expert.predict , [0]*len(indices_unlabeled), indices_unlabeled, param=param)\n",
    "        \n",
    "        dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "        # train model on labeled data\n",
    "        run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled)\n",
    "        # get expert predictions on unlabeled data\n",
    "        dataLoaderTrainUnlabeledUnshuffled = DataLoader(dataset=dataset_train_unlabeled, batch_size=BATCH_SIZE, shuffle=False,  num_workers=0, pin_memory=True)\n",
    "        expert_preds_arr = []\n",
    "        for data in dataLoaderTrainUnlabeledUnshuffled:\n",
    "            images, labels, _, _, _, filenames = data\n",
    "            images = images.to(device)\n",
    "            outputs_exp = model_expert(images)\n",
    "            for i in range(outputs_exp.size()[0]):\n",
    "                #pred_exp = np.argmax(outputs_exp.data[i].cpu().numpy())\n",
    "                pred_exp = outputs_exp.data[i].cpu().numpy()\n",
    "                pred_exp = pred_exp[1]\n",
    "                expert_preds_arr.append(pred_exp)\n",
    "        expert_preds_unlabeled = np.array(expert_preds_arr)\n",
    "        expert_preds_labeled = np.array(expert.predict (all_data_x[indices_labeled], torch.FloatTensor(all_data_y[indices_labeled]), all_data_filenames[indices_labeled]))\n",
    "        expert_preds_labeled = ( expert_preds_labeled == all_data_y[indices_labeled]) * 1\n",
    "        expert_preds_combined = np.concatenate(( expert_preds_labeled, expert_preds_unlabeled))\n",
    "        # create pseudo-labeled dataset\n",
    "        \n",
    "        \"\"\"dataset_train_pseudolabeled = NIHExpertDataset(np.concatenate((all_data_x[indices_labeled] , all_data_x[indices_unlabeled])),\n",
    "                                                         np.concatenate((all_data_filenames[indices_labeled] , all_data_filenames[indices_unlabeled])),\n",
    "                                                         np.concatenate((all_data_y[indices_labeled] , all_data_y[indices_unlabeled])), expert.predict , [1]*(len(indices_labeled) + len(indices_unlabeled))  , None,\n",
    "                                                         expert_preds_combined)\"\"\"\n",
    "        \n",
    "        dataset_train_pseudolabeled = NIHExpertDatasetMemory(None,\n",
    "                                                     np.concatenate((all_data_filenames[indices_labeled], all_data_filenames[indices_unlabeled])),\n",
    "                                                        np.concatenate((all_data_y[indices_labeled] , all_data_y[indices_unlabeled])), \n",
    "                                                     expert.predict , [1]*(len(indices_labeled) + len(indices_unlabeled)), None,\n",
    "                                                        expert_preds_combined, param=param)\n",
    "        \n",
    "        dataLoaderTrainPseudoLabeled = DataLoader(dataset=dataset_train_pseudolabeled, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "\n",
    "        # train model on pseudo labeled data\n",
    "        best_score = 0\n",
    "        best_model = None\n",
    "        for alpha in [1]:\n",
    "            print(f'alpha {alpha}')\n",
    "            model_lce.load_state_dict(model_lce_saved)\n",
    "            model_dict_alpha = run_reject_pseudo(model_lce, n_dataset, expert.predict, EPOCHS_DEFER, 1, dataLoaderTrainPseudoLabeled, dataLoaderTest, True, EPOCHS_DEFER-1)\n",
    "            model_lce.load_state_dict(model_dict_alpha)\n",
    "            score = metrics_print(model_lce, expert.predict, n_dataset, dataLoaderTest)['system accuracy']\n",
    "            if score >= best_score:\n",
    "                best_score =  score\n",
    "                best_model = model_dict_alpha\n",
    "        model_lce.load_state_dict(best_model)\n",
    "\n",
    "        #run_reject(model_lce, 10, Expert.predict, EPOCHS_DEFER, 1, dataLoaderTrainPseudoLabeled, dataLoaderTrainLabeled)\n",
    "        metrics_confidence = metrics_print(model_lce, expert.predict, n_dataset, dataLoaderTest)\n",
    "        error_confidence.append(metrics_confidence['system accuracy'])\n",
    "        data_sizes.append((round+1)*BATCH_SIZE_AL + INITIAL_SIZE)\n",
    "    \n",
    "    error_confidence_trials_LCE.append(error_confidence)\n",
    "    return model_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a31ef22-2255-40c8-b139-988511547d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_expert_confidence(train_loader, model, optimizer, scheduler, epoch, apply_softmax, param=None):\n",
    "    \"\"\"Train for one epoch the model to predict expert agreement with label\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, label, expert_pred, _, _, filenames ) in enumerate(train_loader):\n",
    "        #print(input)\n",
    "        #print(label)\n",
    "        expert_pred = expert_pred.long()\n",
    "        expert_pred = (expert_pred == label) *1\n",
    "        target = expert_pred.to(device)\n",
    "        input = input.to(device)\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # compute loss\n",
    "        \n",
    "        if apply_softmax:\n",
    "            loss = my_CrossEntropyLossWithSoftmax(output, target)\n",
    "        else:\n",
    "            #loss = my_CrossEntropyLoss(output, target)\n",
    "            loss = my_CrossEntropyLoss(output, target, cost=param[\"Cost\"])\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                loss=losses, top1=top1))\n",
    "            \n",
    "\n",
    "def run_expert(model, epochs, train_loader, val_loader, apply_softmax = False):\n",
    "    '''\n",
    "    train expert model to predict disagreement with label\n",
    "    model: WideResNet model or pytorch model (2 outputs)\n",
    "    epochs: number of epochs to train\n",
    "    '''\n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.001, #0.001\n",
    "                                momentum=0.9, nesterov=True,\n",
    "                                weight_decay=5e-4)\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs)\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        # train for one epoch\n",
    "        train_expert_confidence(train_loader, model, optimizer, scheduler, epoch, apply_softmax)\n",
    "        if epoch % 10 == 0:\n",
    "            metrics_print_expert(model, val_loader)\n",
    "    metrics_print_expert(model, val_loader)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "515587c9-0cba-422f-af72-383907bd4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reject_class(model, epochs, train_loader, val_loader, apply_softmax = False):\n",
    "    '''\n",
    "    only train classifier\n",
    "    model: WideResNet model\n",
    "    epochs: number of epochs to train\n",
    "    train_loader:\n",
    "    val_loader:\n",
    "    apply_softmax: apply softmax on top of model\n",
    "    '''\n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "\n",
    "\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs)\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        # train for one epoch\n",
    "        train_reject_class(train_loader, model, optimizer, scheduler, epoch, apply_softmax)\n",
    "        #if epoch % 10 == 0:\n",
    "            #metrics_print_classifier(model, val_loader)\n",
    "\n",
    "def train_reject_class(train_loader, model, optimizer, scheduler, epoch, apply_softmax):\n",
    "    \"\"\"Train for one epoch on the training set without deferral\n",
    "    apply_softmax: boolean to apply softmax, if model last layer doesn't have softmax \n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target, expert, _, _, filenames ) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # compute loss\n",
    "        if apply_softmax:\n",
    "            loss = my_CrossEntropyLossWithSoftmax(output, target)\n",
    "        else:\n",
    "            loss = my_CrossEntropyLoss(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                loss=losses, top1=top1))\n",
    "\n",
    "def train_reject_pseudo(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes, alpha):\n",
    "    \"\"\"Train for one epoch on the training set with deferral with pseudo labels\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target, expert, _, _, filenames ) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "        m = expert.to(device)\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # get expert  predictions and costs\n",
    "        batch_size = output.size()[0]  # batch_size\n",
    "        m2 = [1] * batch_size\n",
    "\n",
    "        #m = torch.tensor(m)\n",
    "        #m2 = torch.tensor(m2)\n",
    "        \n",
    "        m = m.clone().detach().requires_grad_(True)\n",
    "        m2 = m2.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        m = m.to(device)\n",
    "        m2 = m2.to(device)\n",
    "        # done getting expert predictions and costs \n",
    "        # compute loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "def run_reject_pseudo(model, n_dataset, expert_fn, epochs, alpha, train_loader, val_loader, best_on_val = False, epoch_freq = 10):\n",
    "    '''\n",
    "    This trains the model with labeled and pseudo labeled data, same mechanics as run_reject\n",
    "    '''\n",
    "    # Data loading code\n",
    "   \n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # for training on multiple GPUs.\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs)\n",
    "    \n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    best_val_score = 0\n",
    "    for epoch in range(0, epochs):\n",
    "        # train for one epoch\n",
    "        train_reject_pseudo(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset, alpha)\n",
    "        if epoch % epoch_freq == 0:\n",
    "            score = metrics_print(model, expert_fn, n_dataset, val_loader)['system accuracy']\n",
    "            if score > best_val_score:\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "    if best_on_val:\n",
    "        return  best_model \n",
    "    \n",
    "def reject_CrossEntropyLoss(outputs, m, labels, m2, n_classes):\n",
    "    '''\n",
    "    The L_{CE} loss implementation for CIFAR\n",
    "    ----\n",
    "    outputs: network outputs\n",
    "    m: cost of deferring to expert cost of classifier predicting (I_{m =y})\n",
    "    labels: target\n",
    "    m2:  cost of classifier predicting (alpha* I_{m\\neq y} + I_{m =y})\n",
    "    n_classes: number of classes\n",
    "    '''\n",
    "    batch_size = outputs.size()[0]  # batch_size\n",
    "    rc = [n_classes] * batch_size\n",
    "    outputs = -m * torch.log2(outputs[range(batch_size), rc]) - m2 * torch.log2(\n",
    "        outputs[range(batch_size), labels])  \n",
    "    return torch.mean(outputs)\n",
    "\n",
    "def metrics_print(net, expert_fn, n_classes, loader):\n",
    "    '''\n",
    "    Computes metrics for deferal (L_{CE} loss method)\n",
    "    -----\n",
    "    Arguments:\n",
    "    net: model\n",
    "    expert_fn: expert model\n",
    "    n_classes: number of classes\n",
    "    loader: data loader\n",
    "    '''\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    alone_correct = 0\n",
    "    correct_pred = {classname: 0 for classname in cifar_classes}\n",
    "    total_pred = {classname: 0 for classname in cifar_classes}\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels, _, _ ,_, filenames = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            batch_size = outputs.size()[0]  # batch_size\n",
    "            exp_prediction = expert_fn(images, labels, filenames)\n",
    "            for i in range(0, batch_size):\n",
    "                r = (predicted[i].item() == n_classes)\n",
    "                prediction = predicted[i]\n",
    "                final_pred = 0\n",
    "                if predicted[i] == n_classes:\n",
    "                    max_idx = 0\n",
    "                    # get second max\n",
    "                    for j in range(0, n_classes):\n",
    "                        if outputs.data[i][j] >= outputs.data[i][max_idx]:\n",
    "                            max_idx = j\n",
    "                    prediction = max_idx\n",
    "                else:\n",
    "                    prediction = predicted[i]\n",
    "                alone_correct += (prediction == labels[i]).item()\n",
    "                if r == 0:\n",
    "                    total += 1\n",
    "                    final_pred = predicted[i]\n",
    "                    correct += (predicted[i] == labels[i]).item()\n",
    "                    correct_sys += (predicted[i] == labels[i]).item()\n",
    "                if r == 1:\n",
    "                    final_pred = exp_prediction[i]\n",
    "                    exp += (exp_prediction[i] == labels[i].item())\n",
    "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
    "                    exp_total += 1\n",
    "                real_total += 1\n",
    "                if labels[i].item() == final_pred:\n",
    "                    correct_pred[cifar_classes[labels[i].item()]] += 1\n",
    "                total_pred[cifar_classes[labels[i].item()]] += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
    "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                \"classifier accuracy\": 100 * correct / (total + 0.0001),\n",
    "                \"alone classifier\": 100 * alone_correct / real_total}\n",
    "    print(to_print)\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"Accuracy for class {:5s} is: {:.3f} %\".format(classname,\n",
    "                                                    accuracy))\n",
    "    return to_print\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def metrics_print_expert(model, data_loader, defer_net = False):\n",
    "    '''\n",
    "    Computes metrics for expert model error prediction\n",
    "    model: model\n",
    "    data_loader: data loader\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    label_list = np.empty(0)\n",
    "    predictions_list = np.empty(0)\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, label, expert_pred, _ ,_, filenames = data\n",
    "            expert_pred = expert_pred.long()\n",
    "            expert_pred = (expert_pred == label) *1\n",
    "            images, labels = images.to(device), expert_pred.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs.data, 1) # maybe no .data\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            \n",
    "            label_list = np.concatenate((label_list, labels.cpu().numpy()), axis=0)\n",
    "            predictions_list = np.concatenate((predictions_list, predictions.cpu().numpy()), axis=0)\n",
    "\n",
    "    print('Accuracy of the network on the %d test images: %.3f %%' % (total,\n",
    "        100 * correct / total))\n",
    "    \n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(label_list, predictions_list).ravel()\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(sklearn.metrics.confusion_matrix(label_list, predictions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fba3560-e7ac-473a-8e61-2c33ec1239cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = param\n",
    "basic_Dataset = ds.BasicDataset(param[\"PATH\"], \"Airspace_Opacity\")\n",
    "nih_dataloader = ds.NIH_K_Fold_Dataloader(\n",
    "            dataset = basic_Dataset,\n",
    "            k = params[\"K\"],\n",
    "            labelerIds = params[\"LABELER_IDS\"],\n",
    "            train_batch_size = params[\"TRAIN_BATCH_SIZE\"],\n",
    "            test_batch_size = params[\"TEST_BATCH_SIZE\"],\n",
    "            #seed = seed,\n",
    "            #maxLabels = maxL,\n",
    "            #preload = True,\n",
    "            #prebuild = True,\n",
    "            param = params\n",
    "        )\n",
    "expert_train, expert_val, expert_test = nih_dataloader.get_dataset_for_folder(1)\n",
    "expert_train_dataset = ds.NIHDataset(expert_train, preload=False, preprocess=False, param=param)\n",
    "expert_val_dataset = ds.NIHDataset(expert_val, preload=False, preprocess=False, param=param)\n",
    "expert_test_dataset = ds.NIHDataset(expert_test, preload=False, preprocess=False, param=param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9f32e31-e2d5-4fd9-928c-f12c86c91e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdabcf74-7228-4f68-bfaa-29116ea23f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5667aef5-d269-4620-a1af-1d4d92cdb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_expert = Expert(dataset = basic_Dataset, labeler_id=4323195249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f7316d-c637-4c12-ba25-f24287ec88b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INITIAL_SIZE = 64\n",
    "EPOCH_TRAIN = 5\n",
    "n_dataset = 2\n",
    "BATCH_SIZE = 16\n",
    "MAX_ROUNDS = 4\n",
    "BATCH_SIZE_AL = 8\n",
    "EPOCHS_DEFER = 5\n",
    "\n",
    "del basic_Dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199d8deb-90c9-425e-a269-74175ac6815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from AL.neural_network import NetSimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1998f6d-7af8-4fa0-adee-4ce824af2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89bb507f-de1c-4ecd-8ca4-2cb9ef867b00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete first data generation\n",
      "Complete dataloader generation\n",
      "model size: 323.716MB\n",
      "Number of model parameters: 84860202\n",
      "Epoch: [0][0/4]\tTime 1.611 (1.611)\tLoss 0.9828 (0.9828)\tPrec@1 100.000 (100.000)\n",
      "Accuracy of the network on the 64 test images: 95.312 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 61]]\n",
      "Epoch: [1][0/4]\tTime 0.343 (0.343)\tLoss 0.8828 (0.8828)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [2][0/4]\tTime 0.333 (0.333)\tLoss 0.4901 (0.4901)\tPrec@1 100.000 (100.000)\n",
      "Epoch: [3][0/4]\tTime 0.337 (0.337)\tLoss 0.3438 (0.3438)\tPrec@1 100.000 (100.000)\n",
      "Epoch: [4][0/4]\tTime 0.340 (0.340)\tLoss 0.8467 (0.8467)\tPrec@1 87.500 (87.500)\n",
      "Accuracy of the network on the 64 test images: 95.312 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 61]]\n",
      "Expert trained\n",
      "Number of model parameters: 21220877\n",
      "Epoch: [0][0/38]\tTime 0.326 (0.326)\tLoss 1.5545 (1.5545)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [0][10/38]\tTime 0.319 (0.324)\tLoss 1.2042 (1.7487)\tPrec@1 50.000 (54.545)\n",
      "Epoch: [0][20/38]\tTime 0.331 (0.324)\tLoss 0.9553 (1.3873)\tPrec@1 50.000 (56.250)\n",
      "Epoch: [0][30/38]\tTime 0.329 (0.323)\tLoss 0.8609 (1.2428)\tPrec@1 75.000 (58.266)\n",
      "Epoch: [1][0/38]\tTime 0.317 (0.317)\tLoss 0.7853 (0.7853)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [1][10/38]\tTime 0.324 (0.320)\tLoss 0.7850 (0.8160)\tPrec@1 75.000 (77.273)\n",
      "Epoch: [1][20/38]\tTime 0.332 (0.324)\tLoss 0.7961 (0.7728)\tPrec@1 68.750 (75.298)\n",
      "Epoch: [1][30/38]\tTime 0.321 (0.323)\tLoss 0.9056 (0.8175)\tPrec@1 62.500 (73.185)\n",
      "Epoch: [2][0/38]\tTime 0.322 (0.322)\tLoss 0.7910 (0.7910)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [2][10/38]\tTime 0.331 (0.326)\tLoss 0.8084 (0.7474)\tPrec@1 81.250 (75.568)\n",
      "Epoch: [2][20/38]\tTime 0.333 (0.324)\tLoss 1.0406 (0.7881)\tPrec@1 62.500 (76.190)\n",
      "Epoch: [2][30/38]\tTime 0.333 (0.323)\tLoss 0.7250 (0.7600)\tPrec@1 62.500 (74.395)\n",
      "Epoch: [3][0/38]\tTime 0.315 (0.315)\tLoss 0.6297 (0.6297)\tPrec@1 81.250 (81.250)\n",
      "Epoch: [3][10/38]\tTime 0.317 (0.324)\tLoss 0.6254 (0.6220)\tPrec@1 81.250 (80.682)\n",
      "Epoch: [3][20/38]\tTime 0.321 (0.321)\tLoss 0.6665 (0.6884)\tPrec@1 81.250 (78.869)\n",
      "Epoch: [3][30/38]\tTime 0.318 (0.322)\tLoss 0.6014 (0.6773)\tPrec@1 81.250 (78.831)\n",
      "Epoch: [4][0/38]\tTime 0.318 (0.318)\tLoss 0.5728 (0.5728)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [4][10/38]\tTime 0.319 (0.322)\tLoss 0.5892 (0.6547)\tPrec@1 87.500 (81.250)\n",
      "Epoch: [4][20/38]\tTime 0.313 (0.321)\tLoss 0.7201 (0.6765)\tPrec@1 81.250 (80.357)\n",
      "Epoch: [4][30/38]\tTime 0.316 (0.321)\tLoss 0.8630 (0.7078)\tPrec@1 62.500 (77.218)\n",
      "Rejector trained\n",
      "Got predictions for all data\n",
      "Number of model parameters: 21220877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1968130/3908595283.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  m = torch.tensor(m)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/38]\tTime 0.324 (0.324)\tLoss 13.2608 (13.2608)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [0][10/38]\tTime 0.318 (0.322)\tLoss 2.9569 (4.2669)\tPrec@1 68.750 (58.523)\n",
      "Epoch: [0][20/38]\tTime 0.316 (0.322)\tLoss 2.9499 (3.6457)\tPrec@1 0.000 (55.655)\n",
      "Epoch: [0][30/38]\tTime 0.330 (0.321)\tLoss 2.8366 (3.4112)\tPrec@1 0.000 (37.702)\n",
      "{'coverage': '0 out of64', 'system accuracy': 95.3125, 'expert accuracy': 95.31220214936828, 'classifier accuracy': 0.0, 'alone classifier': 57.8125}\n",
      "Accuracy for class GT 0  is: 96.296 %\n",
      "Accuracy for class GT 1  is: 94.595 %\n",
      "Epoch: [1][0/38]\tTime 0.330 (0.330)\tLoss 2.8838 (2.8838)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][10/38]\tTime 0.312 (0.324)\tLoss 2.8624 (2.8702)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][20/38]\tTime 0.329 (0.324)\tLoss 2.7478 (2.8586)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][30/38]\tTime 0.321 (0.323)\tLoss 2.8863 (2.8530)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][0/38]\tTime 0.329 (0.329)\tLoss 2.8431 (2.8431)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][10/38]\tTime 0.325 (0.324)\tLoss 2.8521 (2.8500)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][20/38]\tTime 0.315 (0.323)\tLoss 2.8540 (2.8483)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][30/38]\tTime 0.318 (0.323)\tLoss 2.8754 (2.8545)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][0/38]\tTime 0.325 (0.325)\tLoss 2.8267 (2.8267)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][10/38]\tTime 0.332 (0.324)\tLoss 2.8094 (2.8298)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][20/38]\tTime 0.319 (0.323)\tLoss 2.8268 (2.8428)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][30/38]\tTime 0.332 (0.323)\tLoss 2.8121 (2.8456)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][0/38]\tTime 0.326 (0.326)\tLoss 2.8313 (2.8313)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][10/38]\tTime 0.329 (0.324)\tLoss 2.7966 (2.8428)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][20/38]\tTime 0.325 (0.324)\tLoss 2.8558 (2.8506)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][30/38]\tTime 0.316 (0.323)\tLoss 2.7453 (2.8456)\tPrec@1 0.000 (0.000)\n",
      "Model with pseudo labels trained\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "Starting with AL\n",
      "\n",
      " \n",
      " Round 0 \n",
      " \n",
      "\n",
      "Number of model parameters: 84860202\n",
      "Epoch: [0][0/5]\tTime 0.339 (0.339)\tLoss 0.2632 (0.2632)\tPrec@1 100.000 (100.000)\n",
      "Accuracy of the network on the 72 test images: 95.833 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 69]]\n",
      "Epoch: [1][0/5]\tTime 0.323 (0.323)\tLoss 0.1707 (0.1707)\tPrec@1 100.000 (100.000)\n",
      "Epoch: [2][0/5]\tTime 0.321 (0.321)\tLoss 0.9835 (0.9835)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [3][0/5]\tTime 0.343 (0.343)\tLoss 0.5106 (0.5106)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [4][0/5]\tTime 0.328 (0.328)\tLoss 0.1620 (0.1620)\tPrec@1 100.000 (100.000)\n",
      "Accuracy of the network on the 72 test images: 95.833 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 69]]\n",
      "alpha 1\n",
      "Number of model parameters: 21220877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1968130/3908595283.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  m = torch.tensor(m)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/38]\tTime 0.325 (0.325)\tLoss 14.8673 (14.8673)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [0][10/38]\tTime 0.318 (0.325)\tLoss 3.0831 (4.5391)\tPrec@1 50.000 (56.250)\n",
      "Epoch: [0][20/38]\tTime 0.314 (0.323)\tLoss 3.0792 (3.8439)\tPrec@1 43.750 (56.548)\n",
      "Epoch: [0][30/38]\tTime 0.320 (0.322)\tLoss 3.0065 (3.5803)\tPrec@1 0.000 (38.306)\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "Epoch: [1][0/38]\tTime 0.335 (0.335)\tLoss 2.9744 (2.9744)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][10/38]\tTime 0.322 (0.324)\tLoss 2.9167 (2.9442)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][20/38]\tTime 0.325 (0.324)\tLoss 2.8539 (2.9306)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][30/38]\tTime 0.317 (0.323)\tLoss 2.9330 (2.9287)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][0/38]\tTime 0.314 (0.314)\tLoss 2.9063 (2.9063)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][10/38]\tTime 0.322 (0.324)\tLoss 2.9151 (2.9286)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][20/38]\tTime 0.338 (0.325)\tLoss 2.8925 (2.9295)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][30/38]\tTime 0.319 (0.324)\tLoss 2.9225 (2.9224)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][0/38]\tTime 0.318 (0.318)\tLoss 2.9463 (2.9463)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][10/38]\tTime 0.319 (0.322)\tLoss 2.9632 (2.9218)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][20/38]\tTime 0.324 (0.322)\tLoss 2.9311 (2.9192)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][30/38]\tTime 0.322 (0.325)\tLoss 2.8702 (2.9204)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][0/38]\tTime 0.341 (0.341)\tLoss 2.8885 (2.8885)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][10/38]\tTime 0.312 (0.322)\tLoss 2.9576 (2.9257)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][20/38]\tTime 0.321 (0.323)\tLoss 2.8580 (2.9212)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][30/38]\tTime 0.322 (0.324)\tLoss 2.9168 (2.9227)\tPrec@1 0.000 (0.000)\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "\n",
      " \n",
      " Round 1 \n",
      " \n",
      "\n",
      "Number of model parameters: 84860202\n",
      "Epoch: [0][0/5]\tTime 0.340 (0.340)\tLoss 0.1386 (0.1386)\tPrec@1 100.000 (100.000)\n",
      "Accuracy of the network on the 80 test images: 96.250 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 77]]\n",
      "Epoch: [1][0/5]\tTime 0.331 (0.331)\tLoss 0.9565 (0.9565)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [2][0/5]\tTime 0.337 (0.337)\tLoss 0.8890 (0.8890)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [3][0/5]\tTime 0.336 (0.336)\tLoss 0.1261 (0.1261)\tPrec@1 100.000 (100.000)\n",
      "Epoch: [4][0/5]\tTime 0.341 (0.341)\tLoss 0.1638 (0.1638)\tPrec@1 100.000 (100.000)\n",
      "Accuracy of the network on the 80 test images: 96.250 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 77]]\n",
      "alpha 1\n",
      "Number of model parameters: 21220877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1968130/3908595283.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  m = torch.tensor(m)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/38]\tTime 0.328 (0.328)\tLoss 14.9479 (14.9479)\tPrec@1 87.500 (87.500)\n",
      "Epoch: [0][10/38]\tTime 0.322 (0.323)\tLoss 3.1199 (4.5995)\tPrec@1 50.000 (59.659)\n",
      "Epoch: [0][20/38]\tTime 0.320 (0.323)\tLoss 3.0711 (3.8775)\tPrec@1 0.000 (55.952)\n",
      "Epoch: [0][30/38]\tTime 0.315 (0.322)\tLoss 3.0021 (3.6021)\tPrec@1 0.000 (37.903)\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "Epoch: [1][0/38]\tTime 0.330 (0.330)\tLoss 2.9623 (2.9623)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][10/38]\tTime 0.326 (0.324)\tLoss 2.8725 (2.9382)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][20/38]\tTime 0.322 (0.323)\tLoss 2.9153 (2.9387)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][30/38]\tTime 0.316 (0.322)\tLoss 2.9140 (2.9398)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][0/38]\tTime 0.332 (0.332)\tLoss 2.9297 (2.9297)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][10/38]\tTime 0.320 (0.324)\tLoss 2.9129 (2.9355)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][20/38]\tTime 0.314 (0.324)\tLoss 2.9592 (2.9296)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][30/38]\tTime 0.327 (0.323)\tLoss 2.9139 (2.9279)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][0/38]\tTime 0.319 (0.319)\tLoss 2.9271 (2.9271)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][10/38]\tTime 0.325 (0.325)\tLoss 2.7867 (2.9133)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][20/38]\tTime 0.316 (0.324)\tLoss 2.9379 (2.9200)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][30/38]\tTime 0.318 (0.324)\tLoss 2.9389 (2.9246)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][0/38]\tTime 0.326 (0.326)\tLoss 2.9720 (2.9720)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][10/38]\tTime 0.318 (0.323)\tLoss 2.8629 (2.9207)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][20/38]\tTime 0.331 (0.324)\tLoss 2.9305 (2.9281)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][30/38]\tTime 0.319 (0.324)\tLoss 2.8952 (2.9262)\tPrec@1 0.000 (0.000)\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "\n",
      " \n",
      " Round 2 \n",
      " \n",
      "\n",
      "Number of model parameters: 84860202\n",
      "Epoch: [0][0/6]\tTime 0.323 (0.323)\tLoss 0.6327 (0.6327)\tPrec@1 93.750 (93.750)\n",
      "Accuracy of the network on the 88 test images: 96.591 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 85]]\n",
      "Epoch: [1][0/6]\tTime 0.331 (0.331)\tLoss 0.5065 (0.5065)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [2][0/6]\tTime 0.324 (0.324)\tLoss 0.5122 (0.5122)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [3][0/6]\tTime 0.329 (0.329)\tLoss 0.4968 (0.4968)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [4][0/6]\tTime 0.329 (0.329)\tLoss 0.1301 (0.1301)\tPrec@1 100.000 (100.000)\n",
      "Accuracy of the network on the 88 test images: 96.591 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 85]]\n",
      "alpha 1\n",
      "Number of model parameters: 21220877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1968130/3908595283.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  m = torch.tensor(m)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/38]\tTime 0.320 (0.320)\tLoss 14.1952 (14.1952)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [0][10/38]\tTime 0.321 (0.321)\tLoss 3.1377 (4.5373)\tPrec@1 37.500 (57.386)\n",
      "Epoch: [0][20/38]\tTime 0.313 (0.322)\tLoss 3.0948 (3.8570)\tPrec@1 0.000 (49.702)\n",
      "Epoch: [0][30/38]\tTime 0.324 (0.322)\tLoss 3.0635 (3.5992)\tPrec@1 0.000 (33.669)\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "Epoch: [1][0/38]\tTime 0.322 (0.322)\tLoss 3.0262 (3.0262)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][10/38]\tTime 0.337 (0.324)\tLoss 2.9307 (2.9701)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][20/38]\tTime 0.319 (0.322)\tLoss 2.9322 (2.9621)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][30/38]\tTime 0.332 (0.323)\tLoss 3.0007 (2.9590)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][0/38]\tTime 0.317 (0.317)\tLoss 2.9834 (2.9834)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][10/38]\tTime 0.322 (0.325)\tLoss 2.9821 (2.9603)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][20/38]\tTime 0.318 (0.323)\tLoss 2.8567 (2.9484)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][30/38]\tTime 0.321 (0.323)\tLoss 2.9712 (2.9497)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][0/38]\tTime 0.326 (0.326)\tLoss 2.9419 (2.9419)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][10/38]\tTime 0.322 (0.328)\tLoss 2.9416 (2.9420)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][20/38]\tTime 0.318 (0.324)\tLoss 2.9416 (2.9538)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][30/38]\tTime 0.319 (0.323)\tLoss 2.9916 (2.9519)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][0/38]\tTime 0.320 (0.320)\tLoss 2.9610 (2.9610)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][10/38]\tTime 0.318 (0.322)\tLoss 2.9812 (2.9558)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][20/38]\tTime 0.331 (0.324)\tLoss 2.9827 (2.9457)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][30/38]\tTime 0.335 (0.324)\tLoss 2.9933 (2.9496)\tPrec@1 0.000 (0.000)\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "\n",
      " \n",
      " Round 3 \n",
      " \n",
      "\n",
      "Number of model parameters: 84860202\n",
      "Epoch: [0][0/6]\tTime 0.334 (0.334)\tLoss 0.1145 (0.1145)\tPrec@1 100.000 (100.000)\n",
      "Accuracy of the network on the 96 test images: 96.875 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 93]]\n",
      "Epoch: [1][0/6]\tTime 0.331 (0.331)\tLoss 0.4844 (0.4844)\tPrec@1 93.750 (93.750)\n",
      "Epoch: [2][0/6]\tTime 0.326 (0.326)\tLoss 0.1458 (0.1458)\tPrec@1 100.000 (100.000)\n",
      "Epoch: [3][0/6]\tTime 0.337 (0.337)\tLoss 0.1104 (0.1104)\tPrec@1 100.000 (100.000)\n",
      "Epoch: [4][0/6]\tTime 0.336 (0.336)\tLoss 0.0927 (0.0927)\tPrec@1 100.000 (100.000)\n",
      "Accuracy of the network on the 96 test images: 96.875 %\n",
      "Confusion Matrix:\n",
      "[[ 0  3]\n",
      " [ 0 93]]\n",
      "alpha 1\n",
      "Number of model parameters: 21220877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1968130/3908595283.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  m = torch.tensor(m)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/38]\tTime 0.327 (0.327)\tLoss 15.2134 (15.2134)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [0][10/38]\tTime 0.323 (0.321)\tLoss 3.1475 (4.6276)\tPrec@1 50.000 (63.636)\n",
      "Epoch: [0][20/38]\tTime 0.329 (0.322)\tLoss 3.1009 (3.9069)\tPrec@1 0.000 (54.464)\n",
      "Epoch: [0][30/38]\tTime 0.328 (0.323)\tLoss 3.0447 (3.6331)\tPrec@1 0.000 (36.895)\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "Epoch: [1][0/38]\tTime 0.316 (0.316)\tLoss 3.0004 (3.0004)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][10/38]\tTime 0.329 (0.321)\tLoss 3.0052 (2.9668)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][20/38]\tTime 0.310 (0.321)\tLoss 3.0745 (2.9578)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [1][30/38]\tTime 0.326 (0.323)\tLoss 2.9886 (2.9654)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][0/38]\tTime 0.332 (0.332)\tLoss 2.9513 (2.9513)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][10/38]\tTime 0.326 (0.324)\tLoss 2.9486 (2.9532)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][20/38]\tTime 0.327 (0.323)\tLoss 2.8965 (2.9596)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [2][30/38]\tTime 0.322 (0.323)\tLoss 2.9670 (2.9565)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][0/38]\tTime 0.326 (0.326)\tLoss 2.9624 (2.9624)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][10/38]\tTime 0.318 (0.324)\tLoss 2.9887 (2.9473)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][20/38]\tTime 0.325 (0.323)\tLoss 2.9211 (2.9445)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [3][30/38]\tTime 0.321 (0.323)\tLoss 3.0122 (2.9534)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][0/38]\tTime 0.321 (0.321)\tLoss 2.9241 (2.9241)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][10/38]\tTime 0.315 (0.321)\tLoss 2.9682 (2.9683)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][20/38]\tTime 0.322 (0.322)\tLoss 2.9531 (2.9552)\tPrec@1 0.000 (0.000)\n",
      "Epoch: [4][30/38]\tTime 0.317 (0.323)\tLoss 2.9505 (2.9556)\tPrec@1 0.000 (0.000)\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n",
      "{'coverage': '0 out of80', 'system accuracy': 91.25, 'expert accuracy': 91.2497718755703, 'classifier accuracy': 0.0, 'alone classifier': 53.75}\n",
      "Accuracy for class GT 0  is: 94.595 %\n",
      "Accuracy for class GT 1  is: 88.372 %\n"
     ]
    }
   ],
   "source": [
    "nih_expert.setModel(getExpertModel(expert_train_dataset, expert_val_dataset, expert_test_dataset, nih_expert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32505732-0b05-4924-90be-759f4da4f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_expert.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "377705ad-823e-46f5-8da8-7e144d618816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img, target, filename = next(iter(expert_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe0ede79-8a6a-47b7-bbae-0b350b6f2da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(img.shape) == 3:\n",
    "    img = img.unsqueeze(0) \n",
    "outputs = nih_expert.model(img)\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e35fdadd-2400-4b3b-a5e3-3e6b69aac7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, target, filename = next(iter(expert_train_dataset))\n",
    "img = img.to(device).unsqueeze(0) \n",
    "outputs = nih_expert.model(img)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "predicted\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50b3a0c3-8b9f-40f1-83eb-5511c789768b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_train_dataset.targets\n",
    "expert_train_dataset.__getitem__(21)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17df98ef-6b3c-4700-8504-87003a813366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00002763_027.png'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, target, filename = next(iter(expert_train_dataset))\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6296eba1-24c7-45ea-8ac7-0c7b09e8bf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = NIHExpertDatasetMemory(None, np.array(expert_train_dataset.getAllFilenames()), np.array(expert_train_dataset.getAllTargets()), nih_expert.predict , [1]*len(expert_train_dataset.getAllIndices()), param=param)\n",
    "    \n",
    "dataLoaderTrain = DataLoader(dataset=dataset_train, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b838ead-fb57-4c47-9572-70f0822cc8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 244, 244])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9633277-0ade-4d3a-a7c1-502efe47187a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, (img, target, _, _, _, filename) = next(enumerate(dataLoaderTrain))\n",
    "target\n",
    "#img = img.to(device)\n",
    "#nih_expert.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "610592b0-8b69-4b7b-8cad-b5b3b59eb386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df906d-7d10-40f7-926c-642deac385f0",
   "metadata": {},
   "source": [
    "# L2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fbe7d-1b2d-4712-9a00-439812eb0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import expert as ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a913a9-2de8-4584-a8b5-9deaed9348e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_experts(param):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    experiment_experts = [7, 9, 10]\n",
    "    # for seed in ['', 948,  625,  436,  791]:\n",
    "    for seed in [948, 625, 436]:\n",
    "        print(\"run for seed {}\".format(seed))\n",
    "        if seed != \"\":\n",
    "            set_seed(seed)\n",
    "        log = {\"selected_experts\": [], \"selected_expert_fns\": []}\n",
    "        \n",
    "        #Use new Dataset\n",
    "        nih_dataloader = ds.NIH_K_Fold_Dataloader(\n",
    "            dataset = basic_Dataset,\n",
    "            k = params[\"K\"],\n",
    "            labelerIds = params[\"LABELER_IDS\"],\n",
    "            train_batch_size = params[\"TRAIN_BATCH_SIZE\"],\n",
    "            test_batch_size = params[\"TEST_BATCH_SIZE\"],\n",
    "            seed = seed,\n",
    "            maxLabels = maxL,\n",
    "            preload = True,\n",
    "            prebuild = True,\n",
    "            param = params\n",
    "        )\n",
    "            \n",
    "        for fold_idx in range(param[\"K\"]):\n",
    "            print(f'Running fold {fold_idx+1} out of {param[\"K\"]}')\n",
    "        #for i, n in enumerate(experiment_experts):\n",
    "            #print(\"n is {}\".format(n))\n",
    "            #num_experts = n\n",
    "\n",
    "            expert_fns = []\n",
    "            for labelerId in list(params[\"LABELER_IDS\"]):\n",
    "                #nih_expert = ex.Expert(dataset = basic_Dataset, labeler_id=labelerId)\n",
    "                nih_expert = ex.Expert(dataset = basic_Dataset, labeler_id=labelerId)\n",
    "                expert_fns.append(nih_expert.predict)\n",
    "            \n",
    "            num_experts = len(expert_fns)\n",
    "\n",
    "            #Use new Expert\n",
    "            #expert_fns = [experts[j] for j in range(n)]\n",
    "            \n",
    "            model = model = vres.ResNet50_defer(int(param[\"n_classes\"]) + num_experts)\n",
    "            # print(model)\n",
    "            #trainD = GalaxyZooDataset()\n",
    "            #valD = GalaxyZooDataset(split=\"val\")\n",
    "            \n",
    "            train_loader, val_loader, test_loader = nih_dataloader.get_data_loader_for_fold(fold_idx)\n",
    "            \n",
    "            train(model, train_loader, val_loader, test_loader, expert_fns, param, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66c22a-68e6-4b91-b997-d63f0932e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, test_loader, expert_fns, config, seed=\"\"):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    n_classes = config[\"n_classes\"] + len(expert_fns)\n",
    "    kwargs = {\"num_workers\": 0, \"pin_memory\": True}\n",
    "\n",
    "    model = model.to(device)\n",
    "    cudnn.benchmark = True\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), config[\"lr\"], weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    criterion = vlos.Criterion()\n",
    "    loss_fn = getattr(criterion, config[\"loss_type\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, len(train_loader) * config[\"epochs\"]\n",
    "    )\n",
    "    best_validation_loss = np.inf\n",
    "    patience = 0\n",
    "    iters = 0\n",
    "    warmup_iters = config[\"warmup_epochs\"] * len(train_loader)\n",
    "    lrate = config[\"lr\"]\n",
    "\n",
    "    for epoch in range(0, config[\"epochs\"]):\n",
    "        iters, train_loss = train_epoch(\n",
    "            iters,\n",
    "            warmup_iters,\n",
    "            lrate,\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            expert_fns,\n",
    "            loss_fn,\n",
    "            n_classes,\n",
    "            config[\"alpha\"],\n",
    "            config,\n",
    "        )\n",
    "        metrics = evaluate(model, expert_fns, loss_fn, n_classes, valid_loader, config)\n",
    "\n",
    "        validation_loss = metrics[\"validation_loss\"]\n",
    "\n",
    "        if validation_loss < best_validation_loss:\n",
    "            \"\"\"best_validation_loss = validation_loss\n",
    "            print(\n",
    "                \"Saving the model with classifier accuracy {}\".format(\n",
    "                    metrics[\"classifier_accuracy\"]\n",
    "                ),\n",
    "                flush=True,\n",
    "            )\n",
    "            save_path = os.path.join(\n",
    "                config[\"ckp_dir\"],\n",
    "                config[\"experiment_name\"]\n",
    "                + \"_\"\n",
    "                + str(len(expert_fns))\n",
    "                + \"_experts\"\n",
    "                + \"_seed_\"\n",
    "                + str(seed),\n",
    "            )\"\"\"\n",
    "            #torch.save(model.state_dict(), save_path + \".pt\")\n",
    "            # Additionally save the whole config dict\n",
    "            #with open(save_path + \".json\", \"w\") as f:\n",
    "            #    json.dump(config, f)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if patience >= config[\"patience\"]:\n",
    "            print(\"Early Exiting Training.\", flush=True)\n",
    "            break\n",
    "            \n",
    "    print(\"Evaluate on Test Data\")\n",
    "    metrics = evaluate(model, expert_fns, loss_fn, n_classes, test_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241fb99-074b-45e8-a841-704b812d2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    iters,\n",
    "    warmup_iters,\n",
    "    lrate,\n",
    "    train_loader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epoch,\n",
    "    expert_fns,\n",
    "    loss_fn,\n",
    "    n_classes,\n",
    "    alpha,\n",
    "    config,\n",
    "):\n",
    "    \"\"\" Train for one epoch \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    epoch_train_loss = []\n",
    "\n",
    "    for i, (input, target, hpred) in enumerate(train_loader):\n",
    "        if iters < warmup_iters:\n",
    "            lr = lrate * float(iters) / warmup_iters\n",
    "            print(iters, lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "        hpred = hpred\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        if config[\"loss_type\"] == \"softmax\":\n",
    "            output = F.softmax(output, dim=1)\n",
    "\n",
    "        # get expert  predictions and costs\n",
    "        batch_size = output.size()[0]  # batch_size\n",
    "        collection_Ms = []\n",
    "        # We only support \\alpha=1\n",
    "        for _, fn in enumerate(expert_fns):\n",
    "            # We assume each expert function has access to the extra metadata, even if they don't use it.\n",
    "            m = fn(input, target, hpred)\n",
    "            #m = fn(hpred)\n",
    "            m2 = [0] * batch_size\n",
    "            for j in range(0, batch_size):\n",
    "                if m[j] == target[j].item():\n",
    "                    m[j] = 1\n",
    "                    m2[j] = alpha\n",
    "                else:\n",
    "                    m[j] = 0\n",
    "                    m2[j] = 1\n",
    "            m = torch.tensor(m)\n",
    "            m2 = torch.tensor(m2)\n",
    "            m = m.to(device)\n",
    "            m2 = m2.to(device)\n",
    "            collection_Ms.append((m, m2))\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(output, target, collection_Ms, n_classes)\n",
    "        epoch_train_loss.append(loss.item())\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not iters < warmup_iters:\n",
    "            scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        iters += 1\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
    "                \"Prec@1 {top1.val:.3f} ({top1.avg:.3f})\".format(\n",
    "                    epoch,\n",
    "                    i,\n",
    "                    len(train_loader),\n",
    "                    batch_time=batch_time,\n",
    "                    loss=losses,\n",
    "                    top1=top1,\n",
    "                ),\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "    return iters, np.average(epoch_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714149aa-58eb-458d-bfad-f7ba0f24b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, expert_fns, loss_fn, n_classes, data_loader, config):\n",
    "    \"\"\"\n",
    "    Computes metrics for deferal\n",
    "    -----\n",
    "    Arguments:\n",
    "    net: model\n",
    "    expert_fn: expert model\n",
    "    n_classes: number of classes\n",
    "    loader: data loader\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    alone_correct = 0\n",
    "    #  === Individual Expert Accuracies === #\n",
    "    expert_correct_dic = {k: 0 for k in range(len(expert_fns))}\n",
    "    expert_total_dic = {k: 0 for k in range(len(expert_fns))}\n",
    "    #  === Individual  Expert Accuracies === #\n",
    "    alpha = config[\"alpha\"]\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels, hpred = data\n",
    "            images, labels, hpred = images.to(device), labels.to(device), hpred\n",
    "            outputs = model(images)\n",
    "            if config[\"loss_type\"] == \"softmax\":\n",
    "                outputs = F.softmax(outputs, dim=1)\n",
    "            if config[\"loss_type\"] == \"ova\":\n",
    "                ouputs = F.sigmoid(outputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            batch_size = outputs.size()[0]  # batch_size\n",
    "\n",
    "            expert_predictions = []\n",
    "            collection_Ms = []  # a collection of 3-tuple\n",
    "            for i, fn in enumerate(expert_fns, 0):\n",
    "                exp_prediction1 = fn(images, labels, hpred)\n",
    "                #exp_prediction1 = fn(hpred)\n",
    "                m = [0] * batch_size\n",
    "                m2 = [0] * batch_size\n",
    "                for j in range(0, batch_size):\n",
    "                    if exp_prediction1[j] == labels[j].item():\n",
    "                        m[j] = 1\n",
    "                        m2[j] = alpha\n",
    "                    else:\n",
    "                        m[j] = 0\n",
    "                        m2[j] = 1\n",
    "\n",
    "                m = torch.tensor(m)\n",
    "                m2 = torch.tensor(m2)\n",
    "                m = m.to(device)\n",
    "                m2 = m2.to(device)\n",
    "                collection_Ms.append((m, m2))\n",
    "                expert_predictions.append(exp_prediction1)\n",
    "\n",
    "            loss = loss_fn(outputs, labels, collection_Ms, n_classes)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            for i in range(0, batch_size):\n",
    "                r = predicted[i].item() >= n_classes - len(expert_fns)\n",
    "                prediction = predicted[i]\n",
    "                if predicted[i] >= n_classes - len(expert_fns):\n",
    "                    max_idx = 0\n",
    "                    # get second max\n",
    "                    for j in range(0, n_classes - len(expert_fns)):\n",
    "                        if outputs.data[i][j] >= outputs.data[i][max_idx]:\n",
    "                            max_idx = j\n",
    "                    prediction = max_idx\n",
    "                else:\n",
    "                    prediction = predicted[i]\n",
    "                alone_correct += (prediction == labels[i]).item()\n",
    "                if r == 0:\n",
    "                    total += 1\n",
    "                    correct += (predicted[i] == labels[i]).item()\n",
    "                    correct_sys += (predicted[i] == labels[i]).item()\n",
    "                if r == 1:\n",
    "                    deferred_exp = (predicted[i] - (n_classes - len(expert_fns))).item()\n",
    "                    # cdeferred_exp = ((n_classes - 1) - predicted[i]).item()  # reverse order, as in loss function\n",
    "                    exp_prediction = expert_predictions[deferred_exp][i]\n",
    "                    #\n",
    "                    # Deferral accuracy: No matter expert ===\n",
    "                    exp += exp_prediction == labels[i].item()\n",
    "                    exp_total += 1\n",
    "                    # Individual Expert Accuracy ===\n",
    "                    expert_correct_dic[deferred_exp] += (\n",
    "                        exp_prediction == labels[i].item()\n",
    "                    )\n",
    "                    expert_total_dic[deferred_exp] += 1\n",
    "                    #\n",
    "                    correct_sys += exp_prediction == labels[i].item()\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "\n",
    "    #  === Individual Expert Accuracies === #\n",
    "    expert_accuracies = {\n",
    "        \"expert_{}\".format(str(k)): 100\n",
    "        * expert_correct_dic[k]\n",
    "        / (expert_total_dic[k] + 0.0002)\n",
    "        for k in range(len(expert_fns))\n",
    "    }\n",
    "    # Add expert accuracies dict\n",
    "    to_print = {\n",
    "        \"coverage\": cov,\n",
    "        \"system_accuracy\": 100 * correct_sys / real_total,\n",
    "        \"expert_accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "        \"classifier_accuracy\": 100 * correct / (total + 0.0001),\n",
    "        \"alone_classifier\": 100 * alone_correct / real_total,\n",
    "        \"validation_loss\": np.average(losses),\n",
    "        \"n_experts\": len(expert_fns),\n",
    "        **expert_accuracies,\n",
    "    }\n",
    "    print(to_print, flush=True)\n",
    "    return to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d15a8e-9340-4300-b898-4f60b717b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "increase_experts(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joli-env_kernel",
   "language": "python",
   "name": "joli-env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
