{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c235e94e-c93e-4356-adc5-298e50ea48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/joli/joli-env/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import chain\n",
    "from typing import Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb1264f-43d4-4479-be40-a32843d7a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../Datasets/NIH/\"\n",
    "PATH_Labels = PATH\n",
    "PATH_Images = PATH\n",
    "\n",
    "maxLabels = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623f8d2b-4e72-4525-8625-044958cdd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import shutil\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def downloadData(PATH):\n",
    "    links = [\n",
    "        'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
    "        'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
    "        'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
    "        'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
    "        'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
    "        'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
    "        'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
    "        'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
    "        'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
    "        'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
    "        'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
    "        'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
    "    ]\n",
    "    files = os.listdir(PATH + \"images/\")\n",
    "    for idx, link in enumerate(links):\n",
    "        fn = PATH + \"images/\" + 'images_%02d.tar.gz' % (idx+1)\n",
    "        if ('images_%02d.tar.gz' % (idx+1)) in files:\n",
    "            continue\n",
    "        print('downloading'+fn+'...')\n",
    "        urllib.request.urlretrieve(link, fn)  # download the zip file\n",
    "\n",
    "    print(\"Download complete. Please check the checksums\")\n",
    "    \n",
    "def unpackData(PATH):\n",
    "    Path = PATH + \"images/\"\n",
    "    files = os.listdir(Path)\n",
    "    for filename in files:\n",
    "        if \".tar.gz\" not in filename:\n",
    "            continue\n",
    "        # open file\n",
    "        file = tarfile.open(Path + filename)\n",
    "        \n",
    "        # extracting file\n",
    "        file.extractall(Path + filename[:-7])\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "def moveData(PATH):\n",
    "    Path = PATH + \"images/\"\n",
    "    directories = os.listdir(Path)\n",
    "    for direc in directories:\n",
    "        if \"png\" in direc:\n",
    "            continue\n",
    "        if \"tar.gz\" in direc:\n",
    "            continue\n",
    "        if \"checkpoint\" in direc:\n",
    "            continue\n",
    "        if \"images\" not in direc:\n",
    "            continue\n",
    "        filenames = os.listdir(Path + direc + \"/images/\")\n",
    "        for filename in filenames:\n",
    "            shutil.move(Path + direc + \"/images/\" + filename, Path + filename)\n",
    "        shutil.rmtree(Path + direc)\n",
    "        \n",
    "def downloadLabels(Path):\n",
    "    links = [\n",
    "        \"https://storage.googleapis.com/gcs-public-data--healthcare-nih-chest-xray-labels/four_findings_expert_labels/individual_readers.csv\",\n",
    "        \"https://storage.googleapis.com/gcs-public-data--healthcare-nih-chest-xray-labels/four_findings_expert_labels/test_labels.csv\",\n",
    "        \"https://storage.googleapis.com/gcs-public-data--healthcare-nih-chest-xray-labels/four_findings_expert_labels/validation_labels.csv\"\n",
    "    ]\n",
    "    urllib.request.urlretrieve(links[0], Path + \"individual_readers.csv\")\n",
    "    urllib.request.urlretrieve(links[1], Path + \"test_labels.csv\")\n",
    "    urllib.request.urlretrieve(links[2], Path + \"validation_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f4eac-8b6d-4e6c-ba8f-0c7fd96c836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupLabels(PATH_Labels):\n",
    "    path_to_test_Labels = PATH_Labels + \"test_labels.csv\"\n",
    "    path_to_val_Labels = PATH_Labels + \"validation_labels.csv\"\n",
    "    test_labels = pd.read_csv(path_to_test_Labels)\n",
    "    val_labels = pd.read_csv(path_to_val_Labels)\n",
    "\n",
    "    ground_truth_labels = pd.concat([test_labels,val_labels])\n",
    "    ground_truth_labels[\"Fracture_Label\"] = ground_truth_labels[\"Fracture\"].map(dict(YES=1, NO=0))\n",
    "    ground_truth_labels[\"Pneumothorax_Label\"] = ground_truth_labels[\"Pneumothorax\"].map(dict(YES=1, NO=0))\n",
    "    ground_truth_labels[\"Airspace_Opacity_Label\"] = ground_truth_labels[\"Airspace opacity\"].map(dict(YES=1, NO=0))\n",
    "    ground_truth_labels[\"Nodule_Or_Mass_Label\"] = ground_truth_labels[\"Nodule or mass\"].map(dict(YES=1, NO=0))\n",
    "\n",
    "    path_to_individual_reader = PATH_Labels + \"individual_readers.csv\"\n",
    "    individual_readers = pd.read_csv(path_to_individual_reader)\n",
    "\n",
    "    individual_readers[\"Fracture_Expert_Label\"] = individual_readers[\"Fracture\"].map(dict(YES=1, NO=0))\n",
    "    individual_readers[\"Pneumothorax_Expert_Label\"] = individual_readers[\"Pneumothorax\"].map(dict(YES=1, NO=0))\n",
    "    individual_readers[\"Airspace_Opacity_Expert_Label\"] = individual_readers[\"Airspace opacity\"].map(dict(YES=1, NO=0))\n",
    "    individual_readers[\"Nodule_Or_Mass_Expert_Label\"] = individual_readers[\"Nodule/mass\"].map(dict(YES=1, NO=0))\n",
    "\n",
    "    individual_readers[\"Fracture_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Fracture_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "    individual_readers[\"Pneumothorax_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Pneumothorax_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "    individual_readers[\"Airspace_Opacity_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Airspace_Opacity_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "    individual_readers[\"Nodule_Or_Mass_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Nodule_Or_Mass_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "\n",
    "    individual_readers[\"Fracture_Correct\"] = (individual_readers['Fracture_Expert_Label']==individual_readers['Fracture_GT_Label']).astype(int)\n",
    "    individual_readers[\"Pneumothorax_Correct\"] = (individual_readers['Pneumothorax_Expert_Label']==individual_readers['Pneumothorax_GT_Label']).astype(int)\n",
    "    individual_readers[\"Airspace_Opacity_Correct\"] = (individual_readers['Airspace_Opacity_Expert_Label']==individual_readers['Airspace_Opacity_GT_Label']).astype(int)\n",
    "    individual_readers[\"Nodule_Or_Mass_Correct\"] = (individual_readers['Nodule_Or_Mass_Expert_Label']==individual_readers['Nodule_Or_Mass_GT_Label']).astype(int)\n",
    "\n",
    "    individual_readers.to_csv(PATH_Labels + \"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bada4086-9c84-487f-965b-59ce66b53c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33517f1b-9727-425d-b58e-feb2510efe06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965cd081-b8e2-46e4-92e6-11775744a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset:\n",
    "    \"\"\"\n",
    "    Contains the main Dataset with GT Label and Expert Label for every Image, sorted by file name\n",
    "    \"\"\"\n",
    "    def __init__(self, Path, target):\n",
    "        df = pd.read_csv(Path + \"labels.csv\")\n",
    "        ids = df[\"Reader ID\"].unique()\n",
    "        result = df[[\"Patient ID\", \"Image ID\", target + \"_GT_Label\"]].drop_duplicates().rename(columns={target + \"_GT_Label\": 'GT'})\n",
    "        for reader_id in ids:\n",
    "            temp = df[df[\"Reader ID\"] == reader_id][[\"Image ID\", target + \"_Expert_Label\"]].rename(columns={target + \"_Expert_Label\": str(reader_id)})\n",
    "            result = result.join(temp.set_index('Image ID'), on='Image ID')\n",
    "        self.data = result.fillna(-1).reset_index(drop=True)\n",
    "\n",
    "    def getExpert(self, id):\n",
    "        \"\"\"\n",
    "        Returns the data for the given expert\n",
    "        \"\"\"\n",
    "        return result[\"Image ID\", \"GT\", str(id)]\n",
    "\n",
    "    def getData(self):\n",
    "        return self.data\n",
    "\n",
    "class NIHDataset:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, transformation=None, preload=False, preprocess=False, param=None, image_container=None, size=(128, 128)):\n",
    "        self.data = data\n",
    "        self.image_ids = data[\"Image ID\"].values\n",
    "        self.targets = data[\"GT\"].values\n",
    "\n",
    "        if transformation == None:\n",
    "            self.tfms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(128),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.tfms = transformation\n",
    "            \n",
    "        self.param = param\n",
    "        self.PATH = param[\"PATH\"]\n",
    "\n",
    "        self.images = []\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.preload = preload\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        self.image_list = np.empty(0)\n",
    "        \n",
    "        self.current = 0\n",
    "        self.high = len(self.image_ids)\n",
    "\n",
    "        self.image_container = image_container\n",
    "        self.size = size\n",
    "        \n",
    "        if self.preload:\n",
    "            self.loadImages()\n",
    "            \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.current += 1\n",
    "        if self.current < self.high:\n",
    "            return self.__getitem__(self.current)\n",
    "        raise StopIteration\n",
    "            \n",
    "    def loadImage(self, idx):\n",
    "        \"\"\"\n",
    "        Load one single image\n",
    "        \"\"\"\n",
    "        if self.image_container is not None:\n",
    "            return self.image_container.get_image_from_name(self.image_ids[idx])\n",
    "        else:\n",
    "            return Image.open(self.PATH + \"images/\" + self.image_ids[idx]).convert(\"RGB\").resize(self.size)\n",
    "            \n",
    "    def getImage(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the image from index idx\n",
    "        \"\"\"\n",
    "        if self.preload:\n",
    "            return self.images[idx]\n",
    "        else:\n",
    "            return self.loadImage(idx)\n",
    "\n",
    "    def loadImages(self):\n",
    "        \"\"\"\n",
    "        Load all images\n",
    "        \"\"\"\n",
    "        if self.image_container is not None:\n",
    "            self.images = self.image_container.get_images_from_name(self.image_ids)\n",
    "        else:\n",
    "            for idx in range(len(self.image_ids)):\n",
    "                if self.preprocess:\n",
    "                    self.images.append(self.transformImage(self.loadImage(idx)))\n",
    "                else:\n",
    "                    self.images.append(self.loadImage(idx))\n",
    "        #print(\"Loading complete\")\n",
    "        \n",
    "    def transformImage(self, image):\n",
    "        \"\"\"\n",
    "        Transforms the image\n",
    "        \"\"\"\n",
    "        return self.tfms(image).to(self.device)\n",
    "        \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        filename, target = self.image_ids[index], self.targets[index]\n",
    "        img = self.getImage(index)\n",
    "        if not self.preprocess:\n",
    "            img = self.transformImage(img)\n",
    "        return img, target, filename\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        #return len(self.images)\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    \"\"\"\n",
    "    Functions for Verma active learning\n",
    "    \"\"\"\n",
    "    def getAllImagesNP(self):\n",
    "        \"\"\"\n",
    "        Returns all images from the Dataset\n",
    "        \"\"\"\n",
    "        if not self.preload:\n",
    "            self.preload = True\n",
    "            self.loadImages()\n",
    "        if self.image_list.size == 0:\n",
    "            image_liste = []\n",
    "            for img in self.images:\n",
    "                #np_img = np.moveaxis(np.array(img), -1, 0)\n",
    "                np_img = np.array(img)\n",
    "                image_liste.append(np_img)\n",
    "            self.image_list = np.array(image_liste)\n",
    "        return self.image_list\n",
    "\n",
    "    def getAllImages(self):\n",
    "        \"\"\"\n",
    "        Returns all images from the Dataset\n",
    "        \"\"\"\n",
    "        if not self.preload:\n",
    "            self.preload = True\n",
    "            self.loadImages()\n",
    "        return self.images\n",
    "\n",
    "    def getAllTargets(self):\n",
    "        \"\"\"\n",
    "        Returns all targets\n",
    "        \"\"\"\n",
    "        return self.targets\n",
    "\n",
    "    def getAllFilenames(self):\n",
    "        \"\"\"\n",
    "        Returns all filenames\n",
    "        \"\"\"\n",
    "        return self.image_ids\n",
    "\n",
    "    def getAllIndices(self):\n",
    "        return self.data.index\n",
    "\n",
    "Datas = BasicDataset(PATH, \"Airspace_Opacity\")\n",
    "result = Datas.getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0290294-e983-4975-afff-3e82bbb1d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIH_K_Fold_Dataloader:\n",
    "    def __init__(self, dataset, k=10, labelerIds=[4323195249, 4295194124], train_batch_size=8, test_batch_size=8,\n",
    "                 seed=42, fraction=1.0, maxLabels=800, preload=False, preprocess=False, prebuild=False, param=None):\n",
    "        self.dataset = dataset.getData()\n",
    "        self.k = k\n",
    "        self.labelerIds = labelerIds\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.seed = seed\n",
    "        self.k_fold_datasets = []\n",
    "        self.k_fold_patient_ids = []\n",
    "        self.preload = preload\n",
    "        self.preprocess = preprocess\n",
    "        self.param = param\n",
    "        self.prebuild = prebuild\n",
    "\n",
    "        #TODO: Implement support for non overlapping Labels\n",
    "\n",
    "        ##\n",
    "        self.common = True\n",
    "        if self.common:\n",
    "            self.common_image_ids = self.dataset[\"Image ID\"].values.tolist()\n",
    "            names = [\"Patient ID\", \"Image ID\", \"GT\"]\n",
    "            for labelerId in self.labelerIds:\n",
    "                temp = self.dataset[self.dataset[str(labelerId)] != -1][\"Image ID\"].values.tolist()\n",
    "                self.common_image_ids = np.intersect1d(self.common_image_ids, temp)\n",
    "                names.append(str(labelerId))\n",
    "            self.data = self.dataset[self.dataset[\"Image ID\"].isin(self.common_image_ids)][names]\n",
    "\n",
    "        #Performance\n",
    "        patient_ids = self.data[\"Patient ID\"].unique()\n",
    "        num_patient_images = self.data.drop_duplicates(subset=[\"Image ID\"]).groupby(by=\"Patient ID\", as_index=False).count()[\"Image ID\"]\n",
    "        self.patient_performance = pd.DataFrame({\"Patient ID\": patient_ids, \"Num Patient Images\": num_patient_images})\n",
    "                     \n",
    "        for labeler_id in self.labelerIds:\n",
    "            temp = self.data[[\"Patient ID\", \"Image ID\", \"GT\", str(labeler_id)]]\n",
    "            temp[\"Expert_Correct\"] = self.data[\"GT\"] == self.data[str(labeler_id)]\n",
    "            sum = temp[[\"Patient ID\", \"Expert_Correct\"]].groupby(by=\"Patient ID\", as_index=False).sum()\n",
    "            sum.columns = [\"Patient ID\", f'{labeler_id}_num_correct']\n",
    "            self.patient_performance = pd.merge(self.patient_performance, sum, left_on=\"Patient ID\", right_on=\"Patient ID\")\n",
    "            self.patient_performance[f'{labeler_id}_perf'] = self.patient_performance[f'{labeler_id}_num_correct'] / self.patient_performance['Num Patient Images']\n",
    "\n",
    "        target_temp = self.patient_performance[f'{labelerIds[0]}_perf'].astype(str)\n",
    "        for labeler_id in labelerIds[1:]:\n",
    "            target_temp = target_temp + \"_\" + self.patient_performance[f'{labeler_id}_perf'].astype(str)\n",
    "        self.patient_performance[\"target\"] = target_temp \n",
    "\n",
    "        self.expert_labels = self.data\n",
    "        self._init_k_folds(maxLabels=maxLabels)\n",
    "\n",
    "        if self.prebuild:\n",
    "            self.buildDataloaders()\n",
    "\n",
    "    def _init_k_folds(self, fraction=1.0, maxLabels=800):\n",
    "        self.labels = self.expert_labels.drop_duplicates(subset=[\"Image ID\"])\n",
    "        self.labels = self.labels.fillna(0)\n",
    "        self.labels = self.labels[[\"Patient ID\", \"Image ID\", \"GT\"]]\n",
    "\n",
    "        self.image_container = ImageContainer(path=self.param[\"PATH\"], img_ids=self.labels[\"Image ID\"], preload=True, transform=None, preprocess=False, img_size=(128, 128))\n",
    "\n",
    "        kf_cv = StratifiedKFold(n_splits=self.k, shuffle=True, random_state=self.seed)\n",
    "\n",
    "        # _ sind train indizes, fold_test_idxs ist liste der test indizes\n",
    "        fold_data_idxs = [fold_test_idxs for (_, fold_test_idxs) in kf_cv.split(self.patient_performance[\"Patient ID\"].values, self.patient_performance[\"target\"].values)]\n",
    "\n",
    "       \n",
    "        for fold_idx in range(len(fold_data_idxs)):\n",
    "            \n",
    "            test = round(self.k*0.1)\n",
    "            val = round(self.k*0.2)\n",
    "            train = self.k - test - val\n",
    "            \n",
    "            test_folds_idxs = [(fold_idx + i) % self.k for i in range(test)]\n",
    "            test_fold_data_idxs = [fold_data_idxs[test_fold_idx] for test_fold_idx in test_folds_idxs]\n",
    "            test_fold_data_idxs = list(chain.from_iterable(test_fold_data_idxs))\n",
    "            \n",
    "            #test_fold_idx = fold_idx # Nummer des Folds\n",
    "            #test_fold_data_idxs = fold_data_idxs[test_fold_idx] # Array der Test Indizes\n",
    "\n",
    "            # use next 2 folds for validation set\n",
    "            val_folds_idxs = [(fold_idx + test + i) % self.k for i in range(val)]\n",
    "            val_fold_data_idxs = [fold_data_idxs[val_fold_idx] for val_fold_idx in val_folds_idxs]\n",
    "            val_fold_data_idxs = list(chain.from_iterable(val_fold_data_idxs))\n",
    "\n",
    "            # use next 7 folds for training set\n",
    "            train_folds_idxs = [(fold_idx + (test + val) + i) % self.k for i in range(train)]\n",
    "            #print(train_folds_idxs)\n",
    "            train_folds_data_idxs = [fold_data_idxs[train_fold_idx] for train_fold_idx in train_folds_idxs]\n",
    "            train_folds_data_idxs = list(chain.from_iterable(train_folds_data_idxs))\n",
    "\n",
    "            \n",
    "            train_patient_ids = self.patient_performance[\"Patient ID\"].iloc[train_folds_data_idxs]\n",
    "            #train_patient_ids = self.patient_performance[\"Patient ID\"].iloc[train_folds_data_idxs].sample(n=min(maxLabels,len(train_folds_data_idxs)))\n",
    "            val_patient_ids = self.patient_performance[\"Patient ID\"].iloc[val_fold_data_idxs]\n",
    "            test_patient_ids = self.patient_performance[\"Patient ID\"].iloc[test_fold_data_idxs]\n",
    "\n",
    "            #expert_train = self.labels[self.labels[\"Patient ID\"].isin(train_patient_ids)]\n",
    "            expert_train = self.labels[self.labels[\"Patient ID\"].isin(train_patient_ids)]\n",
    "            expert_train = self.labels[self.labels[\"Patient ID\"].isin(train_patient_ids)].sample(n=min(maxLabels,len(expert_train)))\n",
    "            expert_val = self.labels[self.labels[\"Patient ID\"].isin(val_patient_ids)]\n",
    "            expert_test = self.labels[self.labels[\"Patient ID\"].isin(test_patient_ids)]\n",
    "\n",
    "            # check that patients are not shared across training, validation and test split\n",
    "            overlap = expert_train[expert_train[\"Patient ID\"].isin(expert_val[\"Patient ID\"])]\n",
    "            assert len(overlap) == 0, \"Train and Val Patient Ids overlap\"\n",
    "\n",
    "            overlap = expert_train[expert_train[\"Patient ID\"].isin(expert_test[\"Patient ID\"])]\n",
    "            assert len(overlap) == 0, \"Train and Test Patient Ids overlap\"\n",
    "\n",
    "            overlap = expert_val[expert_val[\"Patient ID\"].isin(expert_test[\"Patient ID\"])]\n",
    "            assert len(overlap) == 0, \"Val and Test Patient Ids overlap\"\n",
    "\n",
    "            expert_train = expert_train[[\"Image ID\", \"GT\"]]\n",
    "            expert_val = expert_val[[\"Image ID\", \"GT\"]]\n",
    "            expert_test = expert_test[[\"Image ID\", \"GT\"]]\n",
    "\n",
    "            self.k_fold_datasets.append((expert_train, expert_val, expert_test))\n",
    "\n",
    "    def get_data_loader_for_fold(self, fold_idx):\n",
    "        if self.prebuild:\n",
    "            return self.loaders[fold_idx][0], self.loaders[fold_idx][1], self.loaders[fold_idx][2]\n",
    "        else:\n",
    "            return self.create_Dataloader_for_Fold(fold_idx)\n",
    "\n",
    "    def get_dataset_for_folder(self, fold_idx):\n",
    "        expert_train, expert_val, expert_test = self.k_fold_datasets[fold_idx]\n",
    "\n",
    "        return expert_train, expert_val, expert_test\n",
    "\n",
    "    def create_Dataloader_for_Fold(self, idx):\n",
    "        expert_train, expert_val, expert_test = self.k_fold_datasets[idx]\n",
    "\n",
    "        expert_train_dataset = NIHDataset(expert_train, preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "        expert_val_dataset = NIHDataset(expert_val, preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "        expert_test_dataset = NIHDataset(expert_test, preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=expert_train_dataset, batch_size=self.train_batch_size, num_workers=4, shuffle=True, drop_last=True, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=expert_val_dataset, batch_size=self.test_batch_size, num_workers=4, shuffle=True, drop_last=False, pin_memory=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=expert_test_dataset, batch_size=self.test_batch_size, num_workers=4, shuffle=True, drop_last=False, pin_memory=True)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def buildDataloaders(self):\n",
    "        self.loaders = []\n",
    "        for i in range(self.k):\n",
    "            train_loader, val_loader, test_loader = self.create_Dataloader_for_Fold(i)\n",
    "            loader_set = [train_loader, val_loader, test_loader]\n",
    "            self.loaders.append(loader_set)\n",
    "            print(\"Loaded set number \" + str(i))\n",
    "\n",
    "    def getFullDataloader(self):\n",
    "        full_dataset = NIHDataset(self.labels[[\"Image ID\", \"GT\"]], preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "        return torch.utils.data.DataLoader(dataset=full_dataset, batch_size=self.train_batch_size, num_workers=4, shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "    def get_ImageContainer(self):\n",
    "        return self.image_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4bb7d4-8d02-4d6e-a328-fbc05a5e3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageContainer:\n",
    "    def __init__(self, path, img_ids, preload=True, transform=None, preprocess=False, img_size=(128, 128)):\n",
    "        self.PATH = path\n",
    "        self.image_ids = img_ids\n",
    "        self.preload = preload\n",
    "        self.preprocess = preprocess\n",
    "        self.img_size = img_size\n",
    "        self.images = []\n",
    "\n",
    "        if self.preload:\n",
    "            self.loadImages()\n",
    "        \n",
    "\n",
    "    def loadImages(self):\n",
    "        for idx in range(len(self.image_ids)):\n",
    "            self.images.append(self.loadImage(idx))\n",
    "            \n",
    "            if self.preprocess:\n",
    "                self.images[idx] = self.transformImage(self.images[idx])\n",
    "\n",
    "    def loadImage(self, idx):\n",
    "        \"\"\"\n",
    "        Load one single image\n",
    "        \"\"\"\n",
    "        return Image.open(self.PATH + \"images/\" + self.image_ids[idx]).convert(\"RGB\").resize(self.img_size)\n",
    "            \n",
    "    def get_image_from_id(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the image from index idx\n",
    "        \"\"\"\n",
    "        if self.preload:\n",
    "            return self.images[idx]\n",
    "        else:\n",
    "            return self.loadImage(idx)\n",
    "\n",
    "    def get_image_from_name(self, fname):\n",
    "        if self.preload:\n",
    "            return self.images[self.image_ids.index(fname)]\n",
    "        else:\n",
    "            return self.get_image_from_id(self.image_ids.index(fname))\n",
    "\n",
    "    def get_images_from_name(self, fnames):\n",
    "        if self.preload:\n",
    "            return [self.images[self.image_ids.index(fname)] for fname in fnames]\n",
    "        else:\n",
    "            return [self.get_image_from_id(self.image_ids.index(fname)) for fname in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d496a-02c0-4cb0-b486-416ae48c9940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc4f53-d2c4-41e0-9b93-8b4e46175cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joli-env_kernel",
   "language": "python",
   "name": "joli-env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
