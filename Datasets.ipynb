{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c235e94e-c93e-4356-adc5-298e50ea48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import chain\n",
    "from typing import Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "from SSL.datasets import transform as T\n",
    "from SSL.datasets.randaugment import RandomAugment\n",
    "from SSL.datasets.sampler import RandomSampler, BatchSampler\n",
    "\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb1264f-43d4-4479-be40-a32843d7a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../Datasets/NIH/\"\n",
    "PATH_Labels = PATH\n",
    "PATH_Images = PATH\n",
    "\n",
    "#maxLabels = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623f8d2b-4e72-4525-8625-044958cdd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import shutil\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def downloadData(PATH):\n",
    "    links = [\n",
    "        'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
    "        'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
    "        'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
    "        'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
    "        'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
    "        'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
    "        'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
    "        'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
    "        'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
    "        'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
    "        'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
    "        'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
    "    ]\n",
    "    files = os.listdir(PATH + \"images/\")\n",
    "    for idx, link in enumerate(links):\n",
    "        fn = PATH + \"images/\" + 'images_%02d.tar.gz' % (idx+1)\n",
    "        if ('images_%02d.tar.gz' % (idx+1)) in files:\n",
    "            continue\n",
    "        print('downloading'+fn+'...')\n",
    "        urllib.request.urlretrieve(link, fn)  # download the zip file\n",
    "\n",
    "    print(\"Download complete. Please check the checksums\")\n",
    "    \n",
    "def unpackData(PATH):\n",
    "    Path = PATH + \"images/\"\n",
    "    files = os.listdir(Path)\n",
    "    for filename in files:\n",
    "        if \".tar.gz\" not in filename:\n",
    "            continue\n",
    "        # open file\n",
    "        file = tarfile.open(Path + filename)\n",
    "        \n",
    "        # extracting file\n",
    "        file.extractall(Path + filename[:-7])\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "def moveData(PATH):\n",
    "    Path = PATH + \"images/\"\n",
    "    directories = os.listdir(Path)\n",
    "    for direc in directories:\n",
    "        if \"png\" in direc:\n",
    "            continue\n",
    "        if \"tar.gz\" in direc:\n",
    "            continue\n",
    "        if \"checkpoint\" in direc:\n",
    "            continue\n",
    "        if \"images\" not in direc:\n",
    "            continue\n",
    "        filenames = os.listdir(Path + direc + \"/images/\")\n",
    "        for filename in filenames:\n",
    "            shutil.move(Path + direc + \"/images/\" + filename, Path + filename)\n",
    "        shutil.rmtree(Path + direc)\n",
    "        \n",
    "def downloadLabels(Path):\n",
    "    links = [\n",
    "        \"https://storage.googleapis.com/gcs-public-data--healthcare-nih-chest-xray-labels/four_findings_expert_labels/individual_readers.csv\",\n",
    "        \"https://storage.googleapis.com/gcs-public-data--healthcare-nih-chest-xray-labels/four_findings_expert_labels/test_labels.csv\",\n",
    "        \"https://storage.googleapis.com/gcs-public-data--healthcare-nih-chest-xray-labels/four_findings_expert_labels/validation_labels.csv\"\n",
    "    ]\n",
    "    urllib.request.urlretrieve(links[0], Path + \"individual_readers.csv\")\n",
    "    urllib.request.urlretrieve(links[1], Path + \"test_labels.csv\")\n",
    "    urllib.request.urlretrieve(links[2], Path + \"validation_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "454f4eac-8b6d-4e6c-ba8f-0c7fd96c836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupLabels(PATH_Labels):\n",
    "    path_to_test_Labels = PATH_Labels + \"test_labels.csv\"\n",
    "    path_to_val_Labels = PATH_Labels + \"validation_labels.csv\"\n",
    "    test_labels = pd.read_csv(path_to_test_Labels)\n",
    "    val_labels = pd.read_csv(path_to_val_Labels)\n",
    "\n",
    "    ground_truth_labels = pd.concat([test_labels,val_labels])\n",
    "    ground_truth_labels[\"Fracture_Label\"] = ground_truth_labels[\"Fracture\"].map(dict(YES=1, NO=0))\n",
    "    ground_truth_labels[\"Pneumothorax_Label\"] = ground_truth_labels[\"Pneumothorax\"].map(dict(YES=1, NO=0))\n",
    "    ground_truth_labels[\"Airspace_Opacity_Label\"] = ground_truth_labels[\"Airspace opacity\"].map(dict(YES=1, NO=0))\n",
    "    ground_truth_labels[\"Nodule_Or_Mass_Label\"] = ground_truth_labels[\"Nodule or mass\"].map(dict(YES=1, NO=0))\n",
    "\n",
    "    path_to_individual_reader = PATH_Labels + \"individual_readers.csv\"\n",
    "    individual_readers = pd.read_csv(path_to_individual_reader)\n",
    "\n",
    "    individual_readers[\"Fracture_Expert_Label\"] = individual_readers[\"Fracture\"].map(dict(YES=1, NO=0))\n",
    "    individual_readers[\"Pneumothorax_Expert_Label\"] = individual_readers[\"Pneumothorax\"].map(dict(YES=1, NO=0))\n",
    "    individual_readers[\"Airspace_Opacity_Expert_Label\"] = individual_readers[\"Airspace opacity\"].map(dict(YES=1, NO=0))\n",
    "    individual_readers[\"Nodule_Or_Mass_Expert_Label\"] = individual_readers[\"Nodule/mass\"].map(dict(YES=1, NO=0))\n",
    "\n",
    "    individual_readers[\"Fracture_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Fracture_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "    individual_readers[\"Pneumothorax_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Pneumothorax_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "    individual_readers[\"Airspace_Opacity_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Airspace_Opacity_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "    individual_readers[\"Nodule_Or_Mass_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Nodule_Or_Mass_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "\n",
    "    individual_readers[\"Fracture_Correct\"] = (individual_readers['Fracture_Expert_Label']==individual_readers['Fracture_GT_Label']).astype(int)\n",
    "    individual_readers[\"Pneumothorax_Correct\"] = (individual_readers['Pneumothorax_Expert_Label']==individual_readers['Pneumothorax_GT_Label']).astype(int)\n",
    "    individual_readers[\"Airspace_Opacity_Correct\"] = (individual_readers['Airspace_Opacity_Expert_Label']==individual_readers['Airspace_Opacity_GT_Label']).astype(int)\n",
    "    individual_readers[\"Nodule_Or_Mass_Correct\"] = (individual_readers['Nodule_Or_Mass_Expert_Label']==individual_readers['Nodule_Or_Mass_GT_Label']).astype(int)\n",
    "\n",
    "    individual_readers.to_csv(PATH_Labels + \"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada4086-9c84-487f-965b-59ce66b53c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33517f1b-9727-425d-b58e-feb2510efe06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965cd081-b8e2-46e4-92e6-11775744a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset:\n",
    "    \"\"\"\n",
    "    Contains the main Dataset with GT Label and Expert Label for every Image, sorted by file name\n",
    "    \"\"\"\n",
    "    def __init__(self, Path, target):\n",
    "        df = pd.read_csv(Path + \"labels.csv\")\n",
    "        ids = df[\"Reader ID\"].unique()\n",
    "        result = df[[\"Patient ID\", \"Image ID\", target + \"_GT_Label\"]].drop_duplicates().rename(columns={target + \"_GT_Label\": 'GT'})\n",
    "        for reader_id in ids:\n",
    "            temp = df[df[\"Reader ID\"] == reader_id][[\"Image ID\", target + \"_Expert_Label\"]].rename(columns={target + \"_Expert_Label\": str(reader_id)})\n",
    "            result = result.join(temp.set_index('Image ID'), on='Image ID')\n",
    "        self.data = result.fillna(-1).reset_index(drop=True)\n",
    "\n",
    "    def getExpert(self, id):\n",
    "        \"\"\"\n",
    "        Returns the data for the given expert\n",
    "        \"\"\"\n",
    "        return result[\"Image ID\", \"GT\", str(id)]\n",
    "\n",
    "    def getData(self):\n",
    "        \"\"\"\n",
    "        Returns all data\n",
    "        \"\"\"\n",
    "        return self.data\n",
    "    \n",
    "    def getDataForLabelers(self, labelerIds):\n",
    "        \"\"\"\n",
    "        Returns the data with [\"Patient ID\", \"Image ID\", \"GT\", [labelerIds]]\n",
    "        \"\"\"\n",
    "        temp = self.data[[\"Patient ID\", \"Image ID\", \"GT\"]].copy()\n",
    "        for labelerId in labelerIds:\n",
    "            temp[str(labelerId)] = self.data[str(labelerId)]\n",
    "        return temp\n",
    "\n",
    "class NIHDataset:\n",
    "    \"\"\"\n",
    "    Dataset to contain the NIHDataset and pass it to a dataloader\n",
    "    \n",
    "    data - Dataframe with Image ID and GT\n",
    "    transformation - the transformation for the images\n",
    "    preload - should all images be loaded at the beginning\n",
    "    preprocess - should all images be transformed at the beginning\n",
    "    param - parameter for experiments\n",
    "    image_container - to speed this class up, you can pass a ImageContainer object which contains all images loaded so this class doesn't read them from the disk\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, transformation=None, preload=False, preprocess=False, param=None, image_container=None, size=(128, 128)):\n",
    "        self.data = data\n",
    "        self.image_ids = data[\"Image ID\"].values\n",
    "        self.targets = data[\"GT\"].values\n",
    "\n",
    "        if transformation == None:\n",
    "            self.tfms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(128),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.tfms = transformation\n",
    "            \n",
    "        self.param = param\n",
    "        self.PATH = param[\"PATH\"]\n",
    "\n",
    "        self.images = []\n",
    "\n",
    "        #self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = None\n",
    "        \n",
    "        self.preload = preload\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        self.image_list = np.empty(0)\n",
    "        \n",
    "        self.current = 0\n",
    "        self.high = len(self.image_ids)\n",
    "\n",
    "        self.image_container = image_container\n",
    "        self.size = size\n",
    "        \n",
    "        if self.preload:\n",
    "            self.loadImages()\n",
    "            \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.current += 1\n",
    "        if self.current < self.high:\n",
    "            return self.__getitem__(self.current)\n",
    "        raise StopIteration\n",
    "            \n",
    "    def loadImage(self, idx):\n",
    "        \"\"\"\n",
    "        Load one single image\n",
    "        \"\"\"\n",
    "        if self.image_container is not None:\n",
    "            return self.image_container.get_image_from_name(self.image_ids[idx])\n",
    "        else:\n",
    "            print(\"wrong\")\n",
    "            return Image.open(self.PATH + \"images/\" + self.image_ids[idx]).convert(\"RGB\").resize(self.size)\n",
    "            \n",
    "    def getImage(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the image from index idx\n",
    "        \"\"\"\n",
    "        if self.preload:\n",
    "            return self.images[idx]\n",
    "        else:\n",
    "            print(\"wrong\")\n",
    "            return self.loadImage(idx)\n",
    "\n",
    "    def loadImages(self):\n",
    "        \"\"\"\n",
    "        Load all images\n",
    "        \"\"\"\n",
    "        if self.image_container is not None:\n",
    "            self.preload = True\n",
    "            self.images = self.image_container.get_images_from_name(self.image_ids)\n",
    "            if self.preprocess:\n",
    "                print(\"Preprocessed\")\n",
    "                #self.images = [self.transformImage(img) for img in self.images]\n",
    "        else:\n",
    "            for idx in range(len(self.image_ids)):\n",
    "                if self.preprocess:\n",
    "                    self.images.append(self.transformImage(self.loadImage(idx)))\n",
    "                else:\n",
    "                    self.images.append(self.loadImage(idx))\n",
    "        #print(\"Loading complete\")\n",
    "        \n",
    "    def transformImage(self, image):\n",
    "        \"\"\"\n",
    "        Transforms the image\n",
    "        \"\"\"\n",
    "        #print(\"transformed\")\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        return self.tfms(image)#.to(self.device)\n",
    "        \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        filename, target = self.image_ids[index], self.targets[index]\n",
    "        img = self.getImage(index)\n",
    "        \n",
    "        if not self.preprocess:\n",
    "            img = self.transformImage(img)\n",
    "        return img, target, filename\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        #return len(self.images)\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    \"\"\"\n",
    "    Functions for Verma active learning\n",
    "    \"\"\"\n",
    "    def getAllImagesNP(self):\n",
    "        \"\"\"\n",
    "        Returns all images from the Dataset\n",
    "        \"\"\"\n",
    "        if not self.preload:\n",
    "            self.preload = True\n",
    "            self.loadImages()\n",
    "        if self.image_list.size == 0:\n",
    "            image_liste = []\n",
    "            for img in self.images:\n",
    "                #np_img = np.moveaxis(np.array(img), -1, 0)\n",
    "                np_img = np.array(img)\n",
    "                image_liste.append(np_img)\n",
    "            self.image_list = np.array(image_liste)\n",
    "        return self.image_list\n",
    "\n",
    "    def getAllImages(self):\n",
    "        \"\"\"\n",
    "        Returns all images from the Dataset\n",
    "        \"\"\"\n",
    "        if not self.preload:\n",
    "            self.preload = True\n",
    "            self.loadImages()\n",
    "        return self.images\n",
    "\n",
    "    def getAllTargets(self):\n",
    "        \"\"\"\n",
    "        Returns all targets\n",
    "        \"\"\"\n",
    "        return self.targets\n",
    "\n",
    "    def getAllFilenames(self):\n",
    "        \"\"\"\n",
    "        Returns all filenames\n",
    "        \"\"\"\n",
    "        return self.image_ids.astype(str)\n",
    "\n",
    "    def getAllIndices(self):\n",
    "        return self.data.index\n",
    "\n",
    "Datas = BasicDataset(PATH, \"Airspace_Opacity\")\n",
    "result = Datas.getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0290294-e983-4975-afff-3e82bbb1d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIH_K_Fold_Dataloader:\n",
    "    def __init__(self, dataset, k=10, labelerIds=[4323195249, 4295194124], train_batch_size=8, test_batch_size=8,\n",
    "                 seed=42, fraction=1.0, maxLabels=800, preload=False, preprocess=False, prebuild=False, param=None):\n",
    "        self.dataset = dataset.getData()\n",
    "        print(\"Full length: \" + str(len(self.dataset)))\n",
    "        self.k = k\n",
    "        self.labelerIds = labelerIds\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.seed = seed\n",
    "        self.k_fold_datasets = []\n",
    "        self.k_fold_patient_ids = []\n",
    "        self.preload = preload\n",
    "        self.preprocess = preprocess\n",
    "        self.param = param\n",
    "        self.prebuild = prebuild\n",
    "\n",
    "        self.num_workers = 4\n",
    "\n",
    "        #TODO: Implement support for non overlapping Labels\n",
    "\n",
    "        ##\n",
    "        self.common = True\n",
    "        if self.common:\n",
    "            self.common_image_ids = self.dataset[\"Image ID\"].values.tolist()\n",
    "            names = [\"Patient ID\", \"Image ID\", \"GT\"]\n",
    "            for labelerId in self.labelerIds:\n",
    "                temp = self.dataset[self.dataset[str(labelerId)] != -1][\"Image ID\"].values.tolist()\n",
    "                self.common_image_ids = np.intersect1d(self.common_image_ids, temp)\n",
    "                names.append(str(labelerId))\n",
    "            self.data = self.dataset[self.dataset[\"Image ID\"].isin(self.common_image_ids)][names]\n",
    "\n",
    "        #Performance\n",
    "        patient_ids = self.data[\"Patient ID\"].unique()\n",
    "        num_patient_images = self.data.drop_duplicates(subset=[\"Image ID\"]).groupby(by=\"Patient ID\", as_index=False).count()[\"Image ID\"]\n",
    "        self.patient_performance = pd.DataFrame({\"Patient ID\": patient_ids, \"Num Patient Images\": num_patient_images})\n",
    "                     \n",
    "        for labeler_id in self.labelerIds:\n",
    "            temp = self.data[[\"Patient ID\", \"Image ID\", \"GT\", str(labeler_id)]]\n",
    "            temp[\"Expert_Correct\"] = self.data[\"GT\"] == self.data[str(labeler_id)]\n",
    "            sum = temp[[\"Patient ID\", \"Expert_Correct\"]].groupby(by=\"Patient ID\", as_index=False).sum()\n",
    "            sum.columns = [\"Patient ID\", f'{labeler_id}_num_correct']\n",
    "            self.patient_performance = pd.merge(self.patient_performance, sum, left_on=\"Patient ID\", right_on=\"Patient ID\")\n",
    "            self.patient_performance[f'{labeler_id}_perf'] = self.patient_performance[f'{labeler_id}_num_correct'] / self.patient_performance['Num Patient Images']\n",
    "\n",
    "        target_temp = self.patient_performance[f'{labelerIds[0]}_perf'].astype(str)\n",
    "        for labeler_id in labelerIds[1:]:\n",
    "            target_temp = target_temp + \"_\" + self.patient_performance[f'{labeler_id}_perf'].astype(str)\n",
    "        self.patient_performance[\"target\"] = target_temp \n",
    "\n",
    "        self.expert_labels = self.data\n",
    "        self._init_k_folds(maxLabels=maxLabels)\n",
    "\n",
    "        if self.prebuild:\n",
    "            self.buildDataloaders()\n",
    "\n",
    "    def _init_k_folds(self, fraction=1.0, maxLabels=800):\n",
    "        self.labels = self.expert_labels.drop_duplicates(subset=[\"Image ID\"])\n",
    "        self.labels = self.labels.fillna(0)\n",
    "        self.labels = self.labels[[\"Patient ID\", \"Image ID\", \"GT\"]]\n",
    "        \n",
    "        self.labels[\"Image ID\"] = self.labels[\"Image ID\"].astype('category')\n",
    "\n",
    "        self.image_container = ImageContainer(path=self.param[\"PATH\"], img_ids=self.labels[\"Image ID\"], preload=True, transform=None, preprocess=False, img_size=(128, 128))\n",
    "\n",
    "        kf_cv = StratifiedKFold(n_splits=self.k, shuffle=True, random_state=self.seed)\n",
    "\n",
    "        # _ sind train indizes, fold_test_idxs ist liste der test indizes\n",
    "        fold_data_idxs = [fold_test_idxs for (_, fold_test_idxs) in kf_cv.split(self.patient_performance[\"Patient ID\"].values, self.patient_performance[\"target\"].values)]\n",
    "\n",
    "       \n",
    "        for fold_idx in range(len(fold_data_idxs)):\n",
    "            \n",
    "            test = round(self.k*0.1)\n",
    "            val = round(self.k*0.2)\n",
    "            train = self.k - test - val\n",
    "            \n",
    "            test_folds_idxs = [(fold_idx + i) % self.k for i in range(test)]\n",
    "            test_fold_data_idxs = [fold_data_idxs[test_fold_idx] for test_fold_idx in test_folds_idxs]\n",
    "            test_fold_data_idxs = list(chain.from_iterable(test_fold_data_idxs))\n",
    "            \n",
    "            #test_fold_idx = fold_idx # Nummer des Folds\n",
    "            #test_fold_data_idxs = fold_data_idxs[test_fold_idx] # Array der Test Indizes\n",
    "\n",
    "            # use next 2 folds for validation set\n",
    "            val_folds_idxs = [(fold_idx + test + i) % self.k for i in range(val)]\n",
    "            val_fold_data_idxs = [fold_data_idxs[val_fold_idx] for val_fold_idx in val_folds_idxs]\n",
    "            val_fold_data_idxs = list(chain.from_iterable(val_fold_data_idxs))\n",
    "\n",
    "            # use next 7 folds for training set\n",
    "            train_folds_idxs = [(fold_idx + (test + val) + i) % self.k for i in range(train)]\n",
    "            #print(train_folds_idxs)\n",
    "            train_folds_data_idxs = [fold_data_idxs[train_fold_idx] for train_fold_idx in train_folds_idxs]\n",
    "            train_folds_data_idxs = list(chain.from_iterable(train_folds_data_idxs))\n",
    "\n",
    "            \n",
    "            train_patient_ids = self.patient_performance[\"Patient ID\"].iloc[train_folds_data_idxs]\n",
    "            #train_patient_ids = self.patient_performance[\"Patient ID\"].iloc[train_folds_data_idxs].sample(n=min(maxLabels,len(train_folds_data_idxs)))\n",
    "            val_patient_ids = self.patient_performance[\"Patient ID\"].iloc[val_fold_data_idxs]\n",
    "            test_patient_ids = self.patient_performance[\"Patient ID\"].iloc[test_fold_data_idxs]\n",
    "\n",
    "            #expert_train = self.labels[self.labels[\"Patient ID\"].isin(train_patient_ids)]\n",
    "            expert_train = self.labels[self.labels[\"Patient ID\"].isin(train_patient_ids)]\n",
    "            expert_train = self.labels[self.labels[\"Patient ID\"].isin(train_patient_ids)].sample(n=min(maxLabels,len(expert_train)))\n",
    "            expert_val = self.labels[self.labels[\"Patient ID\"].isin(val_patient_ids)]\n",
    "            expert_test = self.labels[self.labels[\"Patient ID\"].isin(test_patient_ids)]\n",
    "\n",
    "            # check that patients are not shared across training, validation and test split\n",
    "            overlap = expert_train[expert_train[\"Patient ID\"].isin(expert_val[\"Patient ID\"])]\n",
    "            assert len(overlap) == 0, \"Train and Val Patient Ids overlap\"\n",
    "\n",
    "            overlap = expert_train[expert_train[\"Patient ID\"].isin(expert_test[\"Patient ID\"])]\n",
    "            assert len(overlap) == 0, \"Train and Test Patient Ids overlap\"\n",
    "\n",
    "            overlap = expert_val[expert_val[\"Patient ID\"].isin(expert_test[\"Patient ID\"])]\n",
    "            assert len(overlap) == 0, \"Val and Test Patient Ids overlap\"\n",
    "\n",
    "            expert_train = expert_train[[\"Image ID\", \"GT\"]]\n",
    "            expert_val = expert_val[[\"Image ID\", \"GT\"]]\n",
    "            expert_test = expert_test[[\"Image ID\", \"GT\"]]\n",
    "            \n",
    "            print(\"Length of train + test + val: \" + str(len(expert_train) + len(expert_val) + len(expert_test)))\n",
    "\n",
    "            self.k_fold_datasets.append((expert_train, expert_val, expert_test))\n",
    "\n",
    "    def get_data_loader_for_fold(self, fold_idx):\n",
    "        if self.prebuild:\n",
    "            return self.loaders[fold_idx][0], self.loaders[fold_idx][1], self.loaders[fold_idx][2]\n",
    "        else:\n",
    "            return self.create_Dataloader_for_Fold(fold_idx)\n",
    "\n",
    "    def get_dataset_for_folder(self, fold_idx):\n",
    "        expert_train, expert_val, expert_test = self.k_fold_datasets[fold_idx]\n",
    "\n",
    "        return expert_train, expert_val, expert_test\n",
    "\n",
    "    def create_Dataloader_for_Fold(self, idx):\n",
    "        expert_train, expert_val, expert_test = self.k_fold_datasets[idx]\n",
    "\n",
    "        expert_train_dataset = NIHDataset(expert_train, preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "        expert_val_dataset = NIHDataset(expert_val, preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "        expert_test_dataset = NIHDataset(expert_test, preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=expert_train_dataset, batch_size=self.train_batch_size, num_workers=self.num_workers, shuffle=True, drop_last=False, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=expert_val_dataset, batch_size=self.test_batch_size, num_workers=self.num_workers, shuffle=True, drop_last=False, pin_memory=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=expert_test_dataset, batch_size=self.test_batch_size, num_workers=self.num_workers, shuffle=True, drop_last=False, pin_memory=True)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def buildDataloaders(self):\n",
    "        self.loaders = []\n",
    "        for i in range(self.k):\n",
    "            train_loader, val_loader, test_loader = self.create_Dataloader_for_Fold(i)\n",
    "            loader_set = [train_loader, val_loader, test_loader]\n",
    "            self.loaders.append(loader_set)\n",
    "            print(\"Loaded set number \" + str(i))\n",
    "\n",
    "    def getFullDataloader(self):\n",
    "        full_dataset = NIHDataset(self.labels[[\"Image ID\", \"GT\"]], preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "        return torch.utils.data.DataLoader(dataset=full_dataset, batch_size=self.train_batch_size, num_workers=self.num_workers, shuffle=True, drop_last=False, pin_memory=True)\n",
    "\n",
    "    def get_ImageContainer(self):\n",
    "        return self.image_container\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.labels[self.labels[\"Patient ID\"].isin(self.patient_performance[\"Patient ID\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4bb7d4-8d02-4d6e-a328-fbc05a5e3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageContainer:\n",
    "    def __init__(self, path, img_ids, preload=True, transform=None, preprocess=False, img_size=(128, 128)):\n",
    "        self.PATH = path\n",
    "        self.image_ids = img_ids.values\n",
    "        self.preload = preload\n",
    "        self.preprocess = preprocess\n",
    "        self.img_size = img_size\n",
    "        self.images = []\n",
    "\n",
    "        if self.preload:\n",
    "            self.loadImages()\n",
    "        \n",
    "\n",
    "    def loadImages(self):\n",
    "        for idx in range(len(self.image_ids)):\n",
    "            self.images.append(self.loadImage(idx))\n",
    "            \n",
    "            if self.preprocess:\n",
    "                self.images[idx] = self.transformImage(self.images[idx])\n",
    "\n",
    "    def loadImage(self, idx):\n",
    "        \"\"\"\n",
    "        Load one single image\n",
    "        \"\"\"\n",
    "        return Image.open(self.PATH + \"images/\" + self.image_ids[idx]).convert(\"RGB\").resize(self.img_size)\n",
    "            \n",
    "    def get_image_from_id(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the image from index idx\n",
    "        \"\"\"\n",
    "        if self.preload:\n",
    "            return self.images[idx]\n",
    "        else:\n",
    "            return self.loadImage(idx)\n",
    "\n",
    "    def get_image_from_name(self, fname):\n",
    "        if self.preload:\n",
    "            return self.images[np.where(self.image_ids == fname)]\n",
    "        else:\n",
    "            return self.get_image_from_id(np.where(self.image_ids == fname))\n",
    "\n",
    "    def get_images_from_name(self, fnames):\n",
    "        if self.preload:\n",
    "            return [self.images[np.where(self.image_ids == fname)[0][0]] for fname in fnames]\n",
    "        else:\n",
    "            return [self.get_image_from_id(np.where(self.image_ids == fname)[0][0]) for fname in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a5d496a-02c0-4cb0-b486-416ae48c9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager():\n",
    "    \"\"\"\n",
    "    Class to contain and manage all data for all experiments\n",
    "    \n",
    "    This class contains all functions to get data and dataloaders for every step of the experiment\n",
    "    \n",
    "    It also implements a k fold\n",
    "    \"\"\"\n",
    "    def set_seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    def __init__(self, path, target, param, seeds):\n",
    "        \n",
    "        self.path = path\n",
    "        self.target = target\n",
    "        self.param = param\n",
    "        self.seeds = seeds\n",
    "        \n",
    "        self.labeler_ids = param[\"LABELER_IDS\"]\n",
    "        \n",
    "        self.basicDataset = BasicDataset(Path=self.path, target=self.target)\n",
    "        \n",
    "        self.fullImageContainer = ImageContainer(path=self.path, img_ids=self.basicDataset.getData()[\"Image ID\"], preload=True, transform=None, preprocess=False, img_size=(128, 128))\n",
    "        \n",
    "        #self.createData()\n",
    "    \n",
    "    def createData(self):\n",
    "        \n",
    "        self.kFoldDataloaders = {}\n",
    "        self.SSLDatasets = {}\n",
    "        \n",
    "        for seed in self.seeds:\n",
    "            self.set_seed(seed)\n",
    "        \n",
    "            self.kFoldDataloaders[seed] = NIH_K_Fold_Dataloader(\n",
    "                    dataset = self.basicDataset,\n",
    "                    k = self.param[\"K\"],\n",
    "                    labelerIds = self.labeler_ids,\n",
    "                    train_batch_size = self.param[\"TRAIN_BATCH_SIZE\"],\n",
    "                    test_batch_size = self.param[\"TEST_BATCH_SIZE\"],\n",
    "                    seed = seed,\n",
    "                    #maxLabels = maxL,\n",
    "                    preprocess = False,\n",
    "                    preload = self.param[\"PRELOAD\"],\n",
    "                    prebuild = self.param[\"PREBUILD\"],\n",
    "                    param = self.param\n",
    "                )\n",
    "            \n",
    "            self.SSLDatasets[seed] = self.SSLDataset = SSLDataset(dataset=self.basicDataset, kFoldDataloader=self.kFoldDataloaders[seed], \n",
    "                                                                 imageContainer=self.fullImageContainer, labeler_ids=self.labeler_ids, param=self.param, seed=seed)\n",
    "        \n",
    "         \n",
    "    def getKFoldDataloader(self, seed):\n",
    "        return self.kFoldDataloaders[seed]\n",
    "    \n",
    "    def getSSLDataset(self, seed):\n",
    "        return self.SSLDatasets[seed]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7bc4f53-d2c4-41e0-9b93-8b4e46175cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class SSLDataset():\n",
    "    def __init__(self, dataset, kFoldDataloader, imageContainer, labeler_ids, param, seed):\n",
    "        self.basicDataset = dataset\n",
    "        self.kFoldDataloader = kFoldDataloader\n",
    "        self.imageContainer = imageContainer\n",
    "        self.labeler_ids = labeler_ids\n",
    "        self.param = param\n",
    "        self.seed = seed\n",
    "        self.set_seed(self.seed)\n",
    "        \n",
    "        self.k_fold_datasets = []\n",
    "        self.k_fold_datasets_labeled = []\n",
    "        \n",
    "        \n",
    "        self.unpack_param()\n",
    "        self.setup()\n",
    "\n",
    "        if self.prebuild:\n",
    "            self.buildDataloaders()\n",
    "        \n",
    "    def unpack_param(self):\n",
    "        self.k = self.param[\"K\"]\n",
    "        self.overlap_k = self.param[\"OVERLAP K\"]\n",
    "        self.n_labels = self.param[\"NUMBER LABELS\"]\n",
    "        \n",
    "    def set_seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    def setup(self):\n",
    "\n",
    "        self.data = self.basicDataset.getDataForLabelers(self.labeler_ids).astype('category').drop_duplicates(subset=['Image ID', \"GT\"], keep='first')\n",
    "        \n",
    "        self.labels = self.data.drop(\"Patient ID\", axis=1)\n",
    "        \n",
    "        self.used_image_ids = self.kFoldDataloader.getData().drop(\"Patient ID\", axis=1)\n",
    "        \n",
    "        self.unused_data = self.data[~self.data[\"Image ID\"].isin(self.used_image_ids[\"Image ID\"])]\n",
    "        self.used_data = self.data[self.data[\"Image ID\"].isin(self.used_image_ids[\"Image ID\"])]\n",
    "        \n",
    "        kf = KFold(n_splits=self.k, shuffle=True, random_state=self.seed)\n",
    "       \n",
    "        #TODO: Is Patient ID better than Image ID, are splitted patients a problem?\n",
    "        \n",
    "        split_target = \"Image ID\"\n",
    "        #split_target = \"Patient ID\"\n",
    "        \n",
    "        self.unused_data.head(10)\n",
    "        \n",
    "        split = kf.split(self.unused_data[\"Image ID\"], self.unused_data[\"GT\"])\n",
    "        \n",
    "        fold_data_idxs = [fold_test_idxs for (_, fold_test_idxs) in split]\n",
    "        \n",
    "        #fold_data_idxs = [fold_test_idxs for (_, fold_test_idxs) in kf_cv.split(self.patient_performance[\"Patient ID\"].values, self.patient_performance[\"target\"].values)]\n",
    "\n",
    "            \n",
    "        #for j, (train_index, test_index) in enumerate(kf.split(self.unused_data[\"Image ID\"], self.unused_data[\"GT\"])):\n",
    "        #for j, (train_index, test_index) in enumerate(split):\n",
    "        for fold_idx in range(len(fold_data_idxs)):\n",
    "            \n",
    "            #fold_idx = j\n",
    "            \n",
    "            #print(j)\n",
    "            \n",
    "            test = round(self.k*0.1)\n",
    "            val = round(self.k*0.2)\n",
    "            train = self.k - test - val\n",
    "            \n",
    "            test_folds_idxs = [(fold_idx + i) % self.k for i in range(test)]\n",
    "            test_fold_data_idxs = [fold_data_idxs[test_fold_idx] for test_fold_idx in test_folds_idxs]\n",
    "            test_fold_data_idxs = list(chain.from_iterable(test_fold_data_idxs))\n",
    "            \n",
    "            #test_fold_idx = fold_idx # Nummer des Folds\n",
    "            #test_fold_data_idxs = fold_data_idxs[test_fold_idx] # Array der Test Indizes\n",
    "\n",
    "            # use next 2 folds for validation set\n",
    "            val_folds_idxs = [(fold_idx + test + i) % self.k for i in range(val)]\n",
    "            val_fold_data_idxs = [fold_data_idxs[val_fold_idx] for val_fold_idx in val_folds_idxs]\n",
    "            val_fold_data_idxs = list(chain.from_iterable(val_fold_data_idxs))\n",
    "\n",
    "            # use next 7 folds for training set\n",
    "            train_folds_idxs = [(fold_idx + (test + val) + i) % self.k for i in range(train)]\n",
    "            #print(train_folds_idxs)\n",
    "            train_folds_data_idxs = [fold_data_idxs[train_fold_idx] for train_fold_idx in train_folds_idxs]\n",
    "            train_folds_data_idxs = list(chain.from_iterable(train_folds_data_idxs))\n",
    "\n",
    "            \n",
    "            train_patient_ids = self.unused_data[split_target].iloc[train_folds_data_idxs]\n",
    "            #train_patient_ids = self.patient_performance[\"Patient ID\"].iloc[train_folds_data_idxs].sample(n=min(maxLabels,len(train_folds_data_idxs)))\n",
    "            val_patient_ids = self.unused_data[split_target].iloc[val_fold_data_idxs]\n",
    "            test_patient_ids = self.unused_data[split_target].iloc[test_fold_data_idxs]\n",
    "            \n",
    "            \n",
    "            #Add the data from the other k_fold\n",
    "            used_train, used_val, used_test = self.kFoldDataloader.get_dataset_for_folder(fold_idx = fold_idx)\n",
    "            \n",
    "            \n",
    "            train_patient_ids = pd.concat([train_patient_ids, used_train[\"Image ID\"]], ignore_index=True)\n",
    "            val_patient_ids = pd.concat([val_patient_ids, used_val[\"Image ID\"]], ignore_index=True)\n",
    "            test_patient_ids = pd.concat([test_patient_ids, used_test[\"Image ID\"]], ignore_index=True)\n",
    "\n",
    "            expert_train = self.labels[self.labels[split_target].isin(train_patient_ids)]\n",
    "            #expert_train = self.labels[self.labels[split_target].isin(train_patient_ids)].sample(n=min(maxLabels,len(expert_train)))\n",
    "            expert_val = self.labels[self.labels[split_target].isin(val_patient_ids)]\n",
    "            expert_test = self.labels[self.labels[split_target].isin(test_patient_ids)]\n",
    "            \n",
    "\n",
    "\n",
    "            # check that patients are not shared across training, validation and test split\n",
    "            overlap = expert_train[expert_train[split_target].isin(expert_val[split_target])]\n",
    "            assert len(overlap) == 0, \"Train and Val Patient Ids overlap\"\n",
    "\n",
    "            overlap = expert_train[expert_train[split_target].isin(expert_test[split_target])]\n",
    "            assert len(overlap) == 0, \"Train and Test Patient Ids overlap\"\n",
    "\n",
    "            overlap = expert_val[expert_val[split_target].isin(expert_test[split_target])]\n",
    "            assert len(overlap) == 0, \"Val and Test Patient Ids overlap\"\n",
    "\n",
    "            #expert_train = expert_train[[\"Image ID\", \"GT\"]]\n",
    "            #expert_val = expert_val[[\"Image ID\", \"GT\"]]\n",
    "            #expert_test = expert_test[[\"Image ID\", \"GT\"]]\n",
    "\n",
    "            self.k_fold_datasets.append((expert_train, expert_val, expert_test))\n",
    "            self.k_fold_datasets_labeled.append((used_train, used_val, used_test))\n",
    "            print(\"Added\")\n",
    "        \n",
    "        self.createLabeledIndices(labelerIds=self.labeler_ids, n_L=n_labels, k=overlap_k, seed=self.seed)\n",
    "            \n",
    "    def sampleIndices(self, n, k, data, experten, seed = None):\n",
    "        \"\"\"\n",
    "        Creates indices for which data are labeled for each expert\n",
    "        \n",
    "        n - number of labels\n",
    "        k - number of shared labeled data\n",
    "        data - the data (x and y)\n",
    "        experten - list of labelerIds\n",
    "        seed\n",
    "        \n",
    "        \"\"\"\n",
    "        #Set seed\n",
    "        if seed is not None:\n",
    "            self.set_seed(seed)\n",
    "            \n",
    "        data = data.reset_index(drop=True)\n",
    "            \n",
    "        #Get all indices\n",
    "        all_indices = indices = [j for j in range(len(data))]\n",
    "        \n",
    "        #Get which indices are labeled from which expert\n",
    "        experts_indices = {}\n",
    "        common_indices = all_indices\n",
    "        for expert in experten:\n",
    "            experts_indices[expert] = [j for j in all_indices if (data[str(expert)][j] != -1)]\n",
    "            common_indices = set(common_indices).intersection(experts_indices[expert])\n",
    "        common_indices = list(common_indices)\n",
    "        common_indices.sort()\n",
    "\n",
    "        #Sample the shared indices\n",
    "        same_indices = random.sample(common_indices, k)\n",
    "        diff_indices = []\n",
    "        used_indices = same_indices\n",
    "        indices = {}\n",
    "        if k == n:\n",
    "            for expert in experten:\n",
    "                indices[expert] = same_indices\n",
    "        if k < n: #If there are not shared indices\n",
    "            for expert in experten:\n",
    "                working_indices = experts_indices[expert]\n",
    "                temp_indices = []\n",
    "                count = 0 # To avoid infinity loop\n",
    "                while len(temp_indices) < (n - k):\n",
    "                    count += 1\n",
    "                    temp = random.sample(working_indices, 1)\n",
    "                    if temp not in used_indices:\n",
    "                        temp_indices = temp_indices + temp\n",
    "                        used_indices = used_indices + temp\n",
    "                    if count >= 1000:\n",
    "                        temp = random.sample(used_indices, n-k-len(temp_indices))\n",
    "                        if isinstance(temp, list):\n",
    "                            temp_indices = temp_indices + temp\n",
    "                        else:\n",
    "                            temp_indices.append(temp)\n",
    "                        break\n",
    "                indices[expert] = (same_indices + temp_indices)\n",
    "        return indices\n",
    "            \n",
    "    def createLabeledIndices(self, labelerIds, n_L, k, seed=0):\n",
    "        \"\"\"\n",
    "        Creates the labeled indices for all folds for every expert\n",
    "        \n",
    "        n_L - number of labeled images\n",
    "        k - number of shared labeled images\n",
    "        seed - random seed to init the random function\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.set_seed(seed)\n",
    "        #Generate random seeds for every loop\n",
    "        seeds = [random.randint(0, 10000) for k in range(self.k)]\n",
    "        self.labeled_indices = []\n",
    "        for i in range(self.k):\n",
    "            train_data, _, _ = self.k_fold_datasets[i]            \n",
    "            sampled_indices = self.sampleIndices(n=n_L, k=k, data=train_data, experten=labelerIds, seed=seeds[i])\n",
    "            #print(sampled_indices)\n",
    "            self.labeled_indices.append(sampled_indices)\n",
    "        \n",
    "    def getLabeledIndices(self, labelerId, fold_idx):\n",
    "        return self.labeled_indices[fold_idx][labelerId]\n",
    "        \n",
    "    def getDatasetsForExpert(self, labelerId, fold_idx):\n",
    "        print(\"Index: \" + str(fold_idx))\n",
    "        working_train, working_val, working_test = self.k_fold_datasets[fold_idx]\n",
    "        working_train = working_train[[\"Image ID\", str(labelerId)]]\n",
    "        working_val = working_val[[\"Image ID\", str(labelerId)]]\n",
    "        working_test = working_test[[\"Image ID\", str(labelerId)]]\n",
    "        \n",
    "        return working_train, working_val, working_test\n",
    "    \n",
    "    def getTrainDataset(self, labelerId, fold_idx):\n",
    "        train_data, _, _ = self.getDatasetsForExpert(labelerId, fold_idx)\n",
    "        X = np.array(train_data[\"Image ID\"])\n",
    "        y = np.array(train_data[str(labelerId)])\n",
    "        \n",
    "        all_indices = [i for i in range(len(y))]\n",
    "        labeled_indices = self.getLabeledIndices(labelerId, fold_idx)\n",
    "        \n",
    "        data_x = [X[ind] for ind in labeled_indices]\n",
    "        label_x = [y[ind] for ind in labeled_indices]\n",
    "        \n",
    "        data_u = [X[ind] for ind in all_indices if (ind not in labeled_indices)]\n",
    "        label_u = [y[ind] for ind in all_indices if (ind not in labeled_indices)]\n",
    "        \n",
    "        return data_x, label_x, data_u, label_u\n",
    "    \n",
    "    def getValDataset(self, labelerId, fold_idx):\n",
    "        _, val_data, _ = self.getDatasetsForExpert(labelerId, fold_idx)\n",
    "        X = np.array(val_data[\"Image ID\"])\n",
    "        y = np.array(val_data[str(labelerId)])\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def getTestDataset(self, labelerId, fold_idx):\n",
    "        _, _, test_data = self.getDatasetsForExpert(labelerId, fold_idx)\n",
    "        X = np.array(test_data[\"Image ID\"])\n",
    "        y = np.array(test_data[str(labelerId)])\n",
    "        \n",
    "        return X, y\n",
    "        \n",
    "    \n",
    "    def get_train_loader_interface(self, expert, batch_size, mu, n_iters_per_epoch, L, method='comatch', imsize=(128, 128), fold_idx=0):\n",
    "        labeler_id = expert.labeler_id\n",
    "        \n",
    "        data_x, label_x, data_u, label_u = self.getTrainDataset(labeler_id, fold_idx)\n",
    "        \n",
    "        #print(f'Label check: {Counter(label_x)}')\n",
    "        print(\"Labels: \" + str(len(label_x)))\n",
    "        ds_x = NIH_SSL_Dataset(\n",
    "            data=data_x,\n",
    "            labels=label_x,\n",
    "            mode='train_x',\n",
    "            image_container=self.imageContainer,\n",
    "            imsize=imsize\n",
    "        )\n",
    "        sampler_x = RandomSampler(ds_x, replacement=True, num_samples=n_iters_per_epoch * batch_size)\n",
    "        batch_sampler_x = BatchSampler(sampler_x, batch_size, drop_last=True)  # yield a batch of samples one time\n",
    "        dl_x = torch.utils.data.DataLoader(\n",
    "            ds_x,\n",
    "            batch_sampler=batch_sampler_x,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        if data_u is None:\n",
    "            return dl_x\n",
    "        else:\n",
    "            ds_u = NIH_SSL_Dataset(\n",
    "                data=data_u,\n",
    "                labels=label_u,\n",
    "                mode='train_u_%s'%method,\n",
    "                image_container=self.imageContainer,\n",
    "                imsize=imsize\n",
    "            )\n",
    "            sampler_u = RandomSampler(ds_u, replacement=True, num_samples=mu * n_iters_per_epoch * batch_size)\n",
    "            batch_sampler_u = BatchSampler(sampler_u, batch_size * mu, drop_last=True)\n",
    "            dl_u = torch.utils.data.DataLoader(\n",
    "                ds_u,\n",
    "                batch_sampler=batch_sampler_u,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            return dl_x, dl_u\n",
    "        \n",
    "    def get_val_loader_interface(self, expert, batch_size, num_workers, pin_memory=True, imsize=(128, 128)):\n",
    "        \"\"\"Get data loader for the validation set\n",
    "\n",
    "        :param expert: Synthetic cifar expert\n",
    "        :param batch_size: Batch size\n",
    "        :param num_workers: Number of workers\n",
    "        :param pin_memory: Pin memory\n",
    "        :param imsize: Size of images\n",
    "\n",
    "        :return: Dataloader\n",
    "        \"\"\"\n",
    "        labeler_id = expert.labeler_id\n",
    "        data, labels = self.getValDataset(labeler_id, fold_idx)\n",
    "\n",
    "        ds = NIH_SSL_Dataset(\n",
    "            data=data,\n",
    "            labels=labels,\n",
    "            mode='test',\n",
    "            imsize=imsize\n",
    "        )\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            ds,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "            drop_last=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        return dl\n",
    "    \n",
    "    def get_test_loader_interface(self, expert, batch_size, num_workers, pin_memory=True, imsize=(128, 128)):\n",
    "        \"\"\"Get data loader for the validation set\n",
    "\n",
    "        :param expert: Synthetic cifar expert\n",
    "        :param batch_size: Batch size\n",
    "        :param num_workers: Number of workers\n",
    "        :param pin_memory: Pin memory\n",
    "        :param imsize: Size of images\n",
    "\n",
    "        :return: Dataloader\n",
    "        \"\"\"\n",
    "        labeler_id = expert.labeler_id\n",
    "        data, labels = self.getTestDataset(labeler_id, fold_idx)\n",
    "\n",
    "        ds = NIH_SSL_Dataset(\n",
    "            data=data,\n",
    "            labels=labels,\n",
    "            mode='test',\n",
    "            imsize=imsize\n",
    "        )\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            ds,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "            drop_last=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        return dl\n",
    "\n",
    "    \"\"\"\n",
    "    Functions to get the whole dataset as dataloaders\n",
    "    \"\"\"\n",
    "    def get_data_loader_for_fold(self, fold_idx):\n",
    "        if self.prebuild:\n",
    "            return self.loaders[fold_idx][0], self.loaders[fold_idx][1], self.loaders[fold_idx][2]\n",
    "        else:\n",
    "            return self.create_Dataloader_for_Fold(fold_idx)\n",
    "\n",
    "    def get_dataset_for_folder(self, fold_idx):\n",
    "        expert_train, expert_val, expert_test = self.k_fold_datasets[fold_idx]\n",
    "\n",
    "        return expert_train, expert_val, expert_test\n",
    "\n",
    "    def create_Dataloader_for_Fold(self, idx):\n",
    "        expert_train, expert_val, expert_test = self.k_fold_datasets[idx]\n",
    "\n",
    "        expert_train_dataset = NIHDataset(expert_train, preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "        expert_val_dataset = NIHDataset(expert_val, preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "        expert_test_dataset = NIHDataset(expert_test, preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=expert_train_dataset, batch_size=self.train_batch_size, num_workers=self.num_workers, shuffle=True, drop_last=False, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=expert_val_dataset, batch_size=self.test_batch_size, num_workers=self.num_workers, shuffle=True, drop_last=False, pin_memory=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=expert_test_dataset, batch_size=self.test_batch_size, num_workers=self.num_workers, shuffle=True, drop_last=False, pin_memory=True)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def buildDataloaders(self):\n",
    "        self.loaders = []\n",
    "        for i in range(self.k):\n",
    "            train_loader, val_loader, test_loader = self.create_Dataloader_for_Fold(i)\n",
    "            loader_set = [train_loader, val_loader, test_loader]\n",
    "            self.loaders.append(loader_set)\n",
    "            print(\"Loaded set number \" + str(i))\n",
    "\n",
    "    def getFullDataloader(self):\n",
    "        full_dataset = NIHDataset(self.labels[[\"Image ID\", \"GT\"]], preload=self.preload, preprocess=self.preprocess, param=self.param, image_container=self.image_container)\n",
    "        return torch.utils.data.DataLoader(dataset=full_dataset, batch_size=self.train_batch_size, num_workers=self.num_workers, shuffle=True, drop_last=False, pin_memory=True)\n",
    "\n",
    "    def get_ImageContainer(self):\n",
    "        return self.image_container\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.labels[self.labels[\"Patient ID\"].isin(self.patient_performance[\"Patient ID\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e601f4-05e3-45ff-bc21-9aa5a49299de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NIH_SSL_Dataset(Dataset):\n",
    "    \"\"\"Class representing the NIH dataset\n",
    "\n",
    "    :param data: Images\n",
    "    :param labels: Labels\n",
    "    :param mode: Mode\n",
    "    :param imsize: Image size\n",
    "\n",
    "    :ivar data: Images\n",
    "    :ivar labels: Labels\n",
    "    :ivar mode: Mode\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels, mode, image_container=None, imsize=(128, 128)) -> None:\n",
    "        self.image_ids = data\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.image_container = image_container\n",
    "        \n",
    "        self.loadImages()\n",
    "\n",
    "        mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "\n",
    "        trans_weak = T.Compose([\n",
    "            T.Resize((imsize, imsize)),\n",
    "            T.PadandRandomCrop(border=4, cropsize=(imsize, imsize)),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.Normalize(mean, std),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "        trans_strong0 = T.Compose([\n",
    "            T.Resize((imsize, imsize)),\n",
    "            T.PadandRandomCrop(border=4, cropsize=(imsize, imsize)),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            RandomAugment(2, 10),\n",
    "            T.Normalize(mean, std),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "        trans_strong1 = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop(imsize, scale=(0.2, 1.)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "        if self.mode == 'train_x':\n",
    "            self.trans = trans_weak\n",
    "        elif self.mode == 'train_u_comatch':\n",
    "            self.trans = ThreeCropsTransform(trans_weak, trans_strong0, trans_strong1)\n",
    "        elif self.mode == 'train_u_fixmatch':\n",
    "            self.trans = TwoCropsTransform(trans_weak, trans_strong0)\n",
    "        else:\n",
    "            self.trans = T.Compose([\n",
    "                T.Resize((imsize, imsize)),\n",
    "                T.Normalize(mean, std),\n",
    "                T.ToTensor(),\n",
    "            ])\n",
    "            \n",
    "    def loadImages(self):\n",
    "        \"\"\"\n",
    "        Load all images\n",
    "        \"\"\"\n",
    "        if self.image_container is not None:\n",
    "            self.images = self.image_container.get_images_from_name(self.image_ids)\n",
    "        else:\n",
    "            for idx in range(len(self.image_ids)):\n",
    "                if self.preprocess:\n",
    "                    self.images.append(self.transformImage(self.loadImage(idx)))\n",
    "                else:\n",
    "                    self.images.append(self.loadImage(idx))\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        filename, label = self.image_ids[index], self.labels[index]\n",
    "        im = self.images[index]\n",
    "        return self.trans(im), label, filename\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7244be9a-6a96-4e44-825f-e6e73b4b56ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TwoCropsTransform:\n",
    "    \"\"\"Take 2 random augmentations of one image\n",
    "\n",
    "    :param trans_weak: Transform for the weak augmentation\n",
    "    :param trans_strong: Transform for the strong augmentation\n",
    "\n",
    "    :ivar trans_weak: Transform for the weak augmentation\n",
    "    :ivar trans_strong: Transform for the strong augmentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trans_weak, trans_strong):\n",
    "        self.trans_weak = trans_weak\n",
    "        self.trans_strong = trans_strong\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x1 = self.trans_weak(x)\n",
    "        x2 = self.trans_strong(x)\n",
    "        return [x1, x2]\n",
    "\n",
    "\n",
    "class ThreeCropsTransform:\n",
    "    \"\"\"Take 3 random augmentations of one image\n",
    "\n",
    "    :param trans_weak: Transform for the weak augmentation\n",
    "    :param trans_strong0: Transform for the first strong augmentation\n",
    "    :param trans_strong1: Transform for the second strong augmentation\n",
    "\n",
    "    :ivar trans_weak: Transform for the weak augmentation\n",
    "    :ivar trans_strong0: Transform for the first strong augmentation\n",
    "    :ivar trans_strong1: Transform for the second strong augmentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trans_weak, trans_strong0, trans_strong1):\n",
    "        self.trans_weak = trans_weak\n",
    "        self.trans_strong0 = trans_strong0\n",
    "        self.trans_strong1 = trans_strong1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x1 = self.trans_weak(x)\n",
    "        x2 = self.trans_strong0(x)\n",
    "        x3 = self.trans_strong1(x)\n",
    "        return [x1, x2, x3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143d14f-f67f-4b23-82c7-3aca57189bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5da690-157e-4eef-b9b6-3a4eb0a8402f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Datasets/NIH/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "252cdfab-5f3f-443f-907b-9cdef33811e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"TARGET\": \"Airspace_Opacity\",\n",
    "    \"PATH\": \"../Datasets/NIH/\",\n",
    "    \"K\": 10, #Number of folds\n",
    "    \"LABELER_IDS\": [4323195249, 4295232296],\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"alpha\": 1.0, #scaling parameter for the loss function, default=1.0\n",
    "    #\"epochs\": 50,\n",
    "    \"epochs\": 50,\n",
    "    \"patience\": 15, #number of patience steps for early stopping the training\n",
    "    \"expert_type\": \"MLPMixer\", #specify the expert type. For the type of experts available, see-> models -> experts. defualt=predict\n",
    "    \"n_classes\": 2, #K for K class classification\n",
    "    \"K\": 10, #\n",
    "    \n",
    "    \"TRAIN_BATCH_SIZE\": 64,\n",
    "    \"TEST_BATCH_SIZE\": 64,\n",
    "    \"NUM_EXPERTS\": 2,\n",
    "\n",
    "    \"PRELOAD\": True,\n",
    "    \"PREBUILD\": True,\n",
    "    \n",
    "    \"OVERLAP K\": 8,\n",
    "    \"NUMBER LABELS\": 8,\n",
    "}\n",
    "SEEDS = [42]\n",
    "NEPTUNE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70316f94-fe3d-4039-9b4e-7dd082d7ce2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataManager = DataManager(path = PATH, target = param[\"TARGET\"], param=param, seeds=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1dc1eab-017f-4108-9c60-7b74e6e842cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full length: 4381\n",
      "Len self.labels: 852\n",
      "Len self.patient_performance: 539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joli/joli-env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Length of train + test + val: 852\n",
      "Loaded set number 0\n",
      "Loaded set number 1\n",
      "Loaded set number 2\n",
      "Loaded set number 3\n",
      "Loaded set number 4\n",
      "Loaded set number 5\n",
      "Loaded set number 6\n",
      "Loaded set number 7\n",
      "Loaded set number 8\n",
      "Loaded set number 9\n",
      "Len used data: 852\n",
      "Len all data: 539\n",
      "Len returned: 852\n",
      "len data: 852\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n",
      "Added\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sslDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 54\u001b[0m, in \u001b[0;36mDataManager.createData\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_seed(seed)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkFoldDataloaders[seed] \u001b[38;5;241m=\u001b[39m NIH_K_Fold_Dataloader(\n\u001b[1;32m     41\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasicDataset,\n\u001b[1;32m     42\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\n\u001b[1;32m     52\u001b[0m     )\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSSLDatasets[seed] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSSLDataset \u001b[38;5;241m=\u001b[39m \u001b[43mSSLDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasicDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkFoldDataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkFoldDataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mimageContainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfullImageContainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabeler_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabeler_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mSSLDataset.__init__\u001b[0;34m(self, dataset, kFoldDataloader, imageContainer, labeler_ids, param, seed)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_fold_datasets_labeled \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpack_param()\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 130\u001b[0m, in \u001b[0;36mSSLDataset.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_fold_datasets_labeled\u001b[38;5;241m.\u001b[39mappend((used_train, used_val, used_test))\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m \u001b[43msslDataset\u001b[49m\u001b[38;5;241m.\u001b[39mcreateLabeledIndices(labelerIds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabeler_ids, n_L\u001b[38;5;241m=\u001b[39mn_labels, k\u001b[38;5;241m=\u001b[39moverlap_k, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sslDataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataManager.createData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ced2f2-0a27-4cb6-b256-ce735776e338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sslDataset = dataManager.getSSLDataset(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebdbbe-523c-4d31-a5de-510eda2548a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, val, test = sslDataset.k_fold_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1926a1-6382-4b6b-8f10-9dfb4757b7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train[train[\"Image ID\"].isin(val[\"Image ID\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c10fc-2ef8-4303-b7f1-c1bca7c489f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sslDataset.createLabeledIndices(labelerIds=[4323195249, 4295232296], n_L=8, k=8, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b489801-d00c-49d1-af21-eccc5701afc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sslDataset.get_train_loader_interface(expert=exper(4323195249), batch_size=64, mu = 5, n_iters_per_epoch = 10, L=8, method='comatch', imsize=128, fold_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbe2664e-fa34-459a-8405-3a360215d70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class exper:\n",
    "    def __init__(self, id):\n",
    "        self.labeler_id = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4084b-5601-4a3c-83e9-ef229ff4f20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joli-env-debug",
   "language": "python",
   "name": "joli-env-debug"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
