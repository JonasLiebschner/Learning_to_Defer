{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f3cb62-2e9d-4c9c-9094-9e1b447e9e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import chain\n",
    "from typing import Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b56cb93-c248-43d6-982b-1b7bfa340a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = \"../Datasets/NIH/\"\n",
    "PATH_Labels = PATH\n",
    "PATH_Images = PATH\n",
    "\n",
    "maxLabels = 20\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT = 0.00\n",
    "NUM_HIDDEN_UNITS = 30\n",
    "LR = 5e-3\n",
    "USE_LR_SCHEDULER = False\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f8757ed-5371-4e97-8b65-c374e0204dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import shutil\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def downloadData(PATH):\n",
    "    links = [\n",
    "        'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
    "        'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
    "        'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
    "        'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
    "        'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
    "        'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
    "        'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
    "        'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
    "        'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
    "        'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
    "        'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
    "        'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
    "    ]\n",
    "    files = os.listdir(PATH + \"images/\")\n",
    "    for idx, link in enumerate(links):\n",
    "        fn = PATH + \"images/\" + 'images_%02d.tar.gz' % (idx+1)\n",
    "        if ('images_%02d.tar.gz' % (idx+1)) in files:\n",
    "            continue\n",
    "        print('downloading'+fn+'...')\n",
    "        urllib.request.urlretrieve(link, fn)  # download the zip file\n",
    "\n",
    "    print(\"Download complete. Please check the checksums\")\n",
    "    \n",
    "def unpackData(PATH):\n",
    "    Path = PATH + \"images/\"\n",
    "    files = os.listdir(Path)\n",
    "    for filename in files:\n",
    "        # open file\n",
    "        file = tarfile.open(Path + filename)\n",
    "        \n",
    "        # extracting file\n",
    "        file.extractall(Path + filename[:-7])\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "def moveData(PATH):\n",
    "    Path = PATH + \"images/\"\n",
    "    directories = os.listdir(Path)\n",
    "    for direc in directories:\n",
    "        if \"png\" in direc:\n",
    "            continue\n",
    "        filenames = os.listdir(Path + direc + \"/images/\")\n",
    "        for filename in filenames:\n",
    "            shutil.move(Path + direc + \"/images/\" + filename, Path + filename)\n",
    "        shutil.rmtree(Path + direc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a7ef35-63e0-4a34-b325-1a62e7ce2874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading../Datasets/NIH/images/images_12.tar.gz...\n",
      "Download complete. Please check the checksums\n"
     ]
    }
   ],
   "source": [
    "#downloadData(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d92e078-672f-4c4d-b68d-18534e3a9a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#unpackData(PATH)\n",
    "#moveData(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9361999c-ac1b-437e-8506-465f73626acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_test_Labels = PATH_Labels + \"test_labels.csv\"\n",
    "path_to_val_Labels = PATH_Labels + \"validation_labels.csv\"\n",
    "test_labels = pd.read_csv(path_to_test_Labels)\n",
    "val_labels = pd.read_csv(path_to_val_Labels)\n",
    "\n",
    "ground_truth_labels = pd.concat([test_labels,val_labels])\n",
    "ground_truth_labels[\"Fracture_Label\"] = ground_truth_labels[\"Fracture\"].map(dict(YES=1, NO=0))\n",
    "ground_truth_labels[\"Pneumothorax_Label\"] = ground_truth_labels[\"Pneumothorax\"].map(dict(YES=1, NO=0))\n",
    "ground_truth_labels[\"Airspace_Opacity_Label\"] = ground_truth_labels[\"Airspace opacity\"].map(dict(YES=1, NO=0))\n",
    "ground_truth_labels[\"Nodule_Or_Mass_Label\"] = ground_truth_labels[\"Nodule or mass\"].map(dict(YES=1, NO=0))\n",
    "\n",
    "path_to_individual_reader = PATH_Labels + \"individual_readers.csv\"\n",
    "individual_readers = pd.read_csv(path_to_individual_reader)\n",
    "\n",
    "individual_readers[\"Fracture_Expert_Label\"] = individual_readers[\"Fracture\"].map(dict(YES=1, NO=0))\n",
    "individual_readers[\"Pneumothorax_Expert_Label\"] = individual_readers[\"Pneumothorax\"].map(dict(YES=1, NO=0))\n",
    "individual_readers[\"Airspace_Opacity_Expert_Label\"] = individual_readers[\"Airspace opacity\"].map(dict(YES=1, NO=0))\n",
    "individual_readers[\"Nodule_Or_Mass_Expert_Label\"] = individual_readers[\"Nodule/mass\"].map(dict(YES=1, NO=0))\n",
    "\n",
    "individual_readers[\"Fracture_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Fracture_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "individual_readers[\"Pneumothorax_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Pneumothorax_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "individual_readers[\"Airspace_Opacity_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Airspace_Opacity_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "individual_readers[\"Nodule_Or_Mass_GT_Label\"] = individual_readers[\"Image ID\"].map(pd.Series(ground_truth_labels[\"Nodule_Or_Mass_Label\"].values,index=ground_truth_labels[\"Image Index\"]).to_dict())\n",
    "\n",
    "individual_readers[\"Fracture_Correct\"] = (individual_readers['Fracture_Expert_Label']==individual_readers['Fracture_GT_Label']).astype(int)\n",
    "individual_readers[\"Pneumothorax_Correct\"] = (individual_readers['Pneumothorax_Expert_Label']==individual_readers['Pneumothorax_GT_Label']).astype(int)\n",
    "individual_readers[\"Airspace_Opacity_Correct\"] = (individual_readers['Airspace_Opacity_Expert_Label']==individual_readers['Airspace_Opacity_GT_Label']).astype(int)\n",
    "individual_readers[\"Nodule_Or_Mass_Correct\"] = (individual_readers['Nodule_Or_Mass_Expert_Label']==individual_readers['Nodule_Or_Mass_GT_Label']).astype(int)\n",
    "\n",
    "individual_readers.to_csv(PATH_Labels + \"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4902ccf3-d85e-42f3-b692-16ec1b31db49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NIH_Dataset(Dataset):\n",
    "    def __init__(self, PATH, data: pd.DataFrame, preload=False, preprocess=False) -> None:\n",
    "        self.PATH = PATH\n",
    "        self.image_ids = data[\"Image ID\"].values\n",
    "        self.targets = data[\"GT_Label\"].values\n",
    "\n",
    "        self.tfms = transforms.Compose(\n",
    "            [\n",
    "              transforms.ToTensor(),\n",
    "              transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "        self.images = []\n",
    "        \n",
    "        self.preload = preload\n",
    "        self.preprocess = preprocess\n",
    "        \n",
    "        if self.preload:\n",
    "            self.loadImages()\n",
    "            \n",
    "    def loadImage(self, idx):\n",
    "        \"\"\"\n",
    "        Load one single image\n",
    "        \"\"\"\n",
    "        return Image.open(self.PATH + \"images/\" + self.image_ids[idx]).convert(\"RGB\").resize((244,244))\n",
    "            \n",
    "    def getImage(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the image from index idx\n",
    "        \"\"\"\n",
    "        if self.preload:\n",
    "            return self.images[idx]\n",
    "        else:\n",
    "            return self.loadImage(idx)\n",
    "\n",
    "    def loadImages(self):\n",
    "        \"\"\"\n",
    "        Load all images\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for idx in range(len(self.image_ids)):\n",
    "            if count % 1000 == 0:\n",
    "                print(\"Loaded \" + str(count) + \" images\")\n",
    "            count += 1\n",
    "            if self.preprocess:\n",
    "                self.images.append(self.transformImage(self.loadImage(idx)))\n",
    "            else:\n",
    "                self.images.append(self.loadImage(idx))\n",
    "        print(\"Loading complete\")\n",
    "        \n",
    "    def transformImage(self, image):\n",
    "        \"\"\"\n",
    "        Transforms the image\n",
    "        \"\"\"\n",
    "        return self.tfms(image).to(device)\n",
    "        \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        filename, target = self.image_ids[index], self.targets[index]\n",
    "        img = self.getImage(index)\n",
    "        if not self.preprocess:\n",
    "            img = self.transformImage(img)\n",
    "        return img, target, filename\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        #return len(self.images)\n",
    "        return len(self.image_ids)\n",
    "\n",
    "class NIH_K_Fold_Dataloader:\n",
    "    def __init__(self, k=10, labelerIds=[4323195249, 4295194124], target=\"Airspace_Opacity\", train_batch_size=8, test_batch_size=8,\n",
    "                 seed=42, fraction=1.0, maxLabels=800, PATH=\"\"):\n",
    "        self.k = k\n",
    "        self.labelerIds = labelerIds\n",
    "        self.target = target\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.seed = seed\n",
    "        self.k_fold_datasets = []\n",
    "        self.k_fold_patient_ids = []\n",
    "        self.PATH=PATH\n",
    "\n",
    "        individual_labels = pd.read_csv(PATH_Labels + \"labels.csv\")\n",
    "\n",
    "        # get image ids for those aimages labelled by both radiologists\n",
    "        common_image_ids = individual_labels[\"Image ID\"].values.tolist()\n",
    "        for labelerId in self.labelerIds:\n",
    "            expert_labels = individual_labels[individual_labels[\"Reader ID\"] == labelerId]\n",
    "            expert_image_ids = expert_labels[\"Image ID\"].values.tolist()\n",
    "            common_image_ids = np.intersect1d(common_image_ids, expert_image_ids)\n",
    "\n",
    "        # filter labels by common images ids\n",
    "        self.expert_labels = individual_labels[individual_labels[\"Image ID\"].isin(common_image_ids)][\n",
    "            [\"Reader ID\", \"Image ID\", self.target + \"_Expert_Label\", self.target + \"_GT_Label\", \"Patient ID\"]]\n",
    "\n",
    "        self.expert_labels.columns = [\"Reader ID\", \"Image ID\", \"Expert_Label\", \"GT_Label\", \"Patient ID\"]\n",
    "\n",
    "        # transform data for stratification. Calculate the performance of each radiologists for each patient\n",
    "        self.expert_labels[\"Expert_Correct\"] = self.expert_labels[\"Expert_Label\"] == self.expert_labels[\"GT_Label\"]\n",
    "\n",
    "        patient_ids = self.expert_labels[\"Patient ID\"].unique()\n",
    "        num_patient_images = self.expert_labels.drop_duplicates(subset=[\"Image ID\"]).groupby(by=\"Patient ID\", as_index=False).count()[\"Image ID\"]\n",
    "        self.patient_performance = pd.DataFrame({\"Patient ID\": patient_ids, \"Num Patient Images\": num_patient_images})\n",
    "\n",
    "        for labeler_id in self.labelerIds:\n",
    "            sum = self.expert_labels[self.expert_labels[\"Reader ID\"] == labeler_id][[\"Patient ID\", \"Expert_Correct\"]].groupby(by=\"Patient ID\",\n",
    "                                                                                                                       as_index=False).sum()\n",
    "            sum.columns = [\"Patient ID\", f'{labeler_id}_num_correct']\n",
    "            self.patient_performance = pd.merge(self.patient_performance, sum, left_on=\"Patient ID\", right_on=\"Patient ID\")\n",
    "            self.patient_performance[f'{labeler_id}_perf'] = self.patient_performance[f'{labeler_id}_num_correct'] / self.patient_performance['Num Patient Images']\n",
    "\n",
    "        # create target variable used for stratification. Target variable is the combination of radiologist#1 performance and radiologist#2 performance\n",
    "        self.patient_performance[\"target\"] = self.patient_performance[f'{self.labelerIds[0]}_perf'].astype(str) + \"_\" + self.patient_performance[\n",
    "            f'{self.labelerIds[1]}_perf'].astype(str)\n",
    "\n",
    "        self._init_k_folds(maxLabels=maxLabels)\n",
    "\n",
    "    def _init_k_folds(self, fraction=1.0, maxLabels=800):\n",
    "        self.labels = self.expert_labels.drop_duplicates(subset=[\"Image ID\"])\n",
    "        self.labels = self.labels.fillna(0)\n",
    "        self.labels = self.labels[[\"Patient ID\", \"Image ID\", \"GT_Label\"]]\n",
    "\n",
    "        kf_cv = StratifiedKFold(n_splits=self.k, shuffle=True, random_state=self.seed)\n",
    "\n",
    "        # _ sind train indizes, fold_test_idxs ist liste der test indizes\n",
    "        fold_data_idxs = [fold_test_idxs for (_, fold_test_idxs) in kf_cv.split(self.patient_performance[\"Patient ID\"].values, self.patient_performance[\"target\"].values)]\n",
    "\n",
    "        for fold_idx in range(len(fold_data_idxs)):\n",
    "            test_fold_idx = fold_idx # Nummer des Folds\n",
    "            test_fold_data_idxs = fold_data_idxs[test_fold_idx] # Array der Test Indizes\n",
    "\n",
    "            # use next 2 folds for validation set\n",
    "            val_folds_idxs = [(test_fold_idx + 1 + i) % 10 for i in range(2)]\n",
    "            val_fold_data_idxs = [fold_data_idxs[val_fold_idx] for val_fold_idx in val_folds_idxs]\n",
    "            val_fold_data_idxs = list(chain.from_iterable(val_fold_data_idxs))\n",
    "\n",
    "            # use next 7 folds for training set\n",
    "            train_folds_idxs = [(test_fold_idx + 3 + i) % 10 for i in range(7)]\n",
    "            train_folds_data_idxs = [fold_data_idxs[train_fold_idx] for train_fold_idx in train_folds_idxs]\n",
    "            train_folds_data_idxs = list(chain.from_iterable(train_folds_data_idxs))\n",
    "\n",
    "            \n",
    "            train_patient_ids = self.patient_performance[\"Patient ID\"].iloc[train_folds_data_idxs]\n",
    "            #train_patient_ids = self.patient_performance[\"Patient ID\"].iloc[train_folds_data_idxs].sample(n=min(maxLabels,len(train_folds_data_idxs)))\n",
    "            val_patient_ids = self.patient_performance[\"Patient ID\"].iloc[val_fold_data_idxs]\n",
    "            test_patient_ids = self.patient_performance[\"Patient ID\"].iloc[test_fold_data_idxs]\n",
    "\n",
    "            #expert_train = self.labels[self.labels[\"Patient ID\"].isin(train_patient_ids)]\n",
    "            expert_train = self.labels[self.labels[\"Patient ID\"].isin(train_patient_ids)]\n",
    "            expert_train = self.labels[self.labels[\"Patient ID\"].isin(train_patient_ids)].sample(n=min(maxLabels,len(expert_train)))\n",
    "            expert_val = self.labels[self.labels[\"Patient ID\"].isin(val_patient_ids)]\n",
    "            expert_test = self.labels[self.labels[\"Patient ID\"].isin(test_patient_ids)]\n",
    "\n",
    "            # check that patients are not shared across training, validation and test split\n",
    "            overlap = expert_train[expert_train[\"Patient ID\"].isin(expert_val[\"Patient ID\"])]\n",
    "            assert len(overlap) == 0, \"Train and Val Patient Ids overlap\"\n",
    "\n",
    "            overlap = expert_train[expert_train[\"Patient ID\"].isin(expert_test[\"Patient ID\"])]\n",
    "            assert len(overlap) == 0, \"Train and Test Patient Ids overlap\"\n",
    "\n",
    "            overlap = expert_val[expert_val[\"Patient ID\"].isin(expert_test[\"Patient ID\"])]\n",
    "            assert len(overlap) == 0, \"Val and Test Patient Ids overlap\"\n",
    "\n",
    "            expert_train = expert_train[[\"Image ID\", \"GT_Label\"]]\n",
    "            expert_val = expert_val[[\"Image ID\", \"GT_Label\"]]\n",
    "            expert_test = expert_test[[\"Image ID\", \"GT_Label\"]]\n",
    "\n",
    "            self.k_fold_datasets.append((expert_train, expert_val, expert_test))\n",
    "\n",
    "    def get_data_loader_for_fold(self, fold_idx):\n",
    "        expert_train, expert_val, expert_test = self.k_fold_datasets[fold_idx]\n",
    "\n",
    "        expert_train_dataset = NIH_Dataset(self.PATH, expert_train)\n",
    "        expert_val_dataset = NIH_Dataset(self.PATH, expert_val)\n",
    "        expert_test_dataset = NIH_Dataset(self.PATH, expert_test)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=expert_train_dataset, batch_size=self.train_batch_size, shuffle=True, drop_last=True)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=expert_val_dataset, batch_size=self.test_batch_size, shuffle=True, drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=expert_test_dataset, batch_size=self.test_batch_size, shuffle=True, drop_last=False)\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8e0eb7c-4f72-4cb5-be00-2b3853dbc1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=15.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loader = NIH_K_Fold_Dataloader(k=15,PATH=PATH, maxLabels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70fc8a40-bc8d-4017-82eb-2c141894002e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf_cv = StratifiedKFold(n_splits=loader.k, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a063e678-31c2-488c-a634-67a253154252",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=15.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fold_data_idxs = [fold_test_idxs for (_, fold_test_idxs) in kf_cv.split(loader.patient_performance[\"Patient ID\"].values, loader.patient_performance[\"target\"].values)]\n",
    "#fold_data_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7626b-d5dc-48ca-9496-06d498d84903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6002220d-4f35-409e-9254-658dc525831e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.k_fold_datasets[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a9775-9492-4d2d-858f-c51ae25a09d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f46c4d88-e860-4021-859c-1b9072102ce2",
   "metadata": {},
   "source": [
    "# Unver√§nderter Teil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b2fbc-7fb2-44a8-8ddd-74d929e0643b",
   "metadata": {
    "id": "b4qxVJZkjcsr"
   },
   "source": [
    "Functions for our loss and JSF loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ba4db3-18bd-446d-af8c-c5f3153524b7",
   "metadata": {
    "id": "-s72UAWpjcss",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def joint_sparse_framework_loss(epoch, classifier_output, allocation_system_output, expert_preds, targets):\n",
    "    # Input:\n",
    "    #   epoch: int = current epoch (used for epoch-dependent weighting of allocation system loss)\n",
    "    #   classifier_output: softmax probabilities as class probabilities,  nxm matrix with n=batch size, m=number of classes\n",
    "    #   allocation_system_output: sigmoid outputs as expert weights,  nx(m+1) matrix with n=batch size, m=number of experts + 1 for machine\n",
    "    #   expert_preds: nxm matrix with expert predictions with n=number of experts, m=number of classes\n",
    "    #   targets: targets as 1-dim vector with n length with n=batch_size\n",
    "\n",
    "    # loss for allocation system \n",
    "\n",
    "    # set up zero-initialized tensor to store weighted team predictions\n",
    "    batch_size = len(targets)\n",
    "    weighted_team_preds = torch.zeros((batch_size, NUM_CLASSES)).to(classifier_output.device)\n",
    "\n",
    "    # for each team member add the weighted prediction to the team prediction\n",
    "    # start with machine\n",
    "    weighted_team_preds = weighted_team_preds + allocation_system_output[:, 0].reshape(-1, 1) * classifier_output\n",
    "    # continue with human experts\n",
    "    for idx in range(NUM_EXPERTS):\n",
    "        one_hot_expert_preds = torch.tensor(np.eye(NUM_CLASSES)[expert_preds[idx].astype(int)]).to(classifier_output.device)\n",
    "        weighted_team_preds = weighted_team_preds + allocation_system_output[:, idx + 1].reshape(-1, 1) * one_hot_expert_preds\n",
    "\n",
    "    # calculate team probabilities using softmax\n",
    "    team_probs = nn.Softmax(dim=1)(weighted_team_preds)\n",
    "\n",
    "    # alpha2 is 1-epoch^0.5 (0.5 taken from code of preprint paper) <--- used for experiments\n",
    "    alpha2 = 1 - (epoch ** -0.5)\n",
    "    alpha2 = torch.tensor(alpha2).to(classifier_output.device)\n",
    "\n",
    "    # weight the negative log likelihood loss with alpha2 to get team loss\n",
    "    log_team_probs = torch.log(team_probs + 1e-7)\n",
    "    allocation_system_loss = nn.NLLLoss(reduction=\"none\")(log_team_probs, targets.long())\n",
    "    allocation_system_loss = torch.mean(alpha2 * allocation_system_loss)\n",
    "\n",
    "    # loss for classifier\n",
    "\n",
    "    alpha1 = 1\n",
    "    log_classifier_output = torch.log(classifier_output + 1e-7)\n",
    "    classifier_loss = nn.NLLLoss(reduction=\"none\")(log_classifier_output, targets.long())\n",
    "    classifier_loss = alpha1 * torch.mean(classifier_loss)\n",
    "\n",
    "    # combine both losses\n",
    "    system_loss = classifier_loss + allocation_system_loss\n",
    "\n",
    "    return system_loss\n",
    "\n",
    "def our_loss(epoch, classifier_output, allocation_system_output, expert_preds, targets):\n",
    "    # Input:\n",
    "    #   epoch: int = current epoch (not used, just to have the same function parameters as with JSF loss)\n",
    "    #   classifier_output: softmax probabilities as class probabilities,  nxm matrix with n=batch size, m=number of classes\n",
    "    #   allocation_system_output: softmax outputs as weights,  nx(m+1) matrix with n=batch size, m=number of experts + 1 for machine\n",
    "    #   expert_preds: nxm matrix with expert predictions with n=number of experts, m=number of classes\n",
    "    #   targets: targets as 1-dim vector with n length with n=batch_size\n",
    "\n",
    "    batch_size = len(targets)\n",
    "    team_probs = torch.zeros((batch_size, NUM_CLASSES)).to(classifier_output.device) # set up zero-initialized tensor to store team predictions\n",
    "    team_probs = team_probs + allocation_system_output[:, 0].reshape(-1, 1) * classifier_output # add the weighted classifier prediction to the team prediction\n",
    "    for idx in range(NUM_EXPERTS): # continue with human experts\n",
    "        one_hot_expert_preds = torch.tensor(np.eye(NUM_CLASSES)[expert_preds[idx].astype(int)]).to(classifier_output.device)\n",
    "        team_probs = team_probs + allocation_system_output[:, idx + 1].reshape(-1, 1) * one_hot_expert_preds\n",
    "\n",
    "    log_output = torch.log(team_probs + 1e-7)\n",
    "    system_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    return system_loss\n",
    "\n",
    "def mixture_of_ai_experts_loss(allocation_system_output, classifiers_outputs, targets):\n",
    "    batch_size = len(targets)\n",
    "    team_probs = torch.zeros((batch_size, NUM_CLASSES)).to(allocation_system_output.device)\n",
    "    classifiers_outputs = classifiers_outputs.to(allocation_system_output.device)\n",
    "\n",
    "    for idx in range(NUM_EXPERTS+1):\n",
    "        team_probs = team_probs + allocation_system_output[:, idx].reshape(-1, 1) * classifiers_outputs[idx]\n",
    "\n",
    "    log_output = torch.log(team_probs + 1e-7)\n",
    "    moae_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    return moae_loss\n",
    "\n",
    "def mixture_of_human_experts_loss(allocation_system_output, human_expert_preds, targets):\n",
    "    batch_size = len(targets)\n",
    "    team_probs = torch.zeros((batch_size, NUM_CLASSES)).to(allocation_system_output.device)\n",
    "\n",
    "    # human experts\n",
    "    for idx in range(NUM_EXPERTS):\n",
    "        one_hot_expert_preds = torch.tensor(np.eye(NUM_CLASSES)[human_expert_preds[idx].astype(int)]).to(allocation_system_output.device)\n",
    "        team_probs = team_probs + allocation_system_output[:, idx].reshape(-1, 1) * one_hot_expert_preds\n",
    "\n",
    "    log_output = torch.log(team_probs + 1e-7)\n",
    "    mohe_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    return mohe_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6583d-5134-4bca-9687-13dc1761d390",
   "metadata": {
    "id": "23EMRGUujcst"
   },
   "source": [
    "Class for classifier and allocation system network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07c403c-6729-488c-bc82-7251cb20af36",
   "metadata": {
    "id": "16aj3uShjcsu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Resnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        del self.resnet.fc\n",
    "\n",
    "        if \"resnet.pth\" in os.listdir():\n",
    "            print('load Resnet-18 checkpoint resnet.pth')\n",
    "            print(self.resnet.load_state_dict(\n",
    "                torch.load(\"resnet.pth\"),\n",
    "                strict=False))\n",
    "        else:\n",
    "            print('load Resnet-18 pretrained on ImageNet')\n",
    "\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.training = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        x = self.resnet.avgpool(x)\n",
    "        features = torch.flatten(x, 1)\n",
    "        return features\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, output_size, softmax_sigmoid=\"softmax\"):\n",
    "        super().__init__()\n",
    "        self.softmax_sigmoid = softmax_sigmoid\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(512, NUM_HIDDEN_UNITS),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_HIDDEN_UNITS, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        output = self.classifier(features)\n",
    "        if self.softmax_sigmoid == \"softmax\":\n",
    "            output = nn.Softmax(dim=1)(output)\n",
    "        elif self.softmax_sigmoid == \"sigmoid\":\n",
    "            output = nn.Sigmoid()(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56048446-1678-4bb1-b943-b8fd69410760",
   "metadata": {
    "id": "6YunYs7Rjcsu"
   },
   "source": [
    "Classes and Functions for Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3089a4-6d06-413a-8744-58a132c3a562",
   "metadata": {
    "id": "Eldab7TJjcsu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NihExpert:\n",
    "    \"\"\"A class used to represent an Expert on NIH ChestX-ray data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labeler_id : int\n",
    "        the Reader ID to specify which radiologist the expert object represents\n",
    "    target : str\n",
    "        the target to make predictions for\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    labeler_id : int\n",
    "        the Reader ID to specify which radiologist the expert object represents\n",
    "    target : str\n",
    "        the target to make predictions for\n",
    "    image_id_to_prediction : dict of {int : str}\n",
    "        a dictionary that maps the image id to the prediction the radiologist made for the specified target\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    predict(image_ids)\n",
    "        makes a prediction for the given image ids\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labeler_id: int, target: str):\n",
    "        self.labelerId = labeler_id\n",
    "        self.target = target\n",
    "\n",
    "        individual_labels = pd.read_csv(\"labels.csv\")\n",
    "\n",
    "        expert_labels = individual_labels[individual_labels[\"Reader ID\"] == self.labelerId][\n",
    "            [\"Image ID\", self.target + \"_Expert_Label\", self.target + \"_GT_Label\"]]\n",
    "        expert_labels = expert_labels.fillna(0)\n",
    "\n",
    "        self.image_id_to_prediction = pd.Series(expert_labels[self.target + \"_Expert_Label\"].values,\n",
    "                                                index=expert_labels[\"Image ID\"]).to_dict()\n",
    "\n",
    "    def predict(self, image_ids):\n",
    "        \"\"\"Returns the experts predictions for the given image ids. Works only for image ids that are labeled by the expert\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_ids : list of int\n",
    "            the image ids to get the radiologists predictions for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of int\n",
    "            returns a list of 0 or 1 that represent the radiologists prediction for the specified target\n",
    "        \"\"\"\n",
    "        return [self.image_id_to_prediction[image_id] for image_id in image_ids]\n",
    "\n",
    "    def predict_unlabeled_data(self, image_ids):\n",
    "        \"\"\"Returns the experts predictions for the given image ids. Works for all image ids, returns -1 if not labeled by expert\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_ids : list of int\n",
    "            the image ids to get the radiologists predictions for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of int\n",
    "            returns a list of 0 or 1 that represent the radiologists prediction for the specified target, or -1 if no prediction\n",
    "        \"\"\"\n",
    "        return [self.image_id_to_prediction[image_id] if image_id in self.image_id_to_prediction else -1 for image_id in image_ids]\n",
    "\n",
    "class NihAverageExpert:\n",
    "    def __init__(self, expert_fns=[]):\n",
    "        self.expert_fns = expert_fns\n",
    "        self.num_experts = len(self.expert_fns)\n",
    "\n",
    "    def predict(self, filenames):\n",
    "        all_experts_predictions = [expert_fn(filenames) for expert_fn in self.expert_fns]\n",
    "        predictions = [None] * len(filenames)\n",
    "\n",
    "        for idx, expert_predictions in enumerate(all_experts_predictions):\n",
    "            predictions[idx::self.num_experts] = expert_predictions[idx::self.num_experts]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3a4d5-8f5a-4dd8-9289-d72d1d19deb9",
   "metadata": {
    "id": "_4Dc3laXjcsv"
   },
   "source": [
    "Functions for Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84112bd1-0458-4ca4-ae43-cf589b4128b5",
   "metadata": {
    "id": "3dVHQL-Wjcsv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(preds, targets):\n",
    "    if len(targets) > 0:\n",
    "        acc = accuracy_score(targets, preds)\n",
    "    else:\n",
    "        acc = 0\n",
    "\n",
    "    return acc\n",
    "\n",
    "def get_coverage(task_subset_targets, targets):\n",
    "    num_images = len(targets)\n",
    "    num_images_in_task_subset = len(task_subset_targets)\n",
    "    coverage = num_images_in_task_subset / num_images\n",
    "\n",
    "    return coverage\n",
    "\n",
    "def get_classifier_metrics(classifier_preds, allocation_system_decisions, targets):\n",
    "    # classifier performance on all tasks\n",
    "    classifier_accuracy = get_accuracy(classifier_preds, targets)\n",
    "\n",
    "    # filter for subset of tasks that are allocated to the classifier\n",
    "    task_subset = (allocation_system_decisions == 0)\n",
    "\n",
    "    # classifier performance on those tasks\n",
    "    task_subset_classifier_preds = classifier_preds[task_subset]\n",
    "    task_subset_targets = targets[task_subset]\n",
    "    classifier_task_subset_accuracy = get_accuracy(task_subset_classifier_preds, task_subset_targets)\n",
    "\n",
    "    # coverage\n",
    "    classifier_coverage = get_coverage(task_subset_targets, targets)\n",
    "\n",
    "    return classifier_accuracy, classifier_task_subset_accuracy, classifier_coverage\n",
    "\n",
    "def get_experts_metrics(expert_preds, allocation_system_decisions, targets):\n",
    "    expert_accuracies = []\n",
    "    expert_task_subset_accuracies = []\n",
    "    expert_coverages = []\n",
    "\n",
    "    # calculate metrics for each expert\n",
    "    for expert_idx in range(NUM_EXPERTS):\n",
    "\n",
    "        # expert performance on all tasks\n",
    "        preds = expert_preds[expert_idx]\n",
    "        expert_accuracy = get_accuracy(preds, targets)\n",
    "\n",
    "        # filter for subset of tasks that are allocated to the expert with number \"idx\"\n",
    "        task_subset = (allocation_system_decisions == expert_idx+1)\n",
    "\n",
    "        # expert performance on tasks assigned by allocation system\n",
    "        task_subset_expert_preds = preds[task_subset]\n",
    "        task_subset_targets = targets[task_subset]\n",
    "        expert_task_subset_accuracy = get_accuracy(task_subset_expert_preds, task_subset_targets)\n",
    "\n",
    "        # coverage\n",
    "        expert_coverage = get_coverage(task_subset_targets, targets)\n",
    "\n",
    "        expert_accuracies.append(expert_accuracy)\n",
    "        expert_task_subset_accuracies.append(expert_task_subset_accuracy)\n",
    "        expert_coverages.append(expert_coverage)\n",
    "\n",
    "    return expert_accuracies, expert_task_subset_accuracies, expert_coverages\n",
    "\n",
    "def get_metrics(epoch, allocation_system_outputs, classifier_outputs, expert_preds, targets, loss_fn):\n",
    "    metrics = {}\n",
    "\n",
    "    # Metrics for system\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    classifier_preds = np.argmax(classifier_outputs, 1)\n",
    "    preds = np.vstack((classifier_preds, expert_preds)).T\n",
    "    system_preds = preds[range(len(preds)), allocation_system_decisions.astype(int)]\n",
    "    system_accuracy = get_accuracy(system_preds, targets)\n",
    "\n",
    "    system_loss = loss_fn(epoch=epoch,\n",
    "                          classifier_output=torch.tensor(classifier_outputs).float(),\n",
    "                          allocation_system_output=torch.tensor(allocation_system_outputs).float(),\n",
    "                          expert_preds=expert_preds,\n",
    "                          targets=torch.tensor(targets).long())\n",
    "\n",
    "    metrics[\"System Accuracy\"] = system_accuracy\n",
    "    metrics[\"System Loss\"] = system_loss\n",
    "\n",
    "    # Metrics for classifier\n",
    "    classifier_accuracy, classifier_task_subset_accuracy, classifier_coverage = get_classifier_metrics(classifier_preds, allocation_system_decisions, targets)\n",
    "    metrics[\"Classifier Accuracy\"] = classifier_accuracy\n",
    "    metrics[\"Classifier Task Subset Accuracy\"] = classifier_task_subset_accuracy\n",
    "    metrics[\"Classifier Coverage\"] = classifier_coverage\n",
    "\n",
    "    # Metrics for experts \n",
    "    \"\"\"expert_accuracies, experts_task_subset_accuracies, experts_coverages = get_experts_metrics(expert_preds, allocation_system_decisions, targets)\n",
    "\n",
    "    for expert_idx, (expert_accuracy, expert_task_subset_accuracy, expert_coverage) in enumerate(zip(expert_accuracies, experts_task_subset_accuracies, experts_coverages)):\n",
    "        metrics[f'Expert {expert_idx+1} Accuracy'] = expert_accuracy\n",
    "        metrics[f'Expert {expert_idx+1} Task Subset Accuracy'] = expert_task_subset_accuracy\n",
    "        metrics[f'Expert {expert_idx+1} Coverage'] = expert_coverage\"\"\"\n",
    "\n",
    "    return system_accuracy, system_loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3df1e-96ff-4d08-9a85-cb82a0ccb131",
   "metadata": {
    "id": "YF7ldvYMjcsw"
   },
   "source": [
    "Functions for Training and Evaluation of Our Approach and JSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d367fa4d-8442-4c29-9b1e-63777e256a16",
   "metadata": {
    "id": "wVr4UTxbjcsw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, feature_extractor, classifier, allocation_system, train_loader, optimizer, scheduler, expert_fns, loss_fn):\n",
    "    feature_extractor.eval()\n",
    "    classifier.train()\n",
    "    allocation_system.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, batch_filenames) in enumerate(train_loader):\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        expert_batch_preds = np.empty((NUM_EXPERTS, len(batch_targets)))\n",
    "        for idx, expert_fn in enumerate(expert_fns):\n",
    "            expert_batch_preds[idx] = np.array(expert_fn(batch_filenames))\n",
    "\n",
    "        batch_features = feature_extractor(batch_input)\n",
    "        batch_outputs_classifier = classifier(batch_features)\n",
    "        batch_outputs_allocation_system = allocation_system(batch_features)\n",
    "\n",
    "        batch_loss = loss_fn(epoch=epoch, classifier_output=batch_outputs_classifier, allocation_system_output=batch_outputs_allocation_system,\n",
    "                                expert_preds=expert_batch_preds, targets=batch_targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate_one_epoch(epoch, feature_extractor, classifier, allocation_system, data_loader, expert_fns, loss_fn):\n",
    "    feature_extractor.eval()\n",
    "    classifier.eval()\n",
    "    allocation_system.eval()\n",
    "\n",
    "    classifier_outputs = torch.tensor([]).to(device)\n",
    "    allocation_system_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).to(device)\n",
    "    filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_filenames) in enumerate(data_loader):\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            batch_features = feature_extractor(batch_input)\n",
    "            batch_classifier_outputs = classifier(batch_features)\n",
    "            batch_allocation_system_outputs = allocation_system(batch_features)\n",
    "\n",
    "            classifier_outputs = torch.cat((classifier_outputs, batch_classifier_outputs))\n",
    "            allocation_system_outputs = torch.cat((allocation_system_outputs, batch_allocation_system_outputs))\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "            filenames.extend(batch_filenames)\n",
    "\n",
    "    expert_preds = np.empty((NUM_EXPERTS, len(targets)))\n",
    "    for idx, expert_fn in enumerate(expert_fns):\n",
    "        expert_preds[idx] = np.array(expert_fn(filenames))\n",
    "\n",
    "    classifier_outputs = classifier_outputs.cpu().numpy()\n",
    "    allocation_system_outputs = allocation_system_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    classifier_preds = np.argmax(classifier_outputs, 1)\n",
    "    preds = np.vstack((classifier_preds, expert_preds)).T\n",
    "    system_preds = preds[range(len(preds)), allocation_system_decisions.astype(int)]\n",
    "\n",
    "    system_accuracy, system_loss, metrics = get_metrics(epoch, allocation_system_outputs, classifier_outputs, expert_preds, targets, loss_fn)\n",
    "\n",
    "    return system_accuracy, system_loss, system_preds, allocation_system_decisions, targets\n",
    "\n",
    "def run_team_performance_optimization(method, seed, expert_fns, PATH, maxLabels=800):\n",
    "    print(f'Team Performance Optimization with {method}')\n",
    "\n",
    "    if method == \"Joint Sparse Framework\":\n",
    "        loss_fn = joint_sparse_framework_loss\n",
    "        allocation_system_activation_function = \"sigmoid\"\n",
    "\n",
    "\n",
    "    elif method == \"Our Approach\":\n",
    "        loss_fn = our_loss\n",
    "        allocation_system_activation_function = \"softmax\"\n",
    "\n",
    "    feature_extractor = Resnet().to(device)\n",
    "\n",
    "    nih_dataloader = NIH_K_Fold_Dataloader(\n",
    "        K,\n",
    "        LABELER_IDS,\n",
    "        TARGET,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        TEST_BATCH_SIZE,\n",
    "        seed,\n",
    "        maxLabels=maxLabels,\n",
    "        PATH=PATH\n",
    "    )\n",
    "\n",
    "    overall_allocation_system_decisions = []\n",
    "    overall_system_preds = []\n",
    "    overall_targets = []\n",
    "\n",
    "    for fold_idx in range(K):\n",
    "        print(f'Running fold {fold_idx+1} out of {K}')\n",
    "\n",
    "        classifier = Network(output_size=NUM_CLASSES,\n",
    "                            softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "        allocation_system = Network(output_size=NUM_EXPERTS + 1,\n",
    "                                 softmax_sigmoid=allocation_system_activation_function).to(device)\n",
    "\n",
    "        train_loader, val_loader, test_loader = nih_dataloader.get_data_loader_for_fold(fold_idx)\n",
    "\n",
    "        parameters = list(classifier.parameters()) + list(allocation_system.parameters())\n",
    "        optimizer = torch.optim.Adam(parameters, lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "        best_val_system_accuracy = 0\n",
    "        best_val_system_loss = 100\n",
    "        best_metrics = None\n",
    "\n",
    "        for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "            train_one_epoch(epoch, feature_extractor, classifier, allocation_system, train_loader, optimizer, scheduler, expert_fns, loss_fn)\n",
    "\n",
    "            val_system_accuracy, val_system_loss, _, _, _ = evaluate_one_epoch(epoch, feature_extractor, classifier, allocation_system, val_loader, expert_fns, loss_fn)\n",
    "            _, _, test_system_preds, test_allocation_system_decisions, test_targets = evaluate_one_epoch(epoch, feature_extractor, classifier, allocation_system, test_loader, expert_fns, loss_fn)\n",
    "\n",
    "            if method == \"Joint Sparse Framework\":\n",
    "                if val_system_accuracy > best_val_system_accuracy:\n",
    "                    best_val_system_accuracy = val_system_accuracy\n",
    "                    best_epoch_system_preds = test_system_preds\n",
    "                    best_epoch_allocation_system_decisions = test_allocation_system_decisions\n",
    "                    best_epoch_targets = test_targets\n",
    "\n",
    "            elif method == \"Our Approach\":\n",
    "                if val_system_loss < best_val_system_loss:\n",
    "                    best_val_system_loss = val_system_loss\n",
    "                    best_epoch_system_preds = test_system_preds\n",
    "                    best_epoch_allocation_system_decisions = test_allocation_system_decisions\n",
    "                    best_epoch_targets = test_targets\n",
    "\n",
    "        overall_system_preds.extend(list(best_epoch_system_preds))\n",
    "        overall_allocation_system_decisions.extend(list(best_epoch_allocation_system_decisions))\n",
    "        overall_targets.extend(list(best_epoch_targets))\n",
    "\n",
    "    system_accuracy = get_accuracy(overall_system_preds, overall_targets)\n",
    "    classifier_coverage = np.sum([1 for dec in overall_allocation_system_decisions if dec==0])\n",
    "    \n",
    "    return system_accuracy, classifier_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cec79b-148f-45f4-85b6-5542b6f081fb",
   "metadata": {
    "id": "iHkf2wxgjcsw"
   },
   "source": [
    "Functions for Evaluation of Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d431b9-ac6e-472a-b4dc-9e1a8093d83f",
   "metadata": {
    "id": "FHK8Yk48jcsx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy_of_best_expert(seed, expert_fns, PATH, maxLabels=800):\n",
    "    nih_dataloader = NIH_K_Fold_Dataloader(\n",
    "        K,\n",
    "        LABELER_IDS,\n",
    "        TARGET,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        TEST_BATCH_SIZE,\n",
    "        seed,\n",
    "        maxLabels=maxLabels,\n",
    "        PATH=PATH\n",
    "    )\n",
    "\n",
    "    targets = []\n",
    "    filenames = []\n",
    "    \n",
    "    for fold_idx in range(K):\n",
    "        print(f'Running fold {fold_idx+1} out of {K}')\n",
    "        _, _, test_loader = nih_dataloader.get_data_loader_for_fold(fold_idx)\n",
    "  \n",
    "        with torch.no_grad():\n",
    "            for i, (_, batch_targets, batch_filenames) in enumerate(test_loader):\n",
    "                targets.extend(list(batch_targets.numpy()))\n",
    "                filenames.extend(batch_filenames)\n",
    "\n",
    "    expert_preds = np.empty((NUM_EXPERTS, len(targets)))\n",
    "    for idx, expert_fn in enumerate(expert_fns):\n",
    "        expert_preds[idx] = np.array(expert_fn(filenames))\n",
    "\n",
    "    expert_accuracies = []\n",
    "    for idx in range(NUM_EXPERTS):\n",
    "        preds = expert_preds[idx]\n",
    "        acc = accuracy_score(targets, preds)\n",
    "        expert_accuracies.append(acc)\n",
    "\n",
    "    print(f'Best Expert Accuracy: {max(expert_accuracies)}\\n')\n",
    "\n",
    "    return max(expert_accuracies)\n",
    "\n",
    "def get_accuracy_of_average_expert(seed, expert_fns, PATH, maxLabels=800):\n",
    "    nih_dataloader = NIH_K_Fold_Dataloader(\n",
    "        K,\n",
    "        LABELER_IDS,\n",
    "        TARGET,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        TEST_BATCH_SIZE,\n",
    "        seed,\n",
    "        maxLabels=maxLabels,\n",
    "        PATH=PATH\n",
    "    )\n",
    "\n",
    "    targets = []\n",
    "    filenames = []\n",
    "\n",
    "    for fold_idx in range(K):\n",
    "        print(f'Running fold {fold_idx+1} out of {K}')\n",
    "        _, _, test_loader = nih_dataloader.get_data_loader_for_fold(fold_idx)\n",
    "  \n",
    "        with torch.no_grad():\n",
    "            for i, (_, batch_targets, batch_filenames) in enumerate(test_loader):\n",
    "                targets.extend(list(batch_targets.numpy()))\n",
    "                filenames.extend(batch_filenames)\n",
    "\n",
    "\n",
    "    avg_expert = NihAverageExpert(expert_fns)\n",
    "    avg_expert_preds = avg_expert.predict(filenames)\n",
    "    avg_expert_acc = accuracy_score(targets, avg_expert_preds)\n",
    "    print(f'Average Expert Accuracy: {avg_expert_acc}\\n')\n",
    "\n",
    "    return avg_expert_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c334a643-40ee-41a3-bbbb-3e48921931f3",
   "metadata": {
    "id": "reZuentAjcsx"
   },
   "source": [
    "Functions for Training and Evaluation of Full Automation Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd3b1db4-7c40-4f6e-b660-6cebaf7f807c",
   "metadata": {
    "id": "6eXEmsK1jcsx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_full_automation_one_epoch(epoch, feature_extractor, classifier, train_loader, optimizer, scheduler):\n",
    "    # switch to train mode\n",
    "    feature_extractor.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, _) in enumerate(train_loader):\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        batch_features = feature_extractor(batch_input)\n",
    "        batch_outputs_classifier = classifier(batch_features)\n",
    "\n",
    "        log_output = torch.log(batch_outputs_classifier + 1e-7)\n",
    "        batch_loss = nn.NLLLoss()(log_output, batch_targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate_full_automation_one_epoch(epoch, feature_extractor, classifier, data_loader):\n",
    "    feature_extractor.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    classifier_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, _) in enumerate(data_loader):\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            batch_features = feature_extractor(batch_input)\n",
    "            batch_classifier_outputs = classifier(batch_features)\n",
    "\n",
    "            classifier_outputs = torch.cat((classifier_outputs, batch_classifier_outputs))\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "\n",
    "    log_output = torch.log(classifier_outputs + 1e-7)\n",
    "    full_automation_loss = nn.NLLLoss()(log_output, targets.long())\n",
    "\n",
    "    classifier_outputs = classifier_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    classifier_preds = np.argmax(classifier_outputs, 1)\n",
    "\n",
    "    return full_automation_loss, classifier_preds, targets\n",
    "\n",
    "def run_full_automation(seed, PATH, maxLabels=800):\n",
    "    print(f'Training full automation baseline')\n",
    "\n",
    "    feature_extractor = Resnet().to(device)\n",
    "\n",
    "    nih_dataloader = NIH_K_Fold_Dataloader(\n",
    "        K,\n",
    "        LABELER_IDS,\n",
    "        TARGET,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        TEST_BATCH_SIZE,\n",
    "        seed,\n",
    "        maxLabels=maxLabels,\n",
    "        PATH=PATH\n",
    "    )\n",
    "\n",
    "    overall_classifier_preds = []\n",
    "    overall_targets = []\n",
    "\n",
    "\n",
    "    for fold_idx in range(K):\n",
    "        print(f'Running fold {fold_idx+1} out of {K}')\n",
    "\n",
    "        classifier = Network(output_size=NUM_CLASSES,\n",
    "                            softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "        train_loader, val_loader, test_loader = nih_dataloader.get_data_loader_for_fold(fold_idx)\n",
    "\n",
    "        parameters = list(classifier.parameters())\n",
    "        optimizer = torch.optim.Adam(parameters, lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "        best_val_system_loss = 100\n",
    "\n",
    "        for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "            train_full_automation_one_epoch(epoch, feature_extractor, classifier, train_loader, optimizer, scheduler)\n",
    "\n",
    "            val_system_loss, _, _ = evaluate_full_automation_one_epoch(epoch, feature_extractor, classifier, val_loader)\n",
    "            _, test_classifier_preds, test_targets = evaluate_full_automation_one_epoch(epoch, feature_extractor, classifier, test_loader)\n",
    "\n",
    "            if val_system_loss < best_val_system_loss:\n",
    "                best_val_system_loss = val_system_loss\n",
    "                best_epoch_classifier_preds = test_classifier_preds\n",
    "                best_epoch_targets = test_targets\n",
    "\n",
    "        overall_classifier_preds.extend(list(best_epoch_classifier_preds))\n",
    "        overall_targets.extend(list(best_epoch_targets))\n",
    "\n",
    "    classifier_accuracy = get_accuracy(overall_classifier_preds, overall_targets)\n",
    "    \n",
    "    return classifier_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e4671-d46d-48b0-ad0f-5b76276fb471",
   "metadata": {
    "id": "JBlyH97CWNR6"
   },
   "source": [
    "Functions for Training and Evaluation of Mixture of Artificial Experts Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3b12346-d1b7-488d-aea6-838b29350015",
   "metadata": {
    "id": "cqHkSgpRWNR6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_moae_one_epoch(feature_extractor, classifiers, allocation_system, train_loader, optimizer, scheduler):\n",
    "    # switch to train mode\n",
    "    feature_extractor.eval()\n",
    "    allocation_system.train()\n",
    "    for classifier in classifiers:\n",
    "        classifier.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, batch_filenames) in enumerate(train_loader):\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        batch_features = feature_extractor(batch_input)\n",
    "        batch_outputs_allocation_system = allocation_system(batch_features)\n",
    "        batch_outputs_classifiers = torch.empty((NUM_EXPERTS+1, len(batch_targets), NUM_CLASSES))\n",
    "        for idx, classifier in enumerate(classifiers):\n",
    "            batch_outputs_classifiers[idx] = classifier(batch_features)\n",
    "\n",
    "        # compute and record loss\n",
    "        batch_loss = mixture_of_ai_experts_loss(allocation_system_output=batch_outputs_allocation_system,\n",
    "                                                   classifiers_outputs=batch_outputs_classifiers, targets=batch_targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate_moae_one_epoch(feature_extractor, classifiers, allocation_system, data_loader):\n",
    "    feature_extractor.eval()\n",
    "    allocation_system.eval()\n",
    "    for classifier in classifiers:\n",
    "        classifier.eval()\n",
    "\n",
    "    classifiers_outputs = torch.tensor([]).to(device)\n",
    "    allocation_system_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).long().to(device)\n",
    "    filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_filenames) in enumerate(data_loader):\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            batch_features = feature_extractor(batch_input)\n",
    "            batch_allocation_system_outputs = allocation_system(batch_features)\n",
    "            batch_outputs_classifiers = torch.empty((NUM_EXPERTS+1, len(batch_targets), NUM_CLASSES)).to(device)\n",
    "            for idx, classifier in enumerate(classifiers):\n",
    "                batch_outputs_classifiers[idx] = classifier(batch_features)\n",
    "\n",
    "            classifiers_outputs = torch.cat((classifiers_outputs, batch_outputs_classifiers), dim=1)\n",
    "            allocation_system_outputs = torch.cat((allocation_system_outputs, batch_allocation_system_outputs))\n",
    "            targets = torch.cat((targets, batch_targets.float()))\n",
    "            filenames.extend(batch_filenames)\n",
    "\n",
    "    moae_loss = mixture_of_ai_experts_loss(allocation_system_output=allocation_system_outputs,\n",
    "                                                   classifiers_outputs=classifiers_outputs, targets=targets.long())\n",
    "\n",
    "    classifiers_outputs = classifiers_outputs.cpu().numpy()\n",
    "    allocation_system_outputs = allocation_system_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    classifiers_preds = np.argmax(classifiers_outputs, 2).T\n",
    "    team_preds = classifiers_preds[range(len(classifiers_preds)), allocation_system_decisions.astype(int)]\n",
    "\n",
    "    return moae_loss, team_preds, targets\n",
    "\n",
    "def run_moae(seed, PATH, maxLabels=800):\n",
    "    print(f'Training Mixture of artificial experts baseline')\n",
    "\n",
    "    feature_extractor = Resnet().to(device)\n",
    "\n",
    "    nih_dataloader = NIH_K_Fold_Dataloader(\n",
    "        K,\n",
    "        LABELER_IDS,\n",
    "        TARGET,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        TEST_BATCH_SIZE,\n",
    "        seed,\n",
    "        maxLabels=maxLabels,\n",
    "        PATH=PATH\n",
    "    )\n",
    "\n",
    "    overall_system_preds = []\n",
    "    overall_targets = []\n",
    "\n",
    "    for fold_idx in range(K):\n",
    "        print(f'Running fold {fold_idx+1} out of {K}')\n",
    "\n",
    "        allocation_system = Network(output_size=NUM_EXPERTS + 1,\n",
    "                                 softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "        classifiers = []\n",
    "        for _ in range(NUM_EXPERTS+1):\n",
    "            classifier = Network(output_size=NUM_CLASSES,\n",
    "                                softmax_sigmoid=\"softmax\").to(device)\n",
    "            classifiers.append(classifier)\n",
    "\n",
    "        train_loader, val_loader, test_loader = nih_dataloader.get_data_loader_for_fold(fold_idx)\n",
    "\n",
    "        parameters = list(allocation_system.parameters())\n",
    "        for classifier in classifiers:\n",
    "            parameters += list(classifier.parameters())\n",
    "\n",
    "        optimizer = torch.optim.Adam(parameters, lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "        best_val_system_loss = 100\n",
    "\n",
    "        for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "            train_moae_one_epoch(feature_extractor, classifiers, allocation_system, train_loader, optimizer, scheduler)\n",
    "\n",
    "            val_system_loss, _, _ = evaluate_moae_one_epoch(feature_extractor, classifiers, allocation_system, val_loader)\n",
    "            _, test_system_preds, test_targets = evaluate_moae_one_epoch(feature_extractor, classifiers, allocation_system, test_loader)\n",
    "\n",
    "            if val_system_loss < best_val_system_loss:\n",
    "                best_val_system_loss = val_system_loss\n",
    "                best_epoch_system_preds = test_system_preds\n",
    "                best_epoch_targets = test_targets\n",
    "\n",
    "        overall_system_preds.extend(list(best_epoch_system_preds))\n",
    "        overall_targets.extend(list(best_epoch_targets))\n",
    "\n",
    "    system_accuracy = get_accuracy(overall_system_preds, overall_targets)\n",
    "\n",
    "    print(f'Mixture of Artificial Experts Accuracy: {system_accuracy}\\n')\n",
    "    return system_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d01507d-8e6c-4685-a10d-db328513bb4c",
   "metadata": {
    "id": "XB7WcEnt3EYj"
   },
   "source": [
    "Functions for Training and Evaluation of Mixture of Human Experts Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4af08743-cbe2-47b3-8008-fae721f82e4a",
   "metadata": {
    "id": "zwIcx_9v3EYl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_mohe_one_epoch(feature_extractor, allocation_system, train_loader, optimizer, scheduler, expert_fns):\n",
    "    # switch to train mode\n",
    "    feature_extractor.eval()\n",
    "    allocation_system.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, batch_filenames) in enumerate(train_loader):\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        expert_batch_preds = np.empty((NUM_EXPERTS, len(batch_targets)))\n",
    "        for idx, expert_fn in enumerate(expert_fns):\n",
    "            expert_batch_preds[idx] = np.array(expert_fn(batch_filenames))\n",
    "\n",
    "        batch_features = feature_extractor(batch_input)\n",
    "        batch_outputs_allocation_system = allocation_system(batch_features)\n",
    "\n",
    "        # compute and record loss\n",
    "        batch_loss = mixture_of_human_experts_loss(allocation_system_output=batch_outputs_allocation_system,\n",
    "                                                   human_expert_preds=expert_batch_preds, targets=batch_targets)\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate_mohe_one_epoch(feature_extractor, allocation_system, data_loader, expert_fns):\n",
    "    feature_extractor.eval()\n",
    "    allocation_system.eval()\n",
    "\n",
    "    allocation_system_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).long().to(device)\n",
    "    filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_filenames) in enumerate(data_loader):\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            batch_features = feature_extractor(batch_input)\n",
    "            batch_allocation_system_outputs = allocation_system(batch_features)\n",
    "\n",
    "            allocation_system_outputs = torch.cat((allocation_system_outputs, batch_allocation_system_outputs))\n",
    "            targets = torch.cat((targets, batch_targets.float()))\n",
    "            filenames.extend(batch_filenames)\n",
    "\n",
    "    expert_preds = np.empty((NUM_EXPERTS, len(targets)))\n",
    "    for idx, expert_fn in enumerate(expert_fns):\n",
    "        expert_preds[idx] = np.array(expert_fn(filenames))\n",
    "\n",
    "    mohe_loss = mixture_of_human_experts_loss(allocation_system_output=allocation_system_outputs,\n",
    "                                                   human_expert_preds=expert_preds, targets=targets.long())\n",
    "\n",
    "    allocation_system_outputs = allocation_system_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    expert_preds = expert_preds.T\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    team_preds = expert_preds[range(len(expert_preds)), allocation_system_decisions.astype(int)-1]\n",
    "    \n",
    "    return mohe_loss, team_preds, targets\n",
    "\n",
    "def run_mohe(seed, expert_fns, PATH, maxLabels=800):\n",
    "    print(f'Training Mixture of human experts baseline')\n",
    "\n",
    "    feature_extractor = Resnet().to(device)\n",
    "\n",
    "    nih_dataloader = NIH_K_Fold_Dataloader(\n",
    "        K,\n",
    "        LABELER_IDS,\n",
    "        TARGET,\n",
    "        TRAIN_BATCH_SIZE,\n",
    "        TEST_BATCH_SIZE,\n",
    "        seed,\n",
    "        maxLabels=maxLabels,\n",
    "        PATH=PATH\n",
    "    )\n",
    "\n",
    "    overall_system_preds = []\n",
    "    overall_targets = []\n",
    "\n",
    "    for fold_idx in range(K):\n",
    "        print(f'Running fold {fold_idx+1} out of {K}')\n",
    "\n",
    "        allocation_system = Network(output_size=NUM_EXPERTS + 1,\n",
    "                                 softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "        train_loader, val_loader, test_loader = nih_dataloader.get_data_loader_for_fold(fold_idx)\n",
    "\n",
    "        optimizer = torch.optim.Adam(allocation_system.parameters(), lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "        best_val_system_loss = 100\n",
    "\n",
    "        for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "            train_mohe_one_epoch(feature_extractor, allocation_system, train_loader, optimizer, scheduler, expert_fns)\n",
    "\n",
    "            val_system_loss, _, _ = evaluate_mohe_one_epoch(feature_extractor, allocation_system, val_loader, expert_fns)\n",
    "            _, test_system_preds, test_targets = evaluate_mohe_one_epoch(feature_extractor, allocation_system, test_loader, expert_fns)\n",
    "\n",
    "            if val_system_loss < best_val_system_loss:\n",
    "                best_val_system_loss = val_system_loss\n",
    "                best_epoch_system_preds = test_system_preds\n",
    "                best_epoch_targets = test_targets\n",
    "\n",
    "        overall_system_preds.extend(list(best_epoch_system_preds))\n",
    "        overall_targets.extend(list(best_epoch_targets))\n",
    "\n",
    "    system_accuracy = get_accuracy(overall_system_preds, overall_targets)\n",
    "\n",
    "    print(f'Mixture of Human Experts Accuracy: {system_accuracy}\\n')\n",
    "    return system_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f7691-d816-4ac4-ad0d-3b19503c5e1e",
   "metadata": {
    "id": "kX69BkD8jcsy"
   },
   "source": [
    "#Run Experiment with four different radiologist pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42410227-13c8-4582-9a33-76a86cd5a3b4",
   "metadata": {
    "id": "qSE8TX0BF-zc"
   },
   "outputs": [],
   "source": [
    "NUM_EXPERTS=2\n",
    "K=10\n",
    "TARGET=\"Airspace_Opacity\"\n",
    "\n",
    "maxLabels = 32\n",
    "\n",
    "#choose radiologist_pair (last three digits are the IDS used in the paper)\n",
    "\n",
    "#LABELER_IDS = [4295342357, 4295349121]\n",
    "#LABELER_IDS = [4323195249, 4295194124]\n",
    "#LABELER_IDS = [4295342357, 4295354117]\n",
    "LABELER_IDS = [4323195249, 4295232296]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdd5072c-48b3-4fdc-b41d-3a5b2267c009",
   "metadata": {
    "collapsed": true,
    "id": "EcgtljuDjcsy",
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 0\n",
      "----------------------------------------\n",
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n",
      "Running fold 3 out of 10\n",
      "Running fold 4 out of 10\n",
      "Running fold 5 out of 10\n",
      "Running fold 6 out of 10\n",
      "Running fold 7 out of 10\n",
      "Running fold 8 out of 10\n",
      "Running fold 9 out of 10\n",
      "Running fold 10 out of 10\n",
      "Best Expert Accuracy: 0.9131455399061033\n",
      "\n",
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n",
      "Running fold 3 out of 10\n",
      "Running fold 4 out of 10\n",
      "Running fold 5 out of 10\n",
      "Running fold 6 out of 10\n",
      "Running fold 7 out of 10\n",
      "Running fold 8 out of 10\n",
      "Running fold 9 out of 10\n",
      "Running fold 10 out of 10\n",
      "Average Expert Accuracy: 0.8591549295774648\n",
      "\n",
      "Team Performance Optimization with Our Approach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Resnet-18 pretrained on ImageNet\n",
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:37<00:00, 18.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m avg_expert_accuracy \u001b[38;5;241m=\u001b[39m get_accuracy_of_average_expert(seed, expert_fns, PATH, maxLabels)\n\u001b[0;32m     26\u001b[0m avg_expert_accuracies\u001b[38;5;241m.\u001b[39mappend(avg_expert_accuracy)\n\u001b[1;32m---> 28\u001b[0m our_approach_accuracy, our_approach_coverage \u001b[38;5;241m=\u001b[39m \u001b[43mrun_team_performance_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOur Approach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpert_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxLabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m our_approach_accuracies\u001b[38;5;241m.\u001b[39mappend(our_approach_accuracy)\n\u001b[0;32m     30\u001b[0m our_approach_coverages\u001b[38;5;241m.\u001b[39mappend(our_approach_coverage)\n",
      "Cell \u001b[1;32mIn[10], line 119\u001b[0m, in \u001b[0;36mrun_team_performance_optimization\u001b[1;34m(method, seed, expert_fns, PATH, maxLabels)\u001b[0m\n\u001b[0;32m    116\u001b[0m train_one_epoch(epoch, feature_extractor, classifier, allocation_system, train_loader, optimizer, scheduler, expert_fns, loss_fn)\n\u001b[0;32m    118\u001b[0m val_system_accuracy, val_system_loss, _, _, _ \u001b[38;5;241m=\u001b[39m evaluate_one_epoch(epoch, feature_extractor, classifier, allocation_system, val_loader, expert_fns, loss_fn)\n\u001b[1;32m--> 119\u001b[0m _, _, test_system_preds, test_allocation_system_decisions, test_targets \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallocation_system\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpert_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJoint Sparse Framework\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_system_accuracy \u001b[38;5;241m>\u001b[39m best_val_system_accuracy:\n",
      "Cell \u001b[1;32mIn[10], line 37\u001b[0m, in \u001b[0;36mevaluate_one_epoch\u001b[1;34m(epoch, feature_extractor, classifier, allocation_system, data_loader, expert_fns, loss_fn)\u001b[0m\n\u001b[0;32m     34\u001b[0m filenames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_input, batch_targets, batch_filenames) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m     38\u001b[0m         batch_input \u001b[38;5;241m=\u001b[39m batch_input\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m         batch_targets \u001b[38;5;241m=\u001b[39m batch_targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 59\u001b[0m, in \u001b[0;36mNIH_Dataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m     58\u001b[0m     filename, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ids[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index]\n\u001b[1;32m---> 59\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess:\n\u001b[0;32m     61\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformImage(img)\n",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m, in \u001b[0;36mNIH_Dataset.getImage\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mNIH_Dataset.loadImage\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadImage\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Load one single image\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m244\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m244\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\PIL\\Image.py:2192\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2184\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2185\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2186\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2187\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2188\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2189\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2190\u001b[0m         )\n\u001b[1;32m-> 2192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_expert_accuracies = []\n",
    "avg_expert_accuracies = []\n",
    "our_approach_accuracies = []\n",
    "our_approach_coverages = []\n",
    "jsf_accuracies = []\n",
    "jsf_coverages = []\n",
    "full_automation_accuracies = []\n",
    "moae_accuracies = []\n",
    "mohe_accuracies = []\n",
    "\n",
    "for seed in range(2):\n",
    "    print(f'Seed: {seed}')\n",
    "    print(\"-\"*40)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    expert_fns = []\n",
    "    for labelerId in list(LABELER_IDS):\n",
    "        nih_expert = NihExpert(labeler_id=labelerId, target=TARGET)\n",
    "        expert_fns.append(nih_expert.predict)\n",
    "\n",
    "    best_expert_accuracy = get_accuracy_of_best_expert(seed, expert_fns, PATH, maxLabels)\n",
    "    best_expert_accuracies.append(best_expert_accuracy)\n",
    "    \n",
    "    avg_expert_accuracy = get_accuracy_of_average_expert(seed, expert_fns, PATH, maxLabels)\n",
    "    avg_expert_accuracies.append(avg_expert_accuracy)\n",
    "\n",
    "    our_approach_accuracy, our_approach_coverage = run_team_performance_optimization(\"Our Approach\", seed, expert_fns, PATH, maxLabels)\n",
    "    our_approach_accuracies.append(our_approach_accuracy)\n",
    "    our_approach_coverages.append(our_approach_coverage)\n",
    "    \n",
    "    jsf_accuracy, jsf_coverage = run_team_performance_optimization(\"Joint Sparse Framework\", seed, expert_fns, PATH, maxLabels)\n",
    "    jsf_accuracies.append(jsf_accuracy)\n",
    "    jsf_coverages.append(jsf_coverage)\n",
    "\n",
    "    full_automation_accuracy = run_full_automation(seed, PATH, maxLabels)\n",
    "    full_automation_accuracies.append(full_automation_accuracy)\n",
    "\n",
    "    moae_accuracy = run_moae(seed, PATH, maxLabels)\n",
    "    moae_accuracies.append(moae_accuracy)\n",
    "\n",
    "    mohe_accuracy = run_mohe(seed, expert_fns, PATH, maxLabels)\n",
    "    mohe_accuracies.append(mohe_accuracy)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac7ee8-a1f0-422b-9aca-152c22bbb459",
   "metadata": {
    "id": "Vh_6HrLYJ8ek"
   },
   "outputs": [],
   "source": [
    "mean_best_expert_accuracy = np.mean(best_expert_accuracies)\n",
    "mean_best_expert_coverage = 0.00\n",
    "\n",
    "mean_avg_expert_accuracy = np.mean(avg_expert_accuracies)\n",
    "mean_avg_expert_coverage = 0.00\n",
    "\n",
    "mean_our_approach_accuracy = np.mean(our_approach_accuracies)\n",
    "mean_our_approach_coverage = np.mean(our_approach_coverages)\n",
    "\n",
    "mean_jsf_accuracy = np.mean(jsf_accuracies)\n",
    "mean_jsf_coverage = np.mean(jsf_coverages)\n",
    "\n",
    "mean_full_automation_accuracy = np.mean(full_automation_accuracies)\n",
    "mean_full_automation_coverage = 100.00\n",
    "\n",
    "mean_moae_accuracy = np.mean(moae_accuracies)\n",
    "mean_moae_coverage = 100.00\n",
    "\n",
    "mean_mohe_accuracy = np.mean(mohe_accuracies)\n",
    "mean_mohe_coverage = 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80f9f364-985a-4593-bd24-a51af11aa5e8",
   "metadata": {
    "id": "3pyRl5EKjcsy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_our_approach_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(tabulate([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOur Approach\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mmean_our_approach_accuracy\u001b[49m, mean_our_approach_coverage],\n\u001b[0;32m      2\u001b[0m                 [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJSF\u001b[39m\u001b[38;5;124m'\u001b[39m, mean_jsf_accuracy, mean_jsf_coverage],\n\u001b[0;32m      3\u001b[0m                 [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      4\u001b[0m                 [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull Automation\u001b[39m\u001b[38;5;124m'\u001b[39m, mean_full_automation_accuracy, mean_full_automation_coverage],\n\u001b[0;32m      5\u001b[0m                 [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMOAE\u001b[39m\u001b[38;5;124m'\u001b[39m, mean_moae_accuracy, mean_moae_coverage],\n\u001b[0;32m      6\u001b[0m                 [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMOHE\u001b[39m\u001b[38;5;124m'\u001b[39m, mean_mohe_accuracy, mean_mohe_coverage],\n\u001b[0;32m      7\u001b[0m                 [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Expert\u001b[39m\u001b[38;5;124m'\u001b[39m, mean_avg_expert_accuracy, mean_avg_expert_coverage],\n\u001b[0;32m      8\u001b[0m                 [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Expert\u001b[39m\u001b[38;5;124m'\u001b[39m, mean_best_expert_accuracy, mean_best_expert_coverage]], \n\u001b[0;32m      9\u001b[0m                headers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoverage\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_our_approach_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "print(tabulate([['Our Approach', mean_our_approach_accuracy, mean_our_approach_coverage],\n",
    "                ['JSF', mean_jsf_accuracy, mean_jsf_coverage],\n",
    "                ['--------', '--------', '--------'],\n",
    "                ['Full Automation', mean_full_automation_accuracy, mean_full_automation_coverage],\n",
    "                ['MOAE', mean_moae_accuracy, mean_moae_coverage],\n",
    "                ['MOHE', mean_mohe_accuracy, mean_mohe_coverage],\n",
    "                ['Random Expert', mean_avg_expert_accuracy, mean_avg_expert_coverage],\n",
    "                ['Best Expert', mean_best_expert_accuracy, mean_best_expert_coverage]], \n",
    "               headers=['Method', 'Accuracy', 'Coverage']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc420da3-457a-484b-aab2-f59b12c02bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 0\n",
      "----------------------------------------\n",
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n",
      "Running fold 3 out of 10\n",
      "Running fold 4 out of 10\n",
      "Running fold 5 out of 10\n",
      "Running fold 6 out of 10\n",
      "Running fold 7 out of 10\n",
      "Running fold 8 out of 10\n",
      "Running fold 9 out of 10\n",
      "Running fold 10 out of 10\n",
      "Best Expert Accuracy: 0.9131455399061033\n",
      "\n",
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n",
      "Running fold 3 out of 10\n",
      "Running fold 4 out of 10\n",
      "Running fold 5 out of 10\n",
      "Running fold 6 out of 10\n",
      "Running fold 7 out of 10\n",
      "Running fold 8 out of 10\n",
      "Running fold 9 out of 10\n",
      "Running fold 10 out of 10\n",
      "Average Expert Accuracy: 0.8568075117370892\n",
      "\n",
      "Team Performance Optimization with Our Approach\n",
      "load Resnet-18 pretrained on ImageNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.79s/it]\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team Performance Optimization with Joint Sparse Framework\n",
      "load Resnet-18 pretrained on ImageNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.58s/it]\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training full automation baseline\n",
      "load Resnet-18 pretrained on ImageNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.59s/it]\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mixture of artificial experts baseline\n",
      "load Resnet-18 pretrained on ImageNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.76s/it]\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixture of Artificial Experts Accuracy: 0.5164319248826291\n",
      "\n",
      "Training Mixture of human experts baseline\n",
      "load Resnet-18 pretrained on ImageNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.78s/it]\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixture of Human Experts Accuracy: 0.8673708920187794\n",
      "\n",
      "----------------------------------------\n",
      "Seed: 1\n",
      "----------------------------------------\n",
      "Running fold 1 out of 10\n",
      "Running fold 2 out of 10\n",
      "Running fold 3 out of 10\n",
      "Running fold 4 out of 10\n",
      "Running fold 5 out of 10\n",
      "Running fold 6 out of 10\n",
      "Running fold 7 out of 10\n",
      "Running fold 8 out of 10\n",
      "Running fold 9 out of 10\n",
      "Running fold 10 out of 10\n",
      "Best Expert Accuracy: 0.9131455399061033\n",
      "\n",
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n",
      "Running fold 3 out of 10\n",
      "Running fold 4 out of 10\n",
      "Running fold 5 out of 10\n",
      "Running fold 6 out of 10\n",
      "Running fold 7 out of 10\n",
      "Running fold 8 out of 10\n",
      "Running fold 9 out of 10\n",
      "Running fold 10 out of 10\n",
      "Average Expert Accuracy: 0.8568075117370892\n",
      "\n",
      "Team Performance Optimization with Our Approach\n",
      "load Resnet-18 pretrained on ImageNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.87s/it]\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team Performance Optimization with Joint Sparse Framework\n",
      "load Resnet-18 pretrained on ImageNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m our_approach_accuracies\u001b[38;5;241m.\u001b[39mappend(our_approach_accuracy)\n\u001b[0;32m     37\u001b[0m our_approach_coverages\u001b[38;5;241m.\u001b[39mappend(our_approach_coverage)\n\u001b[1;32m---> 39\u001b[0m jsf_accuracy, jsf_coverage \u001b[38;5;241m=\u001b[39m \u001b[43mrun_team_performance_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJoint Sparse Framework\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpert_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m jsf_accuracies\u001b[38;5;241m.\u001b[39mappend(jsf_accuracy)\n\u001b[0;32m     41\u001b[0m jsf_coverages\u001b[38;5;241m.\u001b[39mappend(jsf_coverage)\n",
      "Cell \u001b[1;32mIn[9], line 118\u001b[0m, in \u001b[0;36mrun_team_performance_optimization\u001b[1;34m(method, seed, expert_fns, PATH, maxLabels)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    116\u001b[0m     train_one_epoch(epoch, feature_extractor, classifier, allocation_system, train_loader, optimizer, scheduler, expert_fns, loss_fn)\n\u001b[1;32m--> 118\u001b[0m     val_system_accuracy, val_system_loss, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallocation_system\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpert_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     _, _, test_system_preds, test_allocation_system_decisions, test_targets \u001b[38;5;241m=\u001b[39m evaluate_one_epoch(epoch, feature_extractor, classifier, allocation_system, test_loader, expert_fns, loss_fn)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJoint Sparse Framework\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[9], line 37\u001b[0m, in \u001b[0;36mevaluate_one_epoch\u001b[1;34m(epoch, feature_extractor, classifier, allocation_system, data_loader, expert_fns, loss_fn)\u001b[0m\n\u001b[0;32m     34\u001b[0m filenames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_input, batch_targets, batch_filenames) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m     38\u001b[0m         batch_input \u001b[38;5;241m=\u001b[39m batch_input\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m         batch_targets \u001b[38;5;241m=\u001b[39m batch_targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m, in \u001b[0;36mNIH_Dataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m     58\u001b[0m     filename, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ids[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index]\n\u001b[1;32m---> 59\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess:\n\u001b[0;32m     61\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformImage(img)\n",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m, in \u001b[0;36mNIH_Dataset.getImage\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36mNIH_Dataset.loadImage\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadImage\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Load one single image\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m244\u001b[39m,\u001b[38;5;241m244\u001b[39m))\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\PIL\\Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3224\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3227\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3228\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "metrics_print = []\n",
    "\n",
    "maxLabels = [4, 8, 16, 32, 64, 128]\n",
    "\n",
    "for maxL in maxLabels:\n",
    "\n",
    "    best_expert_accuracies = []\n",
    "    avg_expert_accuracies = []\n",
    "    our_approach_accuracies = []\n",
    "    our_approach_coverages = []\n",
    "    jsf_accuracies = []\n",
    "    jsf_coverages = []\n",
    "    full_automation_accuracies = []\n",
    "    moae_accuracies = []\n",
    "    mohe_accuracies = []\n",
    "\n",
    "    for seed in range(3):\n",
    "        print(f'Seed: {seed}')\n",
    "        print(\"-\"*40)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    \n",
    "        expert_fns = []\n",
    "        for labelerId in list(LABELER_IDS):\n",
    "            nih_expert = NihExpert(labeler_id=labelerId, target=TARGET)\n",
    "            expert_fns.append(nih_expert.predict)\n",
    "\n",
    "        best_expert_accuracy = get_accuracy_of_best_expert(seed, expert_fns, PATH, maxL)\n",
    "        best_expert_accuracies.append(best_expert_accuracy)\n",
    "    \n",
    "        avg_expert_accuracy = get_accuracy_of_average_expert(seed, expert_fns, PATH, maxL)\n",
    "        avg_expert_accuracies.append(avg_expert_accuracy)\n",
    "\n",
    "        our_approach_accuracy, our_approach_coverage = run_team_performance_optimization(\"Our Approach\", seed, expert_fns, PATH, maxL)\n",
    "        our_approach_accuracies.append(our_approach_accuracy)\n",
    "        our_approach_coverages.append(our_approach_coverage)\n",
    "    \n",
    "        jsf_accuracy, jsf_coverage = run_team_performance_optimization(\"Joint Sparse Framework\", seed, expert_fns, PATH, maxL)\n",
    "        jsf_accuracies.append(jsf_accuracy)\n",
    "        jsf_coverages.append(jsf_coverage)\n",
    "\n",
    "        full_automation_accuracy = run_full_automation(seed, PATH, maxL)\n",
    "        full_automation_accuracies.append(full_automation_accuracy)\n",
    "\n",
    "        moae_accuracy = run_moae(seed, PATH, maxL)\n",
    "        moae_accuracies.append(moae_accuracy)\n",
    "\n",
    "        mohe_accuracy = run_mohe(seed, expert_fns, PATH, maxL)\n",
    "        mohe_accuracies.append(mohe_accuracy)\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "    mean_best_expert_accuracy = np.mean(best_expert_accuracies)\n",
    "    mean_best_expert_coverage = 0.00\n",
    "\n",
    "    mean_avg_expert_accuracy = np.mean(avg_expert_accuracies)\n",
    "    mean_avg_expert_coverage = 0.00\n",
    "\n",
    "    mean_our_approach_accuracy = np.mean(our_approach_accuracies)\n",
    "    mean_our_approach_coverage = np.mean(our_approach_coverages)\n",
    "\n",
    "    mean_jsf_accuracy = np.mean(jsf_accuracies)\n",
    "    mean_jsf_coverage = np.mean(jsf_coverages)\n",
    "\n",
    "    mean_full_automation_accuracy = np.mean(full_automation_accuracies)\n",
    "    mean_full_automation_coverage = 100.00\n",
    "\n",
    "    mean_moae_accuracy = np.mean(moae_accuracies)\n",
    "    mean_moae_coverage = 100.00\n",
    "\n",
    "    mean_mohe_accuracy = np.mean(mohe_accuracies)\n",
    "    mean_mohe_coverage = 0.00\n",
    "    \n",
    "    metrics_print.append(mean_best_expert_accuracy)\n",
    "    metrics_print.append(mean_best_expert_coverage)\n",
    "    \n",
    "    metrics_print.append(mean_avg_expert_accuracy)\n",
    "    metrics_print.append(mean_avg_expert_coverage)\n",
    "    \n",
    "    metrics_print.append(mean_our_approach_accuracy)\n",
    "    metrics_print.append(mean_our_approach_coverage)\n",
    "    \n",
    "    metrics_print.append(mean_jsf_accuracy)\n",
    "    metrics_print.append(mean_jsf_coverage)\n",
    "    \n",
    "    metrics_print.append(mean_full_automation_accuracy)\n",
    "    metrics_print.append(mean_full_automation_coverage)\n",
    "    \n",
    "    metrics_print.append(mean_moae_accuracy)\n",
    "    metrics_print.append(mean_moae_coverage)\n",
    "    \n",
    "    metrics_print.append(mean_mohe_accuracy)\n",
    "    metrics_print.append(mean_mohe_coverage)\n",
    "    \n",
    "    \n",
    "    metrics.append(best_expert_accuracies)\n",
    "    metrics.append(avg_expert_accuracies)\n",
    "    metrics.append(our_approach_accuracies)\n",
    "    metrics.append(our_approach_coverages)\n",
    "    metrics.append(jsf_accuracies)\n",
    "    metrics.append(jsf_coverages)\n",
    "    metrics.append(full_automation_accuracies)\n",
    "    metrics.append(moae_accuracies)\n",
    "    metrics.append(mohe_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "273cf5c6-79f2-4901-adcf-d894f7053647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for met in metrics_print:\n",
    "    \n",
    "    mean_best_expert_accuracy = met[0]\n",
    "    mean_best_expert_coverage = 0.00\n",
    "\n",
    "    mean_avg_expert_accuracy = met[2]\n",
    "    mean_avg_expert_coverage = 0.00\n",
    "\n",
    "    mean_our_approach_accuracy = met[4]\n",
    "    mean_our_approach_coverage = met[5]\n",
    "\n",
    "    mean_jsf_accuracy = met[6]\n",
    "    mean_jsf_coverage = met[7]\n",
    "\n",
    "    mean_full_automation_accuracy = met[8]\n",
    "    mean_full_automation_coverage = 100.00\n",
    "\n",
    "    mean_moae_accuracy = met[10]\n",
    "    mean_moae_coverage = 100.00\n",
    "\n",
    "    mean_mohe_accuracy = met[12]\n",
    "    mean_mohe_coverage = 0.00\n",
    "    \n",
    "    print(tabulate([['Our Approach', mean_our_approach_accuracy, mean_our_approach_coverage],\n",
    "                ['JSF', mean_jsf_accuracy, mean_jsf_coverage],\n",
    "                ['--------', '--------', '--------'],\n",
    "                ['Full Automation', mean_full_automation_accuracy, mean_full_automation_coverage],\n",
    "                ['MOAE', mean_moae_accuracy, mean_moae_coverage],\n",
    "                ['MOHE', mean_mohe_accuracy, mean_mohe_coverage],\n",
    "                ['Random Expert', mean_avg_expert_accuracy, mean_avg_expert_coverage],\n",
    "                ['Best Expert', mean_best_expert_accuracy, mean_best_expert_coverage]], \n",
    "               headers=['Method', 'Accuracy', 'Coverage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0aab82ab-f857-426c-9d98-bd8f6a5374e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab1f4b88-b505-4ca0-98de-6ed1927f4aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mixture of human experts baseline\n",
      "load Resnet-18 pretrained on ImageNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "D:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     nih_expert \u001b[38;5;241m=\u001b[39m NihExpert(labeler_id\u001b[38;5;241m=\u001b[39mlabelerId, target\u001b[38;5;241m=\u001b[39mTARGET)\n\u001b[0;32m      4\u001b[0m     expert_fns\u001b[38;5;241m.\u001b[39mappend(nih_expert\u001b[38;5;241m.\u001b[39mpredict)\n\u001b[1;32m----> 6\u001b[0m mohe_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mrun_mohe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpert_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 99\u001b[0m, in \u001b[0;36mrun_mohe\u001b[1;34m(seed, expert_fns, PATH, maxLabels)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     97\u001b[0m     train_mohe_one_epoch(feature_extractor, allocation_system, train_loader, optimizer, scheduler, expert_fns)\n\u001b[1;32m---> 99\u001b[0m     val_system_loss, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_mohe_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallocation_system\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpert_fns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     _, test_system_preds, test_targets \u001b[38;5;241m=\u001b[39m evaluate_mohe_one_epoch(feature_extractor, allocation_system, test_loader, expert_fns)\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_system_loss \u001b[38;5;241m<\u001b[39m best_val_system_loss:\n",
      "Cell \u001b[1;32mIn[27], line 37\u001b[0m, in \u001b[0;36mevaluate_mohe_one_epoch\u001b[1;34m(feature_extractor, allocation_system, data_loader, expert_fns)\u001b[0m\n\u001b[0;32m     34\u001b[0m filenames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_input, batch_targets, batch_filenames) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m     38\u001b[0m         batch_input \u001b[38;5;241m=\u001b[39m batch_input\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m         batch_targets \u001b[38;5;241m=\u001b[39m batch_targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m, in \u001b[0;36mNIH_Dataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m     58\u001b[0m     filename, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ids[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index]\n\u001b[1;32m---> 59\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess:\n\u001b[0;32m     61\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformImage(img)\n",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m, in \u001b[0;36mNIH_Dataset.getImage\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36mNIH_Dataset.loadImage\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadImage\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Load one single image\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m244\u001b[39m,\u001b[38;5;241m244\u001b[39m))\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\PIL\\Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    939\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\PIL\\ImageFile.py:249\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[1;32mD:\\Development\\anaconda\\envs\\Masterarbeit\\lib\\site-packages\\PIL\\PngImagePlugin.py:980\u001b[0m, in \u001b[0;36mPngImageFile.load_read\u001b[1;34m(self, read_bytes)\u001b[0m\n\u001b[0;32m    976\u001b[0m     read_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(read_bytes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat)\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat \u001b[38;5;241m-\u001b[39m read_bytes\n\u001b[1;32m--> 980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "expert_fns = []\n",
    "for labelerId in list(LABELER_IDS):\n",
    "    nih_expert = NihExpert(labeler_id=labelerId, target=TARGET)\n",
    "    expert_fns.append(nih_expert.predict)\n",
    "\n",
    "mohe_accuracy = run_mohe(1, expert_fns, PATH, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3bbf3-790a-4f6e-b1a5-7ba21b97c316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
